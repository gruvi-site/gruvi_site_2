,object,id,created_time,last_edited_time,created_by,last_edited_by,cover,icon,parent,archived,in_trash,properties,url,public_url
0,page,509bf7e8-b922-4e77-9840-b4bc43663925,2024-11-25T23:19:00.000Z,2024-11-25T23:25:00.000Z,"{'object': 'user', 'id': '149d872b-594c-8166-87d8-00026278990d'}","{'object': 'user', 'id': '149d872b-594c-8166-87d8-00026278990d'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2024-11-25T23:25:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2025-03-25', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Hou In Ivan Tam, Hou In Derek Pun, Austin T. Wang, Angel X. Chang, Manolis Savva', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Hou In Ivan Tam, Hou In Derek Pun, Austin T. Wang, Angel X. Chang, Manolis Savva', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '149d872b-594c-8166-87d8-00026278990d'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Despite advances in text-to-3D generation methods, generation of multi-object arrangements remains challenging. Current methods exhibit failures in generating physically plausible arrangements that respect the provided text description. We present SceneMotifCoder (SMC), an example-driven framework for generating 3D object arrangements through visual program learning. SMC leverages large language models (LLMs) and program synthesis to overcome these challenges by learning visual programs from example arrangements. These programs are generalized into compact, editable meta-programs. When combined with 3D object retrieval and geometry-aware optimization, they can be used to create object arrangements varying in arrangement structure and contained objects. Our experiments show that SMC generates high-quality arrangements using meta-programs learned from few examples. Evaluation results demonstrates that object arrangements generated by SMC better conform to user-specified text descriptions and are more physically plausible when compared with state-of-the-art text-to-3D generation and layout methods.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Despite advances in text-to-3D generation methods, generation of multi-object arrangements remains challenging. Current methods exhibit failures in generating physically plausible arrangements that respect the provided text description. We present SceneMotifCoder (SMC), an example-driven framework for generating 3D object arrangements through visual program learning. SMC leverages large language models (LLMs) and program synthesis to overcome these challenges by learning visual programs from example arrangements. These programs are generalized into compact, editable meta-programs. When combined with 3D object retrieval and geometry-aware optimization, they can be used to create object arrangements varying in arrangement structure and contained objects. Our experiments show that SMC generates high-quality arrangements using meta-programs learned from few examples. Evaluation results demonstrates that object arrangements generated by SMC better conform to user-specified text descriptions and are more physically plausible when compared with state-of-the-art text-to-3D generation and layout methods.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://github.com/3dlg-hcvc/smc/blob/main/docs/static/images/teaser.png?raw=true', 'type': 'external', 'external': {'url': 'https://github.com/3dlg-hcvc/smc/blob/main/docs/static/images/teaser.png?raw=true'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://3dlg-hcvc.github.io/smc/', 'link': {'url': 'https://3dlg-hcvc.github.io/smc/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://3dlg-hcvc.github.io/smc/', 'href': 'https://3dlg-hcvc.github.io/smc/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': '3DV', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': '3DV', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '149d872b-594c-8166-87d8-00026278990d'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://www.youtube.com/watch?v=iXsx18R7NN8', 'link': {'url': 'https://www.youtube.com/watch?v=iXsx18R7NN8'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://www.youtube.com/watch?v=iXsx18R7NN8', 'href': 'https://www.youtube.com/watch?v=iXsx18R7NN8'}]}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'bccbeae3-55ce-4b38-9abc-61c658ca30b4', 'name': 'Manolis', 'color': 'blue'}, {'id': 'd9d68329-9f05-4187-aa6b-052567aebad4', 'name': '3DV', 'color': 'gray'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2408.02211', 'link': {'url': 'https://arxiv.org/abs/2408.02211'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2408.02211', 'href': 'https://arxiv.org/abs/2408.02211'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'SceneMotifCoder: Example-driven Visual Program Learning for Generating 3D Object Arrangements', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'SceneMotifCoder: Example-driven Visual Program Learning for Generating 3D Object Arrangements', 'href': None}]}}",https://www.notion.so/SceneMotifCoder-Example-driven-Visual-Program-Learning-for-Generating-3D-Object-Arrangements-509bf7e8b9224e779840b4bc43663925,https://yanxg.notion.site/SceneMotifCoder-Example-driven-Visual-Program-Learning-for-Generating-3D-Object-Arrangements-509bf7e8b9224e779840b4bc43663925
1,page,149bceb3-5801-80c9-9844-eceb469cb948,2024-11-25T20:31:00.000Z,2024-11-25T20:41:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2024-11-25T20:41:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2025-03-25', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Xingguang Yan, Han-Hung Lee, Ziyu Wan, Angel X. Chang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Xingguang Yan, Han-Hung Lee, Ziyu Wan, Angel X. Chang', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'We introduce a new approach for generating realistic 3D models with UV maps through a representation termed ""Object Images."" This approach encapsulates surface geometry, appearance, and patch structures within a 64x64 pixel image, effectively converting complex 3D shapes into a more manageable 2D format. By doing so, we address the challenges of both geometric and semantic irregularity inherent in polygonal meshes. This method allows us to use image generation models, such as Diffusion Transformers, directly for 3D shape generation. Evaluated on the ABO dataset, our generated shapes with patch structures achieve point cloud FID comparable to recent 3D generative models, while naturally supporting PBR material generation.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'We introduce a new approach for generating realistic 3D models with UV maps through a representation termed ""Object Images."" This approach encapsulates surface geometry, appearance, and patch structures within a 64x64 pixel image, effectively converting complex 3D shapes into a more manageable 2D format. By doing so, we address the challenges of both geometric and semantic irregularity inherent in polygonal meshes. This method allows us to use image generation models, such as Diffusion Transformers, directly for 3D shape generation. Evaluated on the ABO dataset, our generated shapes with patch structures achieve point cloud FID comparable to recent 3D generative models, while naturally supporting PBR material generation.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://angelxuanchang.github.io/files/omage.png', 'type': 'external', 'external': {'url': 'https://angelxuanchang.github.io/files/omage.png'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://omages.github.io/', 'link': {'url': 'https://omages.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://omages.github.io/', 'href': 'https://omages.github.io/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': '3DV', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': '3DV', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2408.03178', 'link': {'url': 'https://arxiv.org/abs/2408.03178'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2408.03178', 'href': 'https://arxiv.org/abs/2408.03178'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'An object is worth 64x64 pixels: Generating 3d object via image diffusion', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'An object is worth 64x64 pixels: Generating 3d object via image diffusion', 'href': None}]}}",https://www.notion.so/An-object-is-worth-64x64-pixels-Generating-3d-object-via-image-diffusion-149bceb3580180c99844eceb469cb948,https://yanxg.notion.site/An-object-is-worth-64x64-pixels-Generating-3d-object-via-image-diffusion-149bceb3580180c99844eceb469cb948
2,page,1872cc7c-f1d9-42fe-9861-cdef05b31a9d,2024-11-09T01:11:00.000Z,2024-11-09T01:13:00.000Z,"{'object': 'user', 'id': 'b507e305-8efa-443f-9b61-e893102d503a'}","{'object': 'user', 'id': 'b507e305-8efa-443f-9b61-e893102d503a'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2024-11-09T01:13:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2024-12-03', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Chen Tessler, Yunrong Guo, Ofir Nabati, Gal Chechik, Xue Bin Peng', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Chen Tessler, Yunrong Guo, Ofir Nabati, Gal Chechik, Xue Bin Peng', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'b507e305-8efa-443f-9b61-e893102d503a'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'MaskedMimic_thumb.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/feb17482-9a06-45cc-bcae-2df4f23b9329/MaskedMimic_thumb.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234740Z&X-Amz-Expires=3600&X-Amz-Signature=cf3d5bd33924453b529f50943add71f28d50195795faad8f5dfe42d1e1f9349b&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:40.264Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://research.nvidia.com/labs/par/maskedmimic/', 'link': {'url': 'https://research.nvidia.com/labs/par/maskedmimic/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://research.nvidia.com/labs/par/maskedmimic/', 'href': 'https://research.nvidia.com/labs/par/maskedmimic/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'SIGGRAPH Asia 2024', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'SIGGRAPH Asia 2024', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'b507e305-8efa-443f-9b61-e893102d503a'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'd0404583-7d96-4756-b656-bb910bc1e011', 'name': 'SIGGRAPH Asia', 'color': 'blue'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://research.nvidia.com/labs/par/maskedmimic/assets/SIGGRAPHAsia2024_MaskedMimic.pdf', 'link': {'url': 'https://research.nvidia.com/labs/par/maskedmimic/assets/SIGGRAPHAsia2024_MaskedMimic.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://research.nvidia.com/labs/par/maskedmimic/assets/SIGGRAPHAsia2024_MaskedMimic.pdf', 'href': 'https://research.nvidia.com/labs/par/maskedmimic/assets/SIGGRAPHAsia2024_MaskedMimic.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '❌❌❌❌'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'MaskedMimic: Unified Physics-Based Character Control Through Masked Motion Inpainting', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'MaskedMimic: Unified Physics-Based Character Control Through Masked Motion Inpainting', 'href': None}]}}",https://www.notion.so/MaskedMimic-Unified-Physics-Based-Character-Control-Through-Masked-Motion-Inpainting-1872cc7cf1d942fe9861cdef05b31a9d,https://yanxg.notion.site/MaskedMimic-Unified-Physics-Based-Character-Control-Through-Masked-Motion-Inpainting-1872cc7cf1d942fe9861cdef05b31a9d
3,page,20ea0ea9-bfe5-465f-ac7a-4e4423f5fc18,2024-08-01T17:48:00.000Z,2024-11-25T20:43:00.000Z,"{'object': 'user', 'id': '6449ae1c-47ea-4118-b87f-793dc03baabc'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2024-11-25T20:43:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2024-09-29', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Qirui Wu, Sonia Raychaudhuri, Daniel Ritchie, Manolis Savva, Angel X. Chang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Qirui Wu, Sonia Raychaudhuri, Daniel Ritchie, Manolis Savva, Angel X. Chang', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'We introduce the Reality-linked 3D Scenes (R3DS) dataset of synthetic 3D scenes mirroring the real-world scene arrangements from Matterport3D. Compared to prior work, R3DS has more complete and densely populated scenes with objects linked to real-world observations in panoramas. R3DS also provides an object support hierarchy, and matching object sets (e.g., same chairs around a dining table) for each scene.\n\nOverall, R3DS contains 19K objects represented by 3784 distinct CAD models from over 100 object categories. We demonstrate the effectiveness of R3DS on the Panoramic Scene Understanding task. We find that: 1) training on R3DS enables better generalization; 2) support relation prediction trained with R3DS improves performance compared to heuristically calculated support; and 3) R3DS offers a challenging benchmark for future work on panoramic scene understanding.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'We introduce the Reality-linked 3D Scenes (R3DS) dataset of synthetic 3D scenes mirroring the real-world scene arrangements from Matterport3D. Compared to prior work, R3DS has more complete and densely populated scenes with objects linked to real-world observations in panoramas. R3DS also provides an object support hierarchy, and matching object sets (e.g., same chairs around a dining table) for each scene.\n\nOverall, R3DS contains 19K objects represented by 3784 distinct CAD models from over 100 object categories. We demonstrate the effectiveness of R3DS on the Panoramic Scene Understanding task. We find that: 1) training on R3DS enables better generalization; 2) support relation prediction trained with R3DS improves performance compared to heuristically calculated support; and 3) R3DS offers a challenging benchmark for future work on panoramic scene understanding.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://qiruiw.github.io/assets/img/publication_preview/r3ds-480.webp', 'type': 'external', 'external': {'url': 'https://qiruiw.github.io/assets/img/publication_preview/r3ds-480.webp'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://3dlg-hcvc.github.io/r3ds/', 'link': {'url': 'https://3dlg-hcvc.github.io/r3ds/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://3dlg-hcvc.github.io/r3ds/', 'href': 'https://3dlg-hcvc.github.io/r3ds/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ECCV 2024', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ECCV 2024', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '6449ae1c-47ea-4118-b87f-793dc03baabc'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://youtu.be/waI3m7TuyNA', 'link': {'url': 'https://youtu.be/waI3m7TuyNA'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://youtu.be/waI3m7TuyNA', 'href': 'https://youtu.be/waI3m7TuyNA'}]}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '5cbe73f7-7a55-4251-80d4-2375865f5724', 'name': 'ECCV', 'color': 'yellow'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}, {'id': 'bccbeae3-55ce-4b38-9abc-61c658ca30b4', 'name': 'Manolis', 'color': 'blue'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2403.12301', 'link': {'url': 'https://arxiv.org/abs/2403.12301'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2403.12301', 'href': 'https://arxiv.org/abs/2403.12301'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'R3DS: Reality-linked 3D Scenes for Panoramic Scene Understanding', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'R3DS: Reality-linked 3D Scenes for Panoramic Scene Understanding', 'href': None}]}}",https://www.notion.so/R3DS-Reality-linked-3D-Scenes-for-Panoramic-Scene-Understanding-20ea0ea9bfe5465fac7a4e4423f5fc18,https://yanxg.notion.site/R3DS-Reality-linked-3D-Scenes-for-Panoramic-Scene-Understanding-20ea0ea9bfe5465fac7a4e4423f5fc18
4,page,f5e85155-e234-4db6-a8ff-62ab0562cdb4,2024-07-23T18:13:00.000Z,2024-08-26T20:53:00.000Z,"{'object': 'user', 'id': 'bd7176b3-a752-4e75-aaed-ceb6cba2b982'}","{'object': 'user', 'id': 'edfa4d94-358b-4482-91a3-67133bc15be8'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2024-08-26T20:53:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2024-09-29', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Sebastian Dille, Chris Careaga, Yağız Aksoy', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Sebastian Dille, Chris Careaga, Yağız Aksoy', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'edfa4d94-358b-4482-91a3-67133bc15be8'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'The low dynamic range (LDR) of common cameras fails tocapture the rich contrast in natural scenes, resulting in loss of colorand details in saturated pixels. Reconstructing the high dynamic range(HDR) of luminance present in the scene from single LDR photographs isan important task with many applications in computational photographyand realistic display of images. The HDR reconstruction task aims to in-fer the lost details using the context present in the scene, requiring neuralnetworks to understand high-level geometric and illumination cues. Thismakes it challenging for data-driven algorithms to generate accurate andhigh-resolution results. In this work, we introduce a physically-inspiredremodeling of the HDR reconstruction problem in the intrinsic domain.The intrinsic model allows us to train separate networks to extend thedynamic range in the shading domain and to recover lost color details inthe albedo domain. We show that dividing the problem into two simplersub-tasks improves performance in a wide variety of photographs.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'The low dynamic range (LDR) of common cameras fails tocapture the rich contrast in natural scenes, resulting in loss of colorand details in saturated pixels. Reconstructing the high dynamic range(HDR) of luminance present in the scene from single LDR photographs isan important task with many applications in computational photographyand realistic display of images. The HDR reconstruction task aims to in-fer the lost details using the context present in the scene, requiring neuralnetworks to understand high-level geometric and illumination cues. Thismakes it challenging for data-driven algorithms to generate accurate andhigh-resolution results. In this work, we introduce a physically-inspiredremodeling of the HDR reconstruction problem in the intrinsic domain.The intrinsic model allows us to train separate networks to extend thedynamic range in the shading domain and to recover lost color details inthe albedo domain. We show that dividing the problem into two simplersub-tasks improves performance in a wide variety of photographs.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'intrinsicHDR_thumb.jpg', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/c1076504-0907-4606-80d5-6e9aeb608f80/intrinsicHDR_thumb.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234740Z&X-Amz-Expires=3600&X-Amz-Signature=1209cf485aaf734e09943d495a0cc2c7fe5862fad1090b6d8c3ef4100f151b86&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:40.265Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://yaksoy.github.io/intrinsicHDR/', 'link': {'url': 'https://yaksoy.github.io/intrinsicHDR/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://yaksoy.github.io/intrinsicHDR/', 'href': 'https://yaksoy.github.io/intrinsicHDR/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ECCV 2024', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ECCV 2024', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'bd7176b3-a752-4e75-aaed-ceb6cba2b982'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '5cbe73f7-7a55-4251-80d4-2375865f5724', 'name': 'ECCV', 'color': 'yellow'}, {'id': '3eb057d5-ed85-4850-b9ac-ff060c999382', 'name': 'Yagiz', 'color': 'gray'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://yaksoy.github.io/intrinsicHDR/', 'link': {'url': 'https://yaksoy.github.io/intrinsicHDR/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://yaksoy.github.io/intrinsicHDR/', 'href': 'https://yaksoy.github.io/intrinsicHDR/'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Intrinsic Single-Image HDR Reconstruction', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Intrinsic Single-Image HDR Reconstruction', 'href': None}]}}",https://www.notion.so/Intrinsic-Single-Image-HDR-Reconstruction-f5e85155e2344db6a8ff62ab0562cdb4,https://yanxg.notion.site/Intrinsic-Single-Image-HDR-Reconstruction-f5e85155e2344db6a8ff62ab0562cdb4
5,page,cb1a4dec-364b-4f0c-93a3-637df1003b8c,2024-07-23T01:28:00.000Z,2024-08-26T20:54:00.000Z,"{'object': 'user', 'id': '11ba016d-f263-498e-a669-058948397991'}","{'object': 'user', 'id': 'edfa4d94-358b-4482-91a3-67133bc15be8'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2024-08-26T20:54:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2024-09-29', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Weiwei Sun, Eduard Trulls, Yang-Che Tseng, Sneha Sambandam, Gopal Sharma, Andrea Tagliasacchi, Kwang Moo Yi', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Weiwei Sun, Eduard Trulls, Yang-Che Tseng, Sneha Sambandam, Gopal Sharma, Andrea Tagliasacchi, Kwang Moo Yi', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'edfa4d94-358b-4482-91a3-67133bc15be8'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': ""Point clouds offer an attractive source of information to complement images in neural scene representations, especially when few images are available. Neural rendering methods based on point clouds do exist, but they do not perform well when the point cloud quality is low---e.g., sparse or incomplete, which is often the case with real-world data. We overcome these problems with a simple representation that aggregates point clouds at multiple scale levels with sparse voxel grids at different resolutions. To deal with point cloud sparsity, we average across multiple scale levels---but only among those that are valid, i.e., that have enough neighboring points in proximity to the ray of a pixel. To help model areas without points, we add a global voxel at the coarsest scale, thus unifying ``classical'' and point-based NeRF formulations. We validate our method on the NeRF Synthetic, ScanNet, and KITTI-360 datasets, outperforming the state of the art by a significant margin."", 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': ""Point clouds offer an attractive source of information to complement images in neural scene representations, especially when few images are available. Neural rendering methods based on point clouds do exist, but they do not perform well when the point cloud quality is low---e.g., sparse or incomplete, which is often the case with real-world data. We overcome these problems with a simple representation that aggregates point clouds at multiple scale levels with sparse voxel grids at different resolutions. To deal with point cloud sparsity, we average across multiple scale levels---but only among those that are valid, i.e., that have enough neighboring points in proximity to the ray of a pixel. To help model areas without points, we add a global voxel at the coarsest scale, thus unifying ``classical'' and point-based NeRF formulations. We validate our method on the NeRF Synthetic, ScanNet, and KITTI-360 datasets, outperforming the state of the art by a significant margin."", 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'Screenshot 2024-07-22 at 6.29.19\u202fPM.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/2355d1c7-bf38-4c00-80e5-e56ff9ca9f6d/Screenshot_2024-07-22_at_6.29.19_PM.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234740Z&X-Amz-Expires=3600&X-Amz-Signature=d153a30c3719c7f87219c895643c084ae2d848ed287ed4a57da46c34e8cbc752&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:40.266Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://pointnerfpp.github.io', 'link': {'url': 'https://pointnerfpp.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://pointnerfpp.github.io', 'href': 'https://pointnerfpp.github.io/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ECCV 2024', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ECCV 2024', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '11ba016d-f263-498e-a669-058948397991'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '5cbe73f7-7a55-4251-80d4-2375865f5724', 'name': 'ECCV', 'color': 'yellow'}, {'id': '1a31dc62-0b3c-4bfe-8684-3ffe669325d3', 'name': 'Andrea', 'color': 'purple'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://pointnerfpp.github.io/resources/paper.pdf', 'link': {'url': 'https://pointnerfpp.github.io/resources/paper.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://pointnerfpp.github.io/resources/paper.pdf', 'href': 'https://pointnerfpp.github.io/resources/paper.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'PointNeRF++: A multi-scale, point-based Neural Radiance Field', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'PointNeRF++: A multi-scale, point-based Neural Radiance Field', 'href': None}]}}",https://www.notion.so/PointNeRF-A-multi-scale-point-based-Neural-Radiance-Field-cb1a4dec364b4f0c93a3637df1003b8c,https://yanxg.notion.site/PointNeRF-A-multi-scale-point-based-Neural-Radiance-Field-cb1a4dec364b4f0c93a3637df1003b8c
6,page,3b948533-0704-49d1-8c48-8f31094e0ffe,2024-07-23T01:25:00.000Z,2024-08-26T20:54:00.000Z,"{'object': 'user', 'id': '11ba016d-f263-498e-a669-058948397991'}","{'object': 'user', 'id': 'edfa4d94-358b-4482-91a3-67133bc15be8'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2024-08-26T20:54:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2024-09-29', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Gopal Sharma, Daniel Rebain, Kwang Moo Yi, Andrea Tagliasacchi', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Gopal Sharma, Daniel Rebain, Kwang Moo Yi, Andrea Tagliasacchi', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'edfa4d94-358b-4482-91a3-67133bc15be8'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'We propose a novel Neural Radiance Field (NeRF) representation for non-opaque scenes that allows fast inference by utilizing textured polygons. Despite the high-quality novel view rendering that NeRF provides, a critical limitation is that it relies on volume rendering that can be computationally expensive and does not utilize the advancements in modern graphics hardware. Existing methods for this problem fall short when it comes to modelling volumetric effects as they rely purely on surface rendering. We thus propose to model the scene with polygons, which can then be used to obtain the quadrature points required to model volumetric effects, and also their opacity and colour from the texture. To obtain such polygonal mesh, we train a specialized field whose zero-crossings would correspond to the quadrature points when volume rendering, and perform marching cubes on this field. We then rasterize the polygons and utilize the fragment shaders to obtain the final colour image. Our method allows rendering on various devices and easy integration with existing graphics frameworks while keeping the benefits of volume rendering alive.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'We propose a novel Neural Radiance Field (NeRF) representation for non-opaque scenes that allows fast inference by utilizing textured polygons. Despite the high-quality novel view rendering that NeRF provides, a critical limitation is that it relies on volume rendering that can be computationally expensive and does not utilize the advancements in modern graphics hardware. Existing methods for this problem fall short when it comes to modelling volumetric effects as they rely purely on surface rendering. We thus propose to model the scene with polygons, which can then be used to obtain the quadrature points required to model volumetric effects, and also their opacity and colour from the texture. To obtain such polygonal mesh, we train a specialized field whose zero-crossings would correspond to the quadrature points when volume rendering, and perform marching cubes on this field. We then rasterize the polygons and utilize the fragment shaders to obtain the final colour image. Our method allows rendering on various devices and easy integration with existing graphics frameworks while keeping the benefits of volume rendering alive.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'Screenshot 2024-07-22 at 6.26.36\u202fPM.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/4418d584-258e-404e-ae1d-7601a3ff78ef/Screenshot_2024-07-22_at_6.26.36_PM.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234740Z&X-Amz-Expires=3600&X-Amz-Signature=8930e96581102c74dfa4dbe701dd1e689bf88a49fdbd37e33d01a0bc972a01be&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:40.267Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://quadraturefields.github.io', 'link': {'url': 'https://quadraturefields.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://quadraturefields.github.io', 'href': 'https://quadraturefields.github.io/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ECCV 2024', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ECCV 2024', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '11ba016d-f263-498e-a669-058948397991'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '5cbe73f7-7a55-4251-80d4-2375865f5724', 'name': 'ECCV', 'color': 'yellow'}, {'id': '1a31dc62-0b3c-4bfe-8684-3ffe669325d3', 'name': 'Andrea', 'color': 'purple'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://quadraturefields.github.io/static/pdfs/paper.pdf', 'link': {'url': 'https://quadraturefields.github.io/static/pdfs/paper.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://quadraturefields.github.io/static/pdfs/paper.pdf', 'href': 'https://quadraturefields.github.io/static/pdfs/paper.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Volumetric Rendering with Baked Quadrature Fields', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Volumetric Rendering with Baked Quadrature Fields', 'href': None}]}}",https://www.notion.so/Volumetric-Rendering-with-Baked-Quadrature-Fields-3b948533070449d18c488f31094e0ffe,https://yanxg.notion.site/Volumetric-Rendering-with-Baked-Quadrature-Fields-3b948533070449d18c488f31094e0ffe
7,page,3f30a0c6-8893-440b-9b32-d7af4801d6b1,2024-07-23T01:22:00.000Z,2024-08-26T20:54:00.000Z,"{'object': 'user', 'id': '11ba016d-f263-498e-a669-058948397991'}","{'object': 'user', 'id': 'edfa4d94-358b-4482-91a3-67133bc15be8'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2024-08-26T20:54:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2024-09-29', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Shrisudhan Govindarajan, Zeno Sambugaro, Ahan Shabhanov, Towaki Takikawa, Weiwei Sun, Daniel Rebain, Nicola Conci, Kwang Moo Yi, Andrea Tagliasacchi', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Shrisudhan Govindarajan, Zeno Sambugaro, Ahan Shabhanov, Towaki Takikawa, Weiwei Sun, Daniel Rebain, Nicola Conci, Kwang Moo Yi, Andrea Tagliasacchi', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'edfa4d94-358b-4482-91a3-67133bc15be8'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'We present Lagrangian Hashing, a representation for neural fields combining the characteristics of fast training NeRF methods that rely on Eulerian grids (i.e.~InstantNGP), with those that employ points equipped with features as a way to represent information (e.g. 3D Gaussian Splatting or PointNeRF). We achieve this by incorporating a point-based representation into the high-resolution layers of the hierarchical hash tables of an InstantNGP representation. As our points are equipped with a field of influence, our representation can be interpreted as a mixture of Gaussians stored within the hash table. We propose a loss that encourages the movement of our Gaussians towards regions that require more representation budget to be sufficiently well represented. Our main finding is that our representation allows the reconstruction of signals using a more compact representation without compromising quality.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'We present Lagrangian Hashing, a representation for neural fields combining the characteristics of fast training NeRF methods that rely on Eulerian grids (i.e.~InstantNGP), with those that employ points equipped with features as a way to represent information (e.g. 3D Gaussian Splatting or PointNeRF). We achieve this by incorporating a point-based representation into the high-resolution layers of the hierarchical hash tables of an InstantNGP representation. As our points are equipped with a field of influence, our representation can be interpreted as a mixture of Gaussians stored within the hash table. We propose a loss that encourages the movement of our Gaussians towards regions that require more representation budget to be sufficiently well represented. Our main finding is that our representation allows the reconstruction of signals using a more compact representation without compromising quality.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'Screenshot 2024-07-22 at 6.23.26\u202fPM.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/a323e280-7e17-41c2-8752-dcff2403f2c6/Screenshot_2024-07-22_at_6.23.26_PM.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234740Z&X-Amz-Expires=3600&X-Amz-Signature=9350692730d70fd91fe63d9ce6fbddb145f1762b5839fd2b86939128fb45fc68&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:40.267Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://theialab.github.io/laghashes/', 'link': {'url': 'https://theialab.github.io/laghashes/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://theialab.github.io/laghashes/', 'href': 'https://theialab.github.io/laghashes/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ECCV 2024', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ECCV 2024', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '11ba016d-f263-498e-a669-058948397991'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '5cbe73f7-7a55-4251-80d4-2375865f5724', 'name': 'ECCV', 'color': 'yellow'}, {'id': '1a31dc62-0b3c-4bfe-8684-3ffe669325d3', 'name': 'Andrea', 'color': 'purple'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://theialab.github.io/laghashes/resources/paper.pdf', 'link': {'url': 'https://theialab.github.io/laghashes/resources/paper.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://theialab.github.io/laghashes/resources/paper.pdf', 'href': 'https://theialab.github.io/laghashes/resources/paper.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Lagrangian Hashing for Compressed Neural Field Representations', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Lagrangian Hashing for Compressed Neural Field Representations', 'href': None}]}}",https://www.notion.so/Lagrangian-Hashing-for-Compressed-Neural-Field-Representations-3f30a0c68893440b9b32d7af4801d6b1,https://yanxg.notion.site/Lagrangian-Hashing-for-Compressed-Neural-Field-Representations-3f30a0c68893440b9b32d7af4801d6b1
8,page,9aaa39a4-7677-461a-99f2-cad72b6456ea,2024-07-23T01:18:00.000Z,2024-08-26T20:54:00.000Z,"{'object': 'user', 'id': '11ba016d-f263-498e-a669-058948397991'}","{'object': 'user', 'id': 'edfa4d94-358b-4482-91a3-67133bc15be8'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2024-08-26T20:54:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2024-09-29', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Sherwin Bahmani, Xian Liu, Yifan Wang, Ivan Skorokhodov, Victor Rong, Ziwei Liu, Xihui Liu, Jeong Joon Park, Sergey Tulyakov, Gordon Wetzstein, Andrea Tagliasacchi, David B. Lindell', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Sherwin Bahmani, Xian Liu, Yifan Wang, Ivan Skorokhodov, Victor Rong, Ziwei Liu, Xihui Liu, Jeong Joon Park, Sergey Tulyakov, Gordon Wetzstein, Andrea Tagliasacchi, David B. Lindell', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'edfa4d94-358b-4482-91a3-67133bc15be8'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Recent techniques for text-to-4D generation synthesize dynamic 3D scenes using supervision from pre-trained text-to-video models. However, existing representations for motion, such as deformation models or time-dependent neural representations, are limited in the amount of motion they can generate-they cannot synthesize motion extending far beyond the bounding box used for volume rendering. The lack of a more flexible motion model contributes to the gap in realism between 4D generation methods and recent, near-photorealistic video generation models. Here, we propose TC4D: trajectory-conditioned text-to-4D generation, which factors motion into global and local components. We represent the global motion of a scene bounding box using rigid transformation along a spline, and we learn local deformations that conform to the global trajectory using supervision from a text-to-video model. Our approach enables the synthesis of scenes animated along arbitrary trajectories, compositional scene generation, and significant improvements to the realism and amount of generated motion, which we evaluate qualitatively and through a user study.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Recent techniques for text-to-4D generation synthesize dynamic 3D scenes using supervision from pre-trained text-to-video models. However, existing representations for motion, such as deformation models or time-dependent neural representations, are limited in the amount of motion they can generate-they cannot synthesize motion extending far beyond the bounding box used for volume rendering. The lack of a more flexible motion model contributes to the gap in realism between 4D generation methods and recent, near-photorealistic video generation models. Here, we propose TC4D: trajectory-conditioned text-to-4D generation, which factors motion into global and local components. We represent the global motion of a scene bounding box using rigid transformation along a spline, and we learn local deformations that conform to the global trajectory using supervision from a text-to-video model. Our approach enables the synthesis of scenes animated along arbitrary trajectories, compositional scene generation, and significant improvements to the realism and amount of generated motion, which we evaluate qualitatively and through a user study.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'Screenshot 2024-07-22 at 6.21.42\u202fPM.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/4195acb3-af31-46d7-a3f6-5752164c763b/Screenshot_2024-07-22_at_6.21.42_PM.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234740Z&X-Amz-Expires=3600&X-Amz-Signature=0541b396a0f912985098d5d6f189e7d5679bb049e693863a2d09fb8312068b6c&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:40.268Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://sherwinbahmani.github.io/tc4d/', 'link': {'url': 'https://sherwinbahmani.github.io/tc4d/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://sherwinbahmani.github.io/tc4d/', 'href': 'https://sherwinbahmani.github.io/tc4d/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ECCV 2024', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ECCV 2024', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '11ba016d-f263-498e-a669-058948397991'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '5cbe73f7-7a55-4251-80d4-2375865f5724', 'name': 'ECCV', 'color': 'yellow'}, {'id': '1a31dc62-0b3c-4bfe-8684-3ffe669325d3', 'name': 'Andrea', 'color': 'purple'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/pdf/2403.17920', 'link': {'url': 'https://arxiv.org/pdf/2403.17920'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/pdf/2403.17920', 'href': 'https://arxiv.org/pdf/2403.17920'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'TC4D: Trajectory-Conditioned Text-to-4D Generation', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'TC4D: Trajectory-Conditioned Text-to-4D Generation', 'href': None}]}}",https://www.notion.so/TC4D-Trajectory-Conditioned-Text-to-4D-Generation-9aaa39a47677461a99f2cad72b6456ea,https://yanxg.notion.site/TC4D-Trajectory-Conditioned-Text-to-4D-Generation-9aaa39a47677461a99f2cad72b6456ea
9,page,d1cb0fa9-86a7-497d-8cad-2779b3adc4ed,2024-07-22T23:28:00.000Z,2024-08-26T20:54:00.000Z,"{'object': 'user', 'id': 'aa05fbb4-2f8d-442b-b0ec-b01195bfd5e7'}","{'object': 'user', 'id': 'edfa4d94-358b-4482-91a3-67133bc15be8'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2024-08-26T20:54:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2024-09-29', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Mingrui Zhao, Yizhi Wang, Fenggen Yu, Changqing Zou, Ali Mahdavi-Amiri', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Mingrui Zhao, Yizhi Wang, Fenggen Yu, Changqing Zou, Ali Mahdavi-Amiri', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'edfa4d94-358b-4482-91a3-67133bc15be8'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Shape abstraction is an important task for simplifying complex geometric structures while retaining essential features. Sweep surfaces, commonly found in human-made objects, aid in this process by effectively capturing and representing object geometry, thereby facilitating abstraction. In this paper, we introduce \\papername, a novel approach to shape abstraction through sweep surfaces. We propose an effective parameterization for sweep surfaces, utilizing superellipses for profile representation and B-spline curves for the axis. This compact representation, requiring as few as 14 float numbers, facilitates intuitive and interactive editing while preserving shape details effectively. Additionally, by introducing a differentiable neural sweeper and an encoder-decoder architecture, we demonstrate the ability to predict sweep surface representations without supervision. We show the superiority of our model through several quantitative and qualitative experiments throughout the paper.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Shape abstraction is an important task for simplifying complex geometric structures while retaining essential features. Sweep surfaces, commonly found in human-made objects, aid in this process by effectively capturing and representing object geometry, thereby facilitating abstraction. In this paper, we introduce \\papername, a novel approach to shape abstraction through sweep surfaces. We propose an effective parameterization for sweep surfaces, utilizing superellipses for profile representation and B-spline curves for the axis. This compact representation, requiring as few as 14 float numbers, facilitates intuitive and interactive editing while preserving shape details effectively. Additionally, by introducing a differentiable neural sweeper and an encoder-decoder architecture, we demonstrate the ability to predict sweep surface representations without supervision. We show the superiority of our model through several quantitative and qualitative experiments throughout the paper.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'teaser_square.gif', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/b21eec20-addb-4738-8bdc-4a22ec9cc04b/teaser_square.gif?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234740Z&X-Amz-Expires=3600&X-Amz-Signature=bfd1d371be66a8c7ec137f2b00645259e23ae8c41d6acbfbf2f7cbf8cd448220&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:40.268Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://mingrui-zhao.github.io/SweepNet/', 'link': {'url': 'https://mingrui-zhao.github.io/SweepNet/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://mingrui-zhao.github.io/SweepNet/', 'href': 'https://mingrui-zhao.github.io/SweepNet/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ECCV 2024', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ECCV 2024', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'aa05fbb4-2f8d-442b-b0ec-b01195bfd5e7'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '5cbe73f7-7a55-4251-80d4-2375865f5724', 'name': 'ECCV', 'color': 'yellow'}, {'id': 'ed487f9e-f5ad-446d-89eb-2ab46ab0892c', 'name': 'Ali', 'color': 'purple'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2407.06305', 'link': {'url': 'https://arxiv.org/abs/2407.06305'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2407.06305', 'href': 'https://arxiv.org/abs/2407.06305'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'SweepNet: Unsupervised Learning Shape Abstraction via Neural Sweepers', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'SweepNet: Unsupervised Learning Shape Abstraction via Neural Sweepers', 'href': None}]}}",https://www.notion.so/SweepNet-Unsupervised-Learning-Shape-Abstraction-via-Neural-Sweepers-d1cb0fa986a7497d8cad2779b3adc4ed,https://yanxg.notion.site/SweepNet-Unsupervised-Learning-Shape-Abstraction-via-Neural-Sweepers-d1cb0fa986a7497d8cad2779b3adc4ed
10,page,ee6966d8-e673-41c5-a1d8-28740424aee9,2024-07-22T17:47:00.000Z,2024-07-22T17:54:00.000Z,"{'object': 'user', 'id': 'b507e305-8efa-443f-9b61-e893102d503a'}","{'object': 'user', 'id': 'b507e305-8efa-443f-9b61-e893102d503a'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2024-07-22T17:54:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2024-09-29', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Hongwei Yi, Justus Thies, Michael J. Black,\xa0Xue Bin Peng, Davis Rempe\n\n', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Hongwei Yi, Justus Thies, Michael J. Black,\xa0Xue Bin Peng, Davis Rempe\n\n', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'b507e305-8efa-443f-9b61-e893102d503a'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'We present TeSMo, a method for text-controlled scene-aware motion generation based on denoising diffusion models. Previous textto- motion methods focus on characters in isolation without considering scenes due to the limited availability of datasets that include motion, text descriptions, and interactive scenes. Our approach begins with pre-training a scene-agnostic text-to-motion diffusion model, emphasizing goal-reaching constraints on large-scale motion-capture datasets. We then enhance this model with a scene-aware component, fine-tuned using data augmented with detailed scene information, including ground plane and object shapes. To facilitate training, we embed annotated navigation and interaction motions within scenes. The proposed method produces realistic and diverse human-object interactions, such as navigation and sitting, in different scenes with various object shapes, orientations, initial body positions, and poses. Extensive experiments demonstrate that our approach surpasses prior techniques in terms of the plausibility of human-scene interactions, as well as the realism and variety of the generated motions.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'We present TeSMo, a method for text-controlled scene-aware motion generation based on denoising diffusion models. Previous textto- motion methods focus on characters in isolation without considering scenes due to the limited availability of datasets that include motion, text descriptions, and interactive scenes. Our approach begins with pre-training a scene-agnostic text-to-motion diffusion model, emphasizing goal-reaching constraints on large-scale motion-capture datasets. We then enhance this model with a scene-aware component, fine-tuned using data augmented with detailed scene information, including ground plane and object shapes. To facilitate training, we embed annotated navigation and interaction motions within scenes. The proposed method produces realistic and diverse human-object interactions, such as navigation and sitting, in different scenes with various object shapes, orientations, initial body positions, and poses. Extensive experiments demonstrate that our approach surpasses prior techniques in terms of the plausibility of human-scene interactions, as well as the realism and variety of the generated motions.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'TeSMo_thumb.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/4b44c156-0582-4de8-9099-910b7a1b2554/TeSMo_thumb.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234740Z&X-Amz-Expires=3600&X-Amz-Signature=bcb0fe7c0f09ccd6e6910dbd97a3f192fc526ca80468ef9a14c75aa1649224ea&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:40.269Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://xbpeng.github.io/projects/TeSMo/index.html', 'link': {'url': 'https://xbpeng.github.io/projects/TeSMo/index.html'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://xbpeng.github.io/projects/TeSMo/index.html', 'href': 'https://xbpeng.github.io/projects/TeSMo/index.html'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ECCV 2024', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ECCV 2024', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'b507e305-8efa-443f-9b61-e893102d503a'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://www.youtube.com/watch?v=_3e5LRh9jVc&ab_channel=hongweiyi', 'link': {'url': 'https://www.youtube.com/watch?v=_3e5LRh9jVc&ab_channel=hongweiyi'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://www.youtube.com/watch?v=_3e5LRh9jVc&ab_channel=hongweiyi', 'href': 'https://www.youtube.com/watch?v=_3e5LRh9jVc&ab_channel=hongweiyi'}]}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '5cbe73f7-7a55-4251-80d4-2375865f5724', 'name': 'ECCV', 'color': 'yellow'}, {'id': 'dc677421-36af-48e6-be53-0598670c619b', 'name': 'Jason', 'color': 'red'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://xbpeng.github.io/projects/TeSMo/TeSMo_2024.pdf', 'link': {'url': 'https://xbpeng.github.io/projects/TeSMo/TeSMo_2024.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://xbpeng.github.io/projects/TeSMo/TeSMo_2024.pdf', 'href': 'https://xbpeng.github.io/projects/TeSMo/TeSMo_2024.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Generating Human Interaction Motions in Scenes with Text Control', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Generating Human Interaction Motions in Scenes with Text Control', 'href': None}]}}",https://www.notion.so/Generating-Human-Interaction-Motions-in-Scenes-with-Text-Control-ee6966d8e67341c5a1d828740424aee9,https://yanxg.notion.site/Generating-Human-Interaction-Motions-in-Scenes-with-Text-Control-ee6966d8e67341c5a1d828740424aee9
11,page,4f14dee5-29c9-40e9-b342-4d389e8f6a8c,2024-07-22T15:56:00.000Z,2024-08-26T20:54:00.000Z,"{'object': 'user', 'id': 'edfa4d94-358b-4482-91a3-67133bc15be8'}","{'object': 'user', 'id': 'edfa4d94-358b-4482-91a3-67133bc15be8'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2024-08-26T20:54:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2024-09-29', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Qimin Chen, Zhiqin Chen, Vladimir G. Kim, Noam Aigerman, Hao Zhang, Siddhartha Chaudhuri', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Qimin Chen, Zhiqin Chen, Vladimir G. Kim, Noam Aigerman, Hao Zhang, Siddhartha Chaudhuri', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'edfa4d94-358b-4482-91a3-67133bc15be8'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'We present a 3D modeling method which enables end-usersto refine or detailize 3D shapes using machine learning, expanding the ca-pabilities of AI-assisted 3D content creation. Given a coarse voxel shape(e.g., one produced with a simple box extrusion tool or via generativemodeling), a user can directly “paint” desired target styles represent-ing compelling geometric details, from input exemplar shapes, over dif-ferent regions of the coarse shape. These regions are then up-sampledinto high-resolution geometries which adhere with the painted styles.To achieve such controllable and localized 3D detailization, we build ontop of a Pyramid GAN by making it masking-aware. We devise novelstructural losses and priors to ensure that our method preserves bothdesired coarse structures and fine-grained features even if the paintedstyles are borrowed from diverse sources, e.g., different semantic partsand even different shape categories. Through extensive experiments, weshow that our ability to localize details enables novel interactive creativeworkflows and applications. Our experiments further demonstrate that incomparison to prior techniques built on global detailization, our methodgenerates structure-preserving, high-resolution stylized geometries withmore coherent shape details and style transitions.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'We present a 3D modeling method which enables end-usersto refine or detailize 3D shapes using machine learning, expanding the ca-pabilities of AI-assisted 3D content creation. Given a coarse voxel shape(e.g., one produced with a simple box extrusion tool or via generativemodeling), a user can directly “paint” desired target styles represent-ing compelling geometric details, from input exemplar shapes, over dif-ferent regions of the coarse shape. These regions are then up-sampledinto high-resolution geometries which adhere with the painted styles.To achieve such controllable and localized 3D detailization, we build ontop of a Pyramid GAN by making it masking-aware. We devise novelstructural losses and priors to ensure that our method preserves bothdesired coarse structures and fine-grained features even if the paintedstyles are borrowed from diverse sources, e.g., different semantic partsand even different shape categories. Through extensive experiments, weshow that our ability to localize details enables novel interactive creativeworkflows and applications. Our experiments further demonstrate that incomparison to prior techniques built on global detailization, our methodgenerates structure-preserving, high-resolution stylized geometries withmore coherent shape details and style transitions.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'decollage.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/723098fe-86bb-4d52-81bf-1e94308f8a34/decollage.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234740Z&X-Amz-Expires=3600&X-Amz-Signature=4a94097f53200baecae1a199e75c3192a33c447ff5fe7f7830001cf2851ef1f6&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:40.269Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://qiminchen.github.io/decollage/', 'link': {'url': 'https://qiminchen.github.io/decollage/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://qiminchen.github.io/decollage/', 'href': 'https://qiminchen.github.io/decollage/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ECCV 2024', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ECCV 2024', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'edfa4d94-358b-4482-91a3-67133bc15be8'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '5cbe73f7-7a55-4251-80d4-2375865f5724', 'name': 'ECCV', 'color': 'yellow'}, {'id': 'e16d2c21-c87f-4a73-aa66-85bb6bc6e5b0', 'name': 'Richard', 'color': 'pink'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://qiminchen.github.io/decollage/', 'link': {'url': 'https://qiminchen.github.io/decollage/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://qiminchen.github.io/decollage/', 'href': 'https://qiminchen.github.io/decollage/'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'DECOLLAGE: 3D Detailization by Controllable,Localized, and Learned Geometry Enhancement', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'DECOLLAGE: 3D Detailization by Controllable,Localized, and Learned Geometry Enhancement', 'href': None}]}}",https://www.notion.so/DECOLLAGE-3D-Detailization-by-Controllable-Localized-and-Learned-Geometry-Enhancement-4f14dee529c940e9b3424d389e8f6a8c,https://yanxg.notion.site/DECOLLAGE-3D-Detailization-by-Controllable-Localized-and-Learned-Geometry-Enhancement-4f14dee529c940e9b3424d389e8f6a8c
12,page,e62454db-4641-4a35-8338-7897173c714d,2024-07-22T15:51:00.000Z,2024-07-22T15:55:00.000Z,"{'object': 'user', 'id': 'edfa4d94-358b-4482-91a3-67133bc15be8'}","{'object': 'user', 'id': 'edfa4d94-358b-4482-91a3-67133bc15be8'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2024-07-22T15:55:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2024-07-29', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Zhiqin Chen,\xa0Qimin Chen,\xa0Hang Zhou,\xa0Hao Zhang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Zhiqin Chen,\xa0Qimin Chen,\xa0Hang Zhou,\xa0Hao Zhang', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'edfa4d94-358b-4482-91a3-67133bc15be8'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'We present an unsupervised 3D shape co-segmentation method which learns a set of deformable part templates from a shape collection. To accommodate structural variations in the collection, our network composes each shape by a selected subset of template parts which are affine-transformed. To maximize the expressive power of the part templates, we introduce a per-part deformation network to enable the modeling of diverse parts with substantial geometry variations, while imposing constraints on the deformation capacity to ensure fidelity to the originally represented parts. We also propose a training scheme to effectively overcome local minima. Architecturally, our network is a branched autoencoder, with a CNN encoder taking a voxel shape as input and producing per-part transformation matrices, latent codes, and part existence scores, and the decoder outputting point occupancies to define the reconstruction loss. Our network, coined DAE-Net for Deforming Auto-Encoder, can achieve unsupervised 3D shape co-segmentation that yields fine-grained, compact, and meaningful parts that are consistent across diverse shapes. We conduct extensive experiments on the ShapeNet Part dataset, DFAUST, and an animal subset of Objaverse to show superior performance over prior methods.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'We present an unsupervised 3D shape co-segmentation method which learns a set of deformable part templates from a shape collection. To accommodate structural variations in the collection, our network composes each shape by a selected subset of template parts which are affine-transformed. To maximize the expressive power of the part templates, we introduce a per-part deformation network to enable the modeling of diverse parts with substantial geometry variations, while imposing constraints on the deformation capacity to ensure fidelity to the originally represented parts. We also propose a training scheme to effectively overcome local minima. Architecturally, our network is a branched autoencoder, with a CNN encoder taking a voxel shape as input and producing per-part transformation matrices, latent codes, and part existence scores, and the decoder outputting point occupancies to define the reconstruction loss. Our network, coined DAE-Net for Deforming Auto-Encoder, can achieve unsupervised 3D shape co-segmentation that yields fine-grained, compact, and meaningful parts that are consistent across diverse shapes. We conduct extensive experiments on the ShapeNet Part dataset, DFAUST, and an animal subset of Objaverse to show superior performance over prior methods.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'daenet.jpg', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/fb3c93de-07ba-4cb6-8f89-6c9440c5709b/daenet.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234740Z&X-Amz-Expires=3600&X-Amz-Signature=f95c3f8c4f6e195eaa3f041398cbf8647e9cd3a974f7414fada4ed2498a174b9&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:40.270Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://github.com/czq142857/DAE-Net', 'link': {'url': 'https://github.com/czq142857/DAE-Net'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://github.com/czq142857/DAE-Net', 'href': 'https://github.com/czq142857/DAE-Net'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'SIGGRAPH 2024', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'SIGGRAPH 2024', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'edfa4d94-358b-4482-91a3-67133bc15be8'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '45ac99f3-aeab-4f6c-9370-e80f8adbc8c8', 'name': 'SIGGRAPH', 'color': 'brown'}, {'id': 'e16d2c21-c87f-4a73-aa66-85bb6bc6e5b0', 'name': 'Richard', 'color': 'pink'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://github.com/czq142857/DAE-Net', 'link': {'url': 'https://github.com/czq142857/DAE-Net'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://github.com/czq142857/DAE-Net', 'href': 'https://github.com/czq142857/DAE-Net'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'DAE-Net: Deforming Auto-Encoder for fine-grained shape co-segmentation', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'DAE-Net: Deforming Auto-Encoder for fine-grained shape co-segmentation', 'href': None}]}}",https://www.notion.so/DAE-Net-Deforming-Auto-Encoder-for-fine-grained-shape-co-segmentation-e62454db46414a3583387897173c714d,https://yanxg.notion.site/DAE-Net-Deforming-Auto-Encoder-for-fine-grained-shape-co-segmentation-e62454db46414a3583387897173c714d
13,page,43f3383e-fb83-4f9f-a4fc-c5c9126c4032,2024-07-21T18:36:00.000Z,2024-10-01T20:12:00.000Z,"{'object': 'user', 'id': '752f8d93-6b11-44d7-80a5-0d790586a229'}","{'object': 'user', 'id': '752f8d93-6b11-44d7-80a5-0d790586a229'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2024-10-01T20:12:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2024-07-29', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'S. Mahdi H. Miangoleh, Mahesh Reddy, Yağız Aksoy', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'S. Mahdi H. Miangoleh, Mahesh Reddy, Yağız Aksoy', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '752f8d93-6b11-44d7-80a5-0d790586a229'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': ""Existing methods for scale-invariant monocular depth estimation (SI MDE) often struggle due to the complexity of the task, and limited and non-diverse datasets, hindering generalizability in real-world scenarios. This is while shift-and-scale-invariant (SSI) depth estimation, simplifying the task and enabling training with abundant stereo datasets achieves high performance. We present a novel approach that leverages SSI inputs to enhance SI depth estimation, streamlining the network's role and facilitating in-the-wild generalization for SI depth estimation while only using a synthetic dataset for training. Emphasizing the generation of high-resolution details, we introduce a novel sparse ordinal loss that substantially improves detail generation in SSI MDE, addressing critical limitations in existing approaches. Through in-the-wild qualitative examples and zero-shot evaluation we substantiate the practical utility of our approach in computational photography applications, showcasing its ability to generate highly detailed SI depth maps and achieve generalization in diverse scenarios."", 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': ""Existing methods for scale-invariant monocular depth estimation (SI MDE) often struggle due to the complexity of the task, and limited and non-diverse datasets, hindering generalizability in real-world scenarios. This is while shift-and-scale-invariant (SSI) depth estimation, simplifying the task and enabling training with abundant stereo datasets achieves high performance. We present a novel approach that leverages SSI inputs to enhance SI depth estimation, streamlining the network's role and facilitating in-the-wild generalization for SI depth estimation while only using a synthetic dataset for training. Emphasizing the generation of high-resolution details, we introduce a novel sparse ordinal loss that substantially improves detail generation in SSI MDE, addressing critical limitations in existing approaches. Through in-the-wild qualitative examples and zero-shot evaluation we substantiate the practical utility of our approach in computational photography applications, showcasing its ability to generate highly detailed SI depth maps and achieve generalization in diverse scenarios."", 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://yaksoy.github.io/images/research/sidepth.jpg', 'type': 'external', 'external': {'url': 'https://yaksoy.github.io/images/research/sidepth.jpg'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': []}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'SIGGRAPH 2024', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'SIGGRAPH 2024', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '752f8d93-6b11-44d7-80a5-0d790586a229'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '45ac99f3-aeab-4f6c-9370-e80f8adbc8c8', 'name': 'SIGGRAPH', 'color': 'brown'}, {'id': '3eb057d5-ed85-4850-b9ac-ff060c999382', 'name': 'Yagiz', 'color': 'gray'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': []}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Scale-Invariant Monocular Depth Estimation via SSI Depth', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Scale-Invariant Monocular Depth Estimation via SSI Depth', 'href': None}]}}",https://www.notion.so/Scale-Invariant-Monocular-Depth-Estimation-via-SSI-Depth-43f3383efb834f9fa4fcc5c9126c4032,https://yanxg.notion.site/Scale-Invariant-Monocular-Depth-Estimation-via-SSI-Depth-43f3383efb834f9fa4fcc5c9126c4032
14,page,e0c34b56-2086-4010-bed1-a468b37a9890,2024-07-24T00:15:00.000Z,2024-07-24T00:18:00.000Z,"{'object': 'user', 'id': '493c6f46-4677-4e9d-89ae-4f368ad116e8'}","{'object': 'user', 'id': '493c6f46-4677-4e9d-89ae-4f368ad116e8'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2024-07-24T00:18:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2024-07-28', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Xiang Xu, Joseph G. Lambourne, Pradeep Kumar Jayaraman, Zhengqing Wang, Karl D.D. Willis, Yasutaka Furukawa', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Xiang Xu, Joseph G. Lambourne, Pradeep Kumar Jayaraman, Zhengqing Wang, Karl D.D. Willis, Yasutaka Furukawa', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '493c6f46-4677-4e9d-89ae-4f368ad116e8'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'We present a diffusion-based generative approach that directly outputs a CAD B-rep. BrepGen uses a novel structured latent geometry to encode the CAD geometry and topology. A top-down generation approach is used to denoise the faces, edges, and vertices.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'We present a diffusion-based generative approach that directly outputs a CAD B-rep. BrepGen uses a novel structured latent geometry to encode the CAD geometry and topology. A top-down generation approach is used to denoise the faces, edges, and vertices.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'teaser.jpg', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/1b37ec37-b40f-4348-b27b-a3761273f451/teaser.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234740Z&X-Amz-Expires=3600&X-Amz-Signature=8d92adf0892a89748d4a1fbb7152d5d34473fb692fcce0a04f5b7dda185880df&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:40.270Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://github.com/samxuxiang/BrepGen', 'link': {'url': 'https://github.com/samxuxiang/BrepGen'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://github.com/samxuxiang/BrepGen', 'href': 'https://github.com/samxuxiang/BrepGen'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'SIGGRAPH 2024', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'SIGGRAPH 2024', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '493c6f46-4677-4e9d-89ae-4f368ad116e8'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '898c1b27-4fe4-4c46-8f37-c2c8cdfe0997', 'name': 'Yasu', 'color': 'red'}, {'id': '45ac99f3-aeab-4f6c-9370-e80f8adbc8c8', 'name': 'SIGGRAPH', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2401.15563', 'link': {'url': 'https://arxiv.org/abs/2401.15563'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2401.15563', 'href': 'https://arxiv.org/abs/2401.15563'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'BrepGen: A B-rep Generative Diffusion Model with Structured Latent Geometry', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'BrepGen: A B-rep Generative Diffusion Model with Structured Latent Geometry', 'href': None}]}}",https://www.notion.so/BrepGen-A-B-rep-Generative-Diffusion-Model-with-Structured-Latent-Geometry-e0c34b5620864010bed1a468b37a9890,https://yanxg.notion.site/BrepGen-A-B-rep-Generative-Diffusion-Model-with-Structured-Latent-Geometry-e0c34b5620864010bed1a468b37a9890
15,page,14b7e948-934b-444a-9150-5902e5708997,2024-07-22T17:54:00.000Z,2024-07-22T17:55:00.000Z,"{'object': 'user', 'id': 'b507e305-8efa-443f-9b61-e893102d503a'}","{'object': 'user', 'id': 'b507e305-8efa-443f-9b61-e893102d503a'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2024-07-22T17:55:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2024-07-28', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Yi Shi, Jingbo Wang, Xuekun Jiang, Bingkun Lin, Bo Dai,\xa0Xue Bin Peng', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Yi Shi, Jingbo Wang, Xuekun Jiang, Bingkun Lin, Bo Dai,\xa0Xue Bin Peng', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'b507e305-8efa-443f-9b61-e893102d503a'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Real-time character control is an essential component for interactive experiences, with a broad range of applications, including physics simulations, video games, and virtual reality. The success of diffusion models for image synthesis has led to the use of these models for motion synthesis. However, the majority of these motion diffusion models are primarily designed for offline applications, where space-time models are used to synthesize an entire sequence of frames simultaneously with a pre-specified length. To enable real-time motion synthesis with diffusion model that allows timevarying controls, we propose A-MDM (Auto-regressive Motion Diffusion Model). Our conditional diffusion model takes an initial pose as input, and auto-regressively generates successive motion frames conditioned on the previous frame. Despite its streamlined network architecture, which uses simple MLPs, our framework is capable of generating diverse, long-horizon, and high-fidelity motion sequences. Furthermore, we introduce a suite of techniques for incorporating interactive controls into A-MDM, such as taskoriented sampling, in-painting, and hierarchical reinforcement learning (See Figure 1). These techniques enable a pre-trained A-MDM to be efficiently adapted for a variety of new downstream tasks. We conduct a comprehensive suite of experiments to demonstrate the effectiveness of A-MDM, and compare its performance against state-of-the-art auto-regressive methods.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Real-time character control is an essential component for interactive experiences, with a broad range of applications, including physics simulations, video games, and virtual reality. The success of diffusion models for image synthesis has led to the use of these models for motion synthesis. However, the majority of these motion diffusion models are primarily designed for offline applications, where space-time models are used to synthesize an entire sequence of frames simultaneously with a pre-specified length. To enable real-time motion synthesis with diffusion model that allows timevarying controls, we propose A-MDM (Auto-regressive Motion Diffusion Model). Our conditional diffusion model takes an initial pose as input, and auto-regressively generates successive motion frames conditioned on the previous frame. Despite its streamlined network architecture, which uses simple MLPs, our framework is capable of generating diverse, long-horizon, and high-fidelity motion sequences. Furthermore, we introduce a suite of techniques for incorporating interactive controls into A-MDM, such as taskoriented sampling, in-painting, and hierarchical reinforcement learning (See Figure 1). These techniques enable a pre-trained A-MDM to be efficiently adapted for a variety of new downstream tasks. We conduct a comprehensive suite of experiments to demonstrate the effectiveness of A-MDM, and compare its performance against state-of-the-art auto-regressive methods.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'AMDM_thumb.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/c3ba7e17-f033-4326-aa4e-a9e34fd54a03/AMDM_thumb.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234740Z&X-Amz-Expires=3600&X-Amz-Signature=61163b396dcf2f645f91a34e733a62ddc388dfb5c8c45f14d4cf2d26a5733501&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:40.271Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://xbpeng.github.io/projects/AMDM/index.html', 'link': {'url': 'https://xbpeng.github.io/projects/AMDM/index.html'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://xbpeng.github.io/projects/AMDM/index.html', 'href': 'https://xbpeng.github.io/projects/AMDM/index.html'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'SIGGRAPH 2024', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'SIGGRAPH 2024', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'b507e305-8efa-443f-9b61-e893102d503a'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://www.youtube.com/watch?v=5WE9hy0xCI4&ab_channel=YISHI', 'link': {'url': 'https://www.youtube.com/watch?v=5WE9hy0xCI4&ab_channel=YISHI'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://www.youtube.com/watch?v=5WE9hy0xCI4&ab_channel=YISHI', 'href': 'https://www.youtube.com/watch?v=5WE9hy0xCI4&ab_channel=YISHI'}]}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '45ac99f3-aeab-4f6c-9370-e80f8adbc8c8', 'name': 'SIGGRAPH', 'color': 'brown'}, {'id': 'dc677421-36af-48e6-be53-0598670c619b', 'name': 'Jason', 'color': 'red'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://xbpeng.github.io/projects/AMDM/AMDM_2024.pdf', 'link': {'url': 'https://xbpeng.github.io/projects/AMDM/AMDM_2024.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://xbpeng.github.io/projects/AMDM/AMDM_2024.pdf', 'href': 'https://xbpeng.github.io/projects/AMDM/AMDM_2024.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Interactive Character Control with Auto-Regressive Motion Diffusion Models', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Interactive Character Control with Auto-Regressive Motion Diffusion Models', 'href': None}]}}",https://www.notion.so/Interactive-Character-Control-with-Auto-Regressive-Motion-Diffusion-Models-14b7e948934b444a91505902e5708997,https://yanxg.notion.site/Interactive-Character-Control-with-Auto-Regressive-Motion-Diffusion-Models-14b7e948934b444a91505902e5708997
16,page,4836c8b7-6d31-4d53-971e-926946372df4,2024-07-22T17:52:00.000Z,2024-07-22T17:54:00.000Z,"{'object': 'user', 'id': 'b507e305-8efa-443f-9b61-e893102d503a'}","{'object': 'user', 'id': 'b507e305-8efa-443f-9b61-e893102d503a'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2024-07-22T17:54:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2024-07-28', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Jordan Juravsky, Yunrong Guo, Sanja Fidler,\xa0Xue Bin Peng', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Jordan Juravsky, Yunrong Guo, Sanja Fidler,\xa0Xue Bin Peng', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'b507e305-8efa-443f-9b61-e893102d503a'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Physically-simulated models for human motion can generate highquality responsive character animations, often in real-time. Natural language serves as a flexible interface for controlling these models, allowing expert and non-expert users to quickly create and edit their animations. Many recent physics-based animation methods, including those that use text interfaces, train control policies using reinforcement learning (RL). However, scaling these methods beyond several hundred motions has remained challenging. Meanwhile, kinematic animation models are able to successfully learn from thousands of diverse motions by leveraging supervised learning methods. Inspired by these successes, in this work we introduce SuperPADL, a scalable framework for physics-based textto- motion that leverages both RL and supervised learning to train controllers on thousands of diverse motion clips. SuperPADL is trained in stages using progressive distillation, starting with a large number of specialized experts using RL. These experts are then iteratively distilled into larger, more robust policies using a combination of reinforcement learning and supervised learning. Our final SuperPADL controller is trained on a dataset containing over 5000 skills and runs in real time on a consumer GPU. Moreover, our policy can naturally transition between skills, allowing for users to interactively craft multi-stage animations. We experimentally demonstrate that SuperPADL significantly outperforms RL-based baselines at this large data scale.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Physically-simulated models for human motion can generate highquality responsive character animations, often in real-time. Natural language serves as a flexible interface for controlling these models, allowing expert and non-expert users to quickly create and edit their animations. Many recent physics-based animation methods, including those that use text interfaces, train control policies using reinforcement learning (RL). However, scaling these methods beyond several hundred motions has remained challenging. Meanwhile, kinematic animation models are able to successfully learn from thousands of diverse motions by leveraging supervised learning methods. Inspired by these successes, in this work we introduce SuperPADL, a scalable framework for physics-based textto- motion that leverages both RL and supervised learning to train controllers on thousands of diverse motion clips. SuperPADL is trained in stages using progressive distillation, starting with a large number of specialized experts using RL. These experts are then iteratively distilled into larger, more robust policies using a combination of reinforcement learning and supervised learning. Our final SuperPADL controller is trained on a dataset containing over 5000 skills and runs in real time on a consumer GPU. Moreover, our policy can naturally transition between skills, allowing for users to interactively craft multi-stage animations. We experimentally demonstrate that SuperPADL significantly outperforms RL-based baselines at this large data scale.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'SuperPADL_thumb.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/90f170aa-7d17-40af-858f-76f10eaf0d9b/SuperPADL_thumb.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234740Z&X-Amz-Expires=3600&X-Amz-Signature=541bf1038a632fe4cb91b79b2442d7fc121936c45ada843d918f5b8a4bf57dfc&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:40.271Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://xbpeng.github.io/projects/SuperPADL/index.html', 'link': {'url': 'https://xbpeng.github.io/projects/SuperPADL/index.html'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://xbpeng.github.io/projects/SuperPADL/index.html', 'href': 'https://xbpeng.github.io/projects/SuperPADL/index.html'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'SIGGRAPH 2024', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'SIGGRAPH 2024', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'b507e305-8efa-443f-9b61-e893102d503a'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://www.youtube.com/watch?v=0yXTX0FLXaE&embeds_referring_euri=https%3A%2F%2Fxbpeng.github.io%2F&source_ve_path=Mjg2NjY', 'link': {'url': 'https://www.youtube.com/watch?v=0yXTX0FLXaE&embeds_referring_euri=https%3A%2F%2Fxbpeng.github.io%2F&source_ve_path=Mjg2NjY'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://www.youtube.com/watch?v=0yXTX0FLXaE&embeds_referring_euri=https%3A%2F%2Fxbpeng.github.io%2F&source_ve_path=Mjg2NjY', 'href': 'https://www.youtube.com/watch?v=0yXTX0FLXaE&embeds_referring_euri=https%3A%2F%2Fxbpeng.github.io%2F&source_ve_path=Mjg2NjY'}]}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '45ac99f3-aeab-4f6c-9370-e80f8adbc8c8', 'name': 'SIGGRAPH', 'color': 'brown'}, {'id': 'dc677421-36af-48e6-be53-0598670c619b', 'name': 'Jason', 'color': 'red'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://xbpeng.github.io/projects/SuperPADL/SuperPADL_2024.pdf', 'link': {'url': 'https://xbpeng.github.io/projects/SuperPADL/SuperPADL_2024.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://xbpeng.github.io/projects/SuperPADL/SuperPADL_2024.pdf', 'href': 'https://xbpeng.github.io/projects/SuperPADL/SuperPADL_2024.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'SuperPADL: Scaling Language-Directed Physics-Based Control with Progressive Supervised Distillation', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'SuperPADL: Scaling Language-Directed Physics-Based Control with Progressive Supervised Distillation', 'href': None}]}}",https://www.notion.so/SuperPADL-Scaling-Language-Directed-Physics-Based-Control-with-Progressive-Supervised-Distillation-4836c8b76d314d53971e926946372df4,https://yanxg.notion.site/SuperPADL-Scaling-Language-Directed-Physics-Based-Control-with-Progressive-Supervised-Distillation-4836c8b76d314d53971e926946372df4
17,page,6db0bfdb-c3e5-452b-af2c-fed16ac5e9ec,2024-07-22T17:48:00.000Z,2024-07-22T17:54:00.000Z,"{'object': 'user', 'id': 'b507e305-8efa-443f-9b61-e893102d503a'}","{'object': 'user', 'id': 'b507e305-8efa-443f-9b61-e893102d503a'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2024-07-22T17:54:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2024-07-28', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Setareh Cohan, Guy Tevet, Daniele Reda,\xa0Xue Bin Peng, Michiel van de Panne\n\n', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Setareh Cohan, Guy Tevet, Daniele Reda,\xa0Xue Bin Peng, Michiel van de Panne\n\n', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'b507e305-8efa-443f-9b61-e893102d503a'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Motion in-betweening, a fundamental task in character animation, consists of generating motion sequences that plausibly interpolate user-provided keyframe constraints. It has long been recognized as a labor-intensive and challenging process. We investigate the potential of diffusion models in generating diverse human motions guided by keyframes. Unlike previous inbetweening methods, we propose a simple unified model capable of generating precise and diverse motions that conform to a flexible range of user-specified spatial constraints, as well as text conditioning. To this end, we propose Conditional Motion Diffusion In-betweening (CondMDI) which allows for arbitrary dense-or-sparse keyframe placement and partial keyframe constraints while generating high-quality motions that are diverse and coherent with the given keyframes.We evaluate the performance of CondMDI on the text-conditioned HumanML3D dataset and demonstrate the versatility and efficacy of diffusion models for keyframe in-betweening. We further explore the use of guidance and imputation-based approaches for inference-time keyframing and compare CondMDI against these methods.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Motion in-betweening, a fundamental task in character animation, consists of generating motion sequences that plausibly interpolate user-provided keyframe constraints. It has long been recognized as a labor-intensive and challenging process. We investigate the potential of diffusion models in generating diverse human motions guided by keyframes. Unlike previous inbetweening methods, we propose a simple unified model capable of generating precise and diverse motions that conform to a flexible range of user-specified spatial constraints, as well as text conditioning. To this end, we propose Conditional Motion Diffusion In-betweening (CondMDI) which allows for arbitrary dense-or-sparse keyframe placement and partial keyframe constraints while generating high-quality motions that are diverse and coherent with the given keyframes.We evaluate the performance of CondMDI on the text-conditioned HumanML3D dataset and demonstrate the versatility and efficacy of diffusion models for keyframe in-betweening. We further explore the use of guidance and imputation-based approaches for inference-time keyframing and compare CondMDI against these methods.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'CondMDI_thumb.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/df4bba4a-ba0e-4ef0-a0ae-f0f0d344356a/CondMDI_thumb.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234740Z&X-Amz-Expires=3600&X-Amz-Signature=4ce9c1a5a9c278f8f0820fb7872243d71aa075edf60e573f3a281eae8c4d9fe2&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:40.271Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://xbpeng.github.io/projects/CondMDI/index.html', 'link': {'url': 'https://xbpeng.github.io/projects/CondMDI/index.html'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://xbpeng.github.io/projects/CondMDI/index.html', 'href': 'https://xbpeng.github.io/projects/CondMDI/index.html'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'SIGGRAPH 2024', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'SIGGRAPH 2024', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'b507e305-8efa-443f-9b61-e893102d503a'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://www.youtube.com/watch?v=rRgeOXOVzGQ&ab_channel=SetarehCohan', 'link': {'url': 'https://www.youtube.com/watch?v=rRgeOXOVzGQ&ab_channel=SetarehCohan'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://www.youtube.com/watch?v=rRgeOXOVzGQ&ab_channel=SetarehCohan', 'href': 'https://www.youtube.com/watch?v=rRgeOXOVzGQ&ab_channel=SetarehCohan'}]}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '45ac99f3-aeab-4f6c-9370-e80f8adbc8c8', 'name': 'SIGGRAPH', 'color': 'brown'}, {'id': 'dc677421-36af-48e6-be53-0598670c619b', 'name': 'Jason', 'color': 'red'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://xbpeng.github.io/projects/CondMDI/CondMDI_2024.pdf', 'link': {'url': 'https://xbpeng.github.io/projects/CondMDI/CondMDI_2024.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://xbpeng.github.io/projects/CondMDI/CondMDI_2024.pdf', 'href': 'https://xbpeng.github.io/projects/CondMDI/CondMDI_2024.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Flexible Motion In-betweening with Diffusion Models', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Flexible Motion In-betweening with Diffusion Models', 'href': None}]}}",https://www.notion.so/Flexible-Motion-In-betweening-with-Diffusion-Models-6db0bfdbc3e5452baf2cfed16ac5e9ec,https://yanxg.notion.site/Flexible-Motion-In-betweening-with-Diffusion-Models-6db0bfdbc3e5452baf2cfed16ac5e9ec
18,page,f7d57491-6c51-485c-8069-972ec9d3a8cf,2024-07-23T01:40:00.000Z,2024-07-23T01:42:00.000Z,"{'object': 'user', 'id': '11ba016d-f263-498e-a669-058948397991'}","{'object': 'user', 'id': '11ba016d-f263-498e-a669-058948397991'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2024-07-23T01:42:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2024-07-22', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Daniel Rebain, Soroosh Yazdani, Kwang Moo Yi, Andrea Tagliasacchi', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Daniel Rebain, Soroosh Yazdani, Kwang Moo Yi, Andrea Tagliasacchi', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '11ba016d-f263-498e-a669-058948397991'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Neural fields have emerged as a powerful and broadly applicable method for representing signals. However, in contrast to classical discrete digital signal processing, the portfolio of tools to process such representations is still severely limited and restricted to Euclidean domains.In this paper, we address this problem by showing how a probabilistic re-interpretation of neural fields can enable their training and inference processes to become ""filter-aware"". The formulation we propose not only merges training and filtering in an efficient way, but also generalizes beyond the familiar Euclidean coordinate spaces to the more general set of smooth manifolds and convolutions induced by the actions of Lie groups. We demonstrate how this framework can enable novel integrations of signal processing techniques for neural field applications on both Euclidean domains, such as images and audio, as well as non-Euclidean domains, such as rotations and rays. A noteworthy benefit of our method is its applicability. Our method can be summarized as primarily a modification of the loss function, and in most cases does not require changes to the network architecture or the inference process.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Neural fields have emerged as a powerful and broadly applicable method for representing signals. However, in contrast to classical discrete digital signal processing, the portfolio of tools to process such representations is still severely limited and restricted to Euclidean domains.In this paper, we address this problem by showing how a probabilistic re-interpretation of neural fields can enable their training and inference processes to become ""filter-aware"". The formulation we propose not only merges training and filtering in an efficient way, but also generalizes beyond the familiar Euclidean coordinate spaces to the more general set of smooth manifolds and convolutions induced by the actions of Lie groups. We demonstrate how this framework can enable novel integrations of signal processing techniques for neural field applications on both Euclidean domains, such as images and audio, as well as non-Euclidean domains, such as rotations and rays. A noteworthy benefit of our method is its applicability. Our method can be summarized as primarily a modification of the loss function, and in most cases does not require changes to the network architecture or the inference process.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'Screenshot 2024-07-22 at 6.41.23\u202fPM.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/1728f773-541e-4f40-be90-e4a042133883/Screenshot_2024-07-22_at_6.41.23_PM.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234740Z&X-Amz-Expires=3600&X-Amz-Signature=c7b96f3fbc2297b5510b23c7a6a1cfbd7afb510db63538121c836f728498648b&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:40.272Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://ubc-vision.github.io/nfd/', 'link': {'url': 'https://ubc-vision.github.io/nfd/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://ubc-vision.github.io/nfd/', 'href': 'https://ubc-vision.github.io/nfd/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2024', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2024', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '11ba016d-f263-498e-a669-058948397991'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '1a31dc62-0b3c-4bfe-8684-3ffe669325d3', 'name': 'Andrea', 'color': 'purple'}, {'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://drive.google.com/file/d/14r6oEWZUkXOEO0dnTTpb8gjtpmIc1f1p/view?usp=sharing', 'link': {'url': 'https://drive.google.com/file/d/14r6oEWZUkXOEO0dnTTpb8gjtpmIc1f1p/view?usp=sharing'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://drive.google.com/file/d/14r6oEWZUkXOEO0dnTTpb8gjtpmIc1f1p/view?usp=sharing', 'href': 'https://drive.google.com/file/d/14r6oEWZUkXOEO0dnTTpb8gjtpmIc1f1p/view?usp=sharing'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Neural Fields as Distributions: Signal Processing Beyond Euclidean Space', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Neural Fields as Distributions: Signal Processing Beyond Euclidean Space', 'href': None}]}}",https://www.notion.so/Neural-Fields-as-Distributions-Signal-Processing-Beyond-Euclidean-Space-f7d574916c51485c8069972ec9d3a8cf,https://yanxg.notion.site/Neural-Fields-as-Distributions-Signal-Processing-Beyond-Euclidean-Space-f7d574916c51485c8069972ec9d3a8cf
19,page,5c4bb6dc-8a2b-4ba0-a9b2-186e7066f7e5,2024-07-23T01:38:00.000Z,2024-07-23T01:40:00.000Z,"{'object': 'user', 'id': '11ba016d-f263-498e-a669-058948397991'}","{'object': 'user', 'id': '11ba016d-f263-498e-a669-058948397991'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2024-07-23T01:40:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2024-07-22', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Lily Goli, Cody Reading, Silvia Sellan, Alec Jacobson, Andrea Tagliasacchi', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Lily Goli, Cody Reading, Silvia Sellan, Alec Jacobson, Andrea Tagliasacchi', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '11ba016d-f263-498e-a669-058948397991'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Neural Radiance Fields (NeRFs) have shown promise in applications like view synthesis and depth estimation, but their learning from multiview images faces inherent uncertainties. Current methods to quantify them are either heuristic or computationally demanding. We introduce\xa0', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Neural Radiance Fields (NeRFs) have shown promise in applications like view synthesis and depth estimation, but their learning from multiview images faces inherent uncertainties. Current methods to quantify them are either heuristic or computationally demanding. We introduce\xa0', 'href': None}, {'type': 'text', 'text': {'content': 'BaysRays', 'link': None}, 'annotations': {'bold': False, 'italic': True, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'BaysRays', 'href': None}, {'type': 'text', 'text': {'content': ', a post-hoc framework to evaluate uncertainty in any pre-trained NeRF without modifying the training process. Our method establishes a volumetric uncertainty field using spatial perturbations and a Bayesian Laplace approximation. We derive our algorithm statistically and highlight its superior performance in key metrics and NeRF-related applications.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': ', a post-hoc framework to evaluate uncertainty in any pre-trained NeRF without modifying the training process. Our method establishes a volumetric uncertainty field using spatial perturbations and a Bayesian Laplace approximation. We derive our algorithm statistically and highlight its superior performance in key metrics and NeRF-related applications.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'Screenshot 2024-07-22 at 6.39.51\u202fPM.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/51452dad-d156-484f-b88e-996c0d009bf6/Screenshot_2024-07-22_at_6.39.51_PM.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234740Z&X-Amz-Expires=3600&X-Amz-Signature=ca2b31f717b7489a64f57654d3b45349a207d928e5a8bf5267837cdd3ba19d37&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:40.272Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://bayesrays.github.io', 'link': {'url': 'https://bayesrays.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://bayesrays.github.io', 'href': 'https://bayesrays.github.io/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2024', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2024', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '11ba016d-f263-498e-a669-058948397991'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '1a31dc62-0b3c-4bfe-8684-3ffe669325d3', 'name': 'Andrea', 'color': 'purple'}, {'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://github.com/BayesRays/bayesrays.github.io/raw/main/BayesRays.pdf', 'link': {'url': 'https://github.com/BayesRays/bayesrays.github.io/raw/main/BayesRays.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://github.com/BayesRays/bayesrays.github.io/raw/main/BayesRays.pdf', 'href': 'https://github.com/BayesRays/bayesrays.github.io/raw/main/BayesRays.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': ""Bayes' Rays: Uncertainty Quantification for Neural Radiance Fields"", 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': ""Bayes' Rays: Uncertainty Quantification for Neural Radiance Fields"", 'href': None}]}}",https://www.notion.so/Bayes-Rays-Uncertainty-Quantification-for-Neural-Radiance-Fields-5c4bb6dc8a2b4ba0a9b2186e7066f7e5,https://yanxg.notion.site/Bayes-Rays-Uncertainty-Quantification-for-Neural-Radiance-Fields-5c4bb6dc8a2b4ba0a9b2186e7066f7e5
20,page,dc72a115-9111-4c0a-8dcb-777868c07d09,2024-07-23T01:37:00.000Z,2024-07-23T01:38:00.000Z,"{'object': 'user', 'id': '11ba016d-f263-498e-a669-058948397991'}","{'object': 'user', 'id': '11ba016d-f263-498e-a669-058948397991'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2024-07-23T01:38:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2024-07-22', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Shakiba Kheradmand, Daniel Rebain, Gopal Sharma, Hossam Isack, Abhishek Kar, Andrea Tagliasacchi, Kwang Moo Yi', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Shakiba Kheradmand, Daniel Rebain, Gopal Sharma, Hossam Isack, Abhishek Kar, Andrea Tagliasacchi, Kwang Moo Yi', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '11ba016d-f263-498e-a669-058948397991'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'We present an approach to accelerate Neural Field training by efficiently selecting sampling locations. While Neural Fields have recently become popular, it is often trained by uniformly sampling the training domain, or through handcrafted heuristics. We show that improved convergence and final training quality can be achieved by a soft mining technique based on importance sampling: rather than either considering or ignoring a pixel completely, we weigh the corresponding loss by a scalar. To implement our idea we use Langevin Monte-Carlo sampling. We show that by doing so, regions with higher error are being selected more frequently, leading to more than 2x improvement in convergence speed.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'We present an approach to accelerate Neural Field training by efficiently selecting sampling locations. While Neural Fields have recently become popular, it is often trained by uniformly sampling the training domain, or through handcrafted heuristics. We show that improved convergence and final training quality can be achieved by a soft mining technique based on importance sampling: rather than either considering or ignoring a pixel completely, we weigh the corresponding loss by a scalar. To implement our idea we use Langevin Monte-Carlo sampling. We show that by doing so, regions with higher error are being selected more frequently, leading to more than 2x improvement in convergence speed.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'Screenshot 2024-07-22 at 6.38.13\u202fPM.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/1d719c12-4254-42b2-9617-24e099d25684/Screenshot_2024-07-22_at_6.38.13_PM.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234740Z&X-Amz-Expires=3600&X-Amz-Signature=5a721a257043f22175362d31abd2a26f07fce3c65cfe844f16dc92dff64e65b8&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:40.273Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://ubc-vision.github.io/nf-soft-mining/', 'link': {'url': 'https://ubc-vision.github.io/nf-soft-mining/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://ubc-vision.github.io/nf-soft-mining/', 'href': 'https://ubc-vision.github.io/nf-soft-mining/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2024', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2024', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '11ba016d-f263-498e-a669-058948397991'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '1a31dc62-0b3c-4bfe-8684-3ffe669325d3', 'name': 'Andrea', 'color': 'purple'}, {'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/pdf/2312.00075', 'link': {'url': 'https://arxiv.org/pdf/2312.00075'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/pdf/2312.00075', 'href': 'https://arxiv.org/pdf/2312.00075'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Accelerating Neural Field Training via Soft Mining', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Accelerating Neural Field Training via Soft Mining', 'href': None}]}}",https://www.notion.so/Accelerating-Neural-Field-Training-via-Soft-Mining-dc72a11591114c0a8dcb777868c07d09,https://yanxg.notion.site/Accelerating-Neural-Field-Training-via-Soft-Mining-dc72a11591114c0a8dcb777868c07d09
21,page,4f8c1c58-b6c1-4a8f-9bd7-a72ffa3d30b6,2024-07-23T01:36:00.000Z,2024-07-23T01:37:00.000Z,"{'object': 'user', 'id': '11ba016d-f263-498e-a669-058948397991'}","{'object': 'user', 'id': '11ba016d-f263-498e-a669-058948397991'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2024-07-23T01:37:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2024-07-22', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Eric Hedlin, Gopal Sharma, Shweta Mahajan, Hossam Isack, Abhishek Kar, Helge Rhodin, Andrea Tagliasacchi, Kwang Moo Yi', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Eric Hedlin, Gopal Sharma, Shweta Mahajan, Hossam Isack, Abhishek Kar, Helge Rhodin, Andrea Tagliasacchi, Kwang Moo Yi', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '11ba016d-f263-498e-a669-058948397991'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Unsupervised learning of keypoints and landmarks has seen significant progress with the help of modern neural network architectures, but performance is yet to match the supervised counterpart, making their practicability questionable. We leverage the emergent knowledge within text-to-image diffusion models, towards more robust unsupervised keypoints. Our core idea is to find text embeddings that would cause the generative model to consistently attend to compact regions in images (i.e. keypoints). To do so, we simply optimize the text embedding such that the cross-attention maps within the denoising network are localized as Gaussians with small standard deviations. We validate our performance on multiple dataset: the CelebA, CUB-200-2011, Tai-Chi-HD, DeepFashion, and Human3.6m datasets. We achieve significantly improved accuracy, sometimes even outperforming supervised ones, particularly for data that is non-aligned and less curated.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Unsupervised learning of keypoints and landmarks has seen significant progress with the help of modern neural network architectures, but performance is yet to match the supervised counterpart, making their practicability questionable. We leverage the emergent knowledge within text-to-image diffusion models, towards more robust unsupervised keypoints. Our core idea is to find text embeddings that would cause the generative model to consistently attend to compact regions in images (i.e. keypoints). To do so, we simply optimize the text embedding such that the cross-attention maps within the denoising network are localized as Gaussians with small standard deviations. We validate our performance on multiple dataset: the CelebA, CUB-200-2011, Tai-Chi-HD, DeepFashion, and Human3.6m datasets. We achieve significantly improved accuracy, sometimes even outperforming supervised ones, particularly for data that is non-aligned and less curated.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'Screenshot 2024-07-22 at 6.36.49\u202fPM.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/20dc0ad8-8931-486f-a8c1-8eacce400885/Screenshot_2024-07-22_at_6.36.49_PM.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234740Z&X-Amz-Expires=3600&X-Amz-Signature=693a654b0e22c3e0709a4514253537a08f3b4b22cb8c4159b6a4ea72bce02bcf&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:40.273Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://ubc-vision.github.io/StableKeypoints/', 'link': {'url': 'https://ubc-vision.github.io/StableKeypoints/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://ubc-vision.github.io/StableKeypoints/', 'href': 'https://ubc-vision.github.io/StableKeypoints/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2024', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2024', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '11ba016d-f263-498e-a669-058948397991'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '1a31dc62-0b3c-4bfe-8684-3ffe669325d3', 'name': 'Andrea', 'color': 'purple'}, {'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/pdf/2312.00065', 'link': {'url': 'https://arxiv.org/pdf/2312.00065'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/pdf/2312.00065', 'href': 'https://arxiv.org/pdf/2312.00065'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Unsupervised Keypoints from Pretrained Diffusion Models', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Unsupervised Keypoints from Pretrained Diffusion Models', 'href': None}]}}",https://www.notion.so/Unsupervised-Keypoints-from-Pretrained-Diffusion-Models-4f8c1c58b6c14a8f9bd7a72ffa3d30b6,https://yanxg.notion.site/Unsupervised-Keypoints-from-Pretrained-Diffusion-Models-4f8c1c58b6c14a8f9bd7a72ffa3d30b6
22,page,0ebb847f-958b-4f88-93d6-321b31322c16,2024-07-23T01:34:00.000Z,2024-07-23T01:35:00.000Z,"{'object': 'user', 'id': '11ba016d-f263-498e-a669-058948397991'}","{'object': 'user', 'id': '11ba016d-f263-498e-a669-058948397991'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2024-07-23T01:35:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2024-07-22', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Ahan Shabanov, Shrisudhan Govindarajan, Cody Reading, Daniel Rebain, Kwang Moo Yi, Andrea Tagliasacchi', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Ahan Shabanov, Shrisudhan Govindarajan, Cody Reading, Daniel Rebain, Kwang Moo Yi, Andrea Tagliasacchi', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '11ba016d-f263-498e-a669-058948397991'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Largely due to their implicit nature, neural fields lack a direct mechanism for filtering, as Fourier analysis from discrete signal processing is not directly applicable to these representations. Effective filtering of neural fields is critical to enable level-of-detail processing in downstream applications, and support operations that involve sampling the field on regular grids (e.g. marching cubes). Existing methods that attempt to decompose neural fields in the frequency domain either resort to heuristics or require extensive modifications to the neural field architecture. We show that via a simple modification, one can obtain neural fields that are low-pass filtered, and in turn show how this can be exploited to obtain a frequency decomposition of the entire signal. We demonstrate the validity of our technique by investigating level-of-detail reconstruction, and showing how coarser representations can be computed effectively.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Largely due to their implicit nature, neural fields lack a direct mechanism for filtering, as Fourier analysis from discrete signal processing is not directly applicable to these representations. Effective filtering of neural fields is critical to enable level-of-detail processing in downstream applications, and support operations that involve sampling the field on regular grids (e.g. marching cubes). Existing methods that attempt to decompose neural fields in the frequency domain either resort to heuristics or require extensive modifications to the neural field architecture. We show that via a simple modification, one can obtain neural fields that are low-pass filtered, and in turn show how this can be exploited to obtain a frequency decomposition of the entire signal. We demonstrate the validity of our technique by investigating level-of-detail reconstruction, and showing how coarser representations can be computed effectively.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'Screenshot 2024-07-22 at 6.35.07\u202fPM.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/8e28041a-b60d-488c-991c-e22d5a3f370b/Screenshot_2024-07-22_at_6.35.07_PM.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234740Z&X-Amz-Expires=3600&X-Amz-Signature=34181319bd7e1cbb4f02f4984fe169e5eb5f7b032760565068c655da579ffc13&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:40.273Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://theialab.github.io/banf/', 'link': {'url': 'https://theialab.github.io/banf/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://theialab.github.io/banf/', 'href': 'https://theialab.github.io/banf/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2024', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2024', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '11ba016d-f263-498e-a669-058948397991'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '1a31dc62-0b3c-4bfe-8684-3ffe669325d3', 'name': 'Andrea', 'color': 'purple'}, {'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/pdf/2404.13024', 'link': {'url': 'https://arxiv.org/pdf/2404.13024'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/pdf/2404.13024', 'href': 'https://arxiv.org/pdf/2404.13024'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'BANF: Band-limited Neural Fields for Levels of Detail Reconstruction', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'BANF: Band-limited Neural Fields for Levels of Detail Reconstruction', 'href': None}]}}",https://www.notion.so/BANF-Band-limited-Neural-Fields-for-Levels-of-Detail-Reconstruction-0ebb847f958b4f8893d6321b31322c16,https://yanxg.notion.site/BANF-Band-limited-Neural-Fields-for-Levels-of-Detail-Reconstruction-0ebb847f958b4f8893d6321b31322c16
23,page,3aa2f28b-15da-4a3b-ac5d-cb52a153b3f2,2024-07-23T01:32:00.000Z,2024-07-23T01:34:00.000Z,"{'object': 'user', 'id': '11ba016d-f263-498e-a669-058948397991'}","{'object': 'user', 'id': '11ba016d-f263-498e-a669-058948397991'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2024-07-23T01:34:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2024-07-22', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'David Charatan, Sizhe Li, Andrea Tagliasacchi, Vincent Sitzmann', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'David Charatan, Sizhe Li, Andrea Tagliasacchi, Vincent Sitzmann', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '11ba016d-f263-498e-a669-058948397991'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'We introduce pixelSplat, a feed-forward model that learns to reconstruct 3D radiance fields parameterized by 3D Gaussian primitives from pairs of images. Our model features real-time and memory-efficient rendering for scalable training as well as fast 3D reconstruction at inference time. To overcome local minima inherent to sparse and locally supported representations, we predict a dense probability distribution over 3D and sample Gaussian means from that probability distribution. We make this sampling operation differentiable via a reparameterization trick, allowing us to back-propagate gradients through the Gaussian splatting representation. We benchmark our method on wide-baseline novel view synthesis on the real-world RealEstate10k and ACID datasets, where we outperform state-of-the-art light field transformers and accelerate rendering by 2.5 orders of magnitude while reconstructing an interpretable and editable 3D radiance field.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'We introduce pixelSplat, a feed-forward model that learns to reconstruct 3D radiance fields parameterized by 3D Gaussian primitives from pairs of images. Our model features real-time and memory-efficient rendering for scalable training as well as fast 3D reconstruction at inference time. To overcome local minima inherent to sparse and locally supported representations, we predict a dense probability distribution over 3D and sample Gaussian means from that probability distribution. We make this sampling operation differentiable via a reparameterization trick, allowing us to back-propagate gradients through the Gaussian splatting representation. We benchmark our method on wide-baseline novel view synthesis on the real-world RealEstate10k and ACID datasets, where we outperform state-of-the-art light field transformers and accelerate rendering by 2.5 orders of magnitude while reconstructing an interpretable and editable 3D radiance field.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'Screenshot 2024-07-22 at 6.33.30\u202fPM.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/c94c2ba7-3514-45a2-b774-60d2420d6062/Screenshot_2024-07-22_at_6.33.30_PM.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234740Z&X-Amz-Expires=3600&X-Amz-Signature=0362d0a157dd5adb8e7bbd09b158dfd88d495abe7d485d060121b01fbed22cb4&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:40.274Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://davidcharatan.com/pixelsplat/', 'link': {'url': 'https://davidcharatan.com/pixelsplat/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://davidcharatan.com/pixelsplat/', 'href': 'https://davidcharatan.com/pixelsplat/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2024', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2024', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '11ba016d-f263-498e-a669-058948397991'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '1a31dc62-0b3c-4bfe-8684-3ffe669325d3', 'name': 'Andrea', 'color': 'purple'}, {'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/pdf/2312.12337', 'link': {'url': 'https://arxiv.org/pdf/2312.12337'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/pdf/2312.12337', 'href': 'https://arxiv.org/pdf/2312.12337'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'pixelSplat: 3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'pixelSplat: 3D Gaussian Splats from Image Pairs for Scalable Generalizable 3D Reconstruction', 'href': None}]}}",https://www.notion.so/pixelSplat-3D-Gaussian-Splats-from-Image-Pairs-for-Scalable-Generalizable-3D-Reconstruction-3aa2f28b15da4a3bac5dcb52a153b3f2,https://yanxg.notion.site/pixelSplat-3D-Gaussian-Splats-from-Image-Pairs-for-Scalable-Generalizable-3D-Reconstruction-3aa2f28b15da4a3bac5dcb52a153b3f2
24,page,da144cba-bbda-4cf4-96f8-9d5d8b8165df,2024-07-23T01:30:00.000Z,2024-07-23T01:32:00.000Z,"{'object': 'user', 'id': '11ba016d-f263-498e-a669-058948397991'}","{'object': 'user', 'id': '11ba016d-f263-498e-a669-058948397991'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2024-07-23T01:32:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2024-07-22', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Sherwin Bahmani, Ivan Skorokhodov, Victor Rong, Gordon Wetzstein, Leonidas Guibas, Peter Wonka, Sergey Tulyakov, Jeong Joon Park, Andrea Tagliasacchi, David Lindell', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Sherwin Bahmani, Ivan Skorokhodov, Victor Rong, Gordon Wetzstein, Leonidas Guibas, Peter Wonka, Sergey Tulyakov, Jeong Joon Park, Andrea Tagliasacchi, David Lindell', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '11ba016d-f263-498e-a669-058948397991'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Recent breakthroughs in text-to-4D generation rely on pre-trained text-to-image and text-to-video models to generate dynamic 3D scenes. However, current text-to-4D methods face a three-way tradeoff between the quality of scene appearance, 3D structure, and motion. For example, text-to-image models and their 3D-aware variants are trained on internet-scale image datasets and can be used to produce scenes with realistic appearance and 3D structure-but no motion. Text-to-video models are trained on relatively smaller video datasets and can produce scenes with motion, but poorer appearance and 3D structure. While these models have complementary strengths, they also have opposing weaknesses, making it difficult to combine them in a way that alleviates this three-way tradeoff. Here, we introduce hybrid score distillation sampling, an alternating optimization procedure that blends supervision signals from multiple pre-trained diffusion models and incorporates benefits of each for high-fidelity text-to-4D generation. Using hybrid SDS, we demonstrate synthesis of 4D scenes with compelling appearance, 3D structure, and motion.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Recent breakthroughs in text-to-4D generation rely on pre-trained text-to-image and text-to-video models to generate dynamic 3D scenes. However, current text-to-4D methods face a three-way tradeoff between the quality of scene appearance, 3D structure, and motion. For example, text-to-image models and their 3D-aware variants are trained on internet-scale image datasets and can be used to produce scenes with realistic appearance and 3D structure-but no motion. Text-to-video models are trained on relatively smaller video datasets and can produce scenes with motion, but poorer appearance and 3D structure. While these models have complementary strengths, they also have opposing weaknesses, making it difficult to combine them in a way that alleviates this three-way tradeoff. Here, we introduce hybrid score distillation sampling, an alternating optimization procedure that blends supervision signals from multiple pre-trained diffusion models and incorporates benefits of each for high-fidelity text-to-4D generation. Using hybrid SDS, we demonstrate synthesis of 4D scenes with compelling appearance, 3D structure, and motion.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'Screenshot 2024-07-22 at 6.31.48\u202fPM.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/60b7106a-1fac-400b-89fa-f14005541361/Screenshot_2024-07-22_at_6.31.48_PM.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234740Z&X-Amz-Expires=3600&X-Amz-Signature=a5d97dadac5779d7b514faabfc94d1c049d4d4ffe7a8de0df989dfef213f550e&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:40.275Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://sherwinbahmani.github.io/4dfy/', 'link': {'url': 'https://sherwinbahmani.github.io/4dfy/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://sherwinbahmani.github.io/4dfy/', 'href': 'https://sherwinbahmani.github.io/4dfy/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2024', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2024', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '11ba016d-f263-498e-a669-058948397991'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '1a31dc62-0b3c-4bfe-8684-3ffe669325d3', 'name': 'Andrea', 'color': 'purple'}, {'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/pdf/2311.17984', 'link': {'url': 'https://arxiv.org/pdf/2311.17984'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/pdf/2311.17984', 'href': 'https://arxiv.org/pdf/2311.17984'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': '4D-fy: Text-to-4D Generation Using Hybrid Score Distillation Sampling', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': '4D-fy: Text-to-4D Generation Using Hybrid Score Distillation Sampling', 'href': None}]}}",https://www.notion.so/4D-fy-Text-to-4D-Generation-Using-Hybrid-Score-Distillation-Sampling-da144cbabbda4cf496f89d5d8b8165df,https://yanxg.notion.site/4D-fy-Text-to-4D-Generation-Using-Hybrid-Score-Distillation-Sampling-da144cbabbda4cf496f89d5d8b8165df
25,page,361106eb-cb44-4889-9139-c7fb690a0d46,2024-07-22T23:23:00.000Z,2024-07-22T23:29:00.000Z,"{'object': 'user', 'id': '89330a72-bcbd-42e7-9879-4d3e144ed6ec'}","{'object': 'user', 'id': '89330a72-bcbd-42e7-9879-4d3e144ed6ec'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2024-07-22T23:29:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2024-07-22', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Jiayi Liu, Hou In Ivan Tam, Ali Mahdavi-Amiri, Manolis Savva', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Jiayi Liu, Hou In Ivan Tam, Ali Mahdavi-Amiri, Manolis Savva', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '89330a72-bcbd-42e7-9879-4d3e144ed6ec'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': ""We address the challenge of generating 3D articulated objects in a controllable fashion. Currently, modeling articulated 3D objects is either achieved through laborious manual authoring, or using methods from prior work that are hard to scale and control directly.\n\nWe leverage the interplay between part shape, connectivity, and motion using a denoising diffusion-based method with attention modules designed to extract correlations between part attributes. Our method takes an object category label and a part connectivity graph as input and generates an object's geometry and motion parameters. The generated objects conform to user-specified constraints on the object category, part shape, and part articulation.\n\nOur experiments show that our method outperforms the state-of-the-art in articulated object generation, producing more realistic objects while conforming better to user constraints."", 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': ""We address the challenge of generating 3D articulated objects in a controllable fashion. Currently, modeling articulated 3D objects is either achieved through laborious manual authoring, or using methods from prior work that are hard to scale and control directly.\n\nWe leverage the interplay between part shape, connectivity, and motion using a denoising diffusion-based method with attention modules designed to extract correlations between part attributes. Our method takes an object category label and a part connectivity graph as input and generates an object's geometry and motion parameters. The generated objects conform to user-specified constraints on the object category, part shape, and part articulation.\n\nOur experiments show that our method outperforms the state-of-the-art in articulated object generation, producing more realistic objects while conforming better to user constraints."", 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'image.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/f8fa5a57-fe50-4ed1-bf43-e1e0e0d31ed8/image.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234740Z&X-Amz-Expires=3600&X-Amz-Signature=43935852a3736e71d785f6fb3faf892fc1859ef534fbcfadfe218c733b456de4&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:40.274Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://3dlg-hcvc.github.io/cage/', 'link': {'url': 'https://3dlg-hcvc.github.io/cage/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://3dlg-hcvc.github.io/cage/', 'href': 'https://3dlg-hcvc.github.io/cage/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2024', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2024', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '89330a72-bcbd-42e7-9879-4d3e144ed6ec'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://www.youtube.com/watch?v=cH_rbKbyTpE', 'link': {'url': 'https://www.youtube.com/watch?v=cH_rbKbyTpE'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://www.youtube.com/watch?v=cH_rbKbyTpE', 'href': 'https://www.youtube.com/watch?v=cH_rbKbyTpE'}]}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}, {'id': 'bccbeae3-55ce-4b38-9abc-61c658ca30b4', 'name': 'Manolis', 'color': 'blue'}, {'id': 'ed487f9e-f5ad-446d-89eb-2ab46ab0892c', 'name': 'Ali', 'color': 'purple'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_CAGE_Controllable_Articulation_GEneration_CVPR_2024_paper.pdf', 'link': {'url': 'https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_CAGE_Controllable_Articulation_GEneration_CVPR_2024_paper.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_CAGE_Controllable_Articulation_GEneration_CVPR_2024_paper.pdf', 'href': 'https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_CAGE_Controllable_Articulation_GEneration_CVPR_2024_paper.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'CAGE: Controllable Articulation GEneration', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CAGE: Controllable Articulation GEneration', 'href': None}]}}",https://www.notion.so/CAGE-Controllable-Articulation-GEneration-361106ebcb4448899139c7fb690a0d46,https://yanxg.notion.site/CAGE-Controllable-Articulation-GEneration-361106ebcb4448899139c7fb690a0d46
26,page,b914f52c-9b8a-4676-813f-832639e87b28,2024-07-28T03:28:00.000Z,2024-08-26T21:00:00.000Z,"{'object': 'user', 'id': '3957b1ca-5978-4d50-8f00-6e56c5ef8dc0'}","{'object': 'user', 'id': 'edfa4d94-358b-4482-91a3-67133bc15be8'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2024-08-26T21:00:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2024-06-22', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Yizhi Wang, Wallace Lira, Wenqi Wang, Ali Mahdavi-Amiri, Hao Zhang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Yizhi Wang, Wallace Lira, Wenqi Wang, Ali Mahdavi-Amiri, Hao Zhang', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'edfa4d94-358b-4482-91a3-67133bc15be8'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'We introduce multi-slice reasoning, a new notion for single-view 3D reconstruction which challenges the current and prevailing belief that multi-view synthesis is the most natural conduit between single-view and 3D. Our key observation is that object slicing is more advantageous than altering views to reveal occluded structures. Specifically, slicing is more occlusion-revealing since it can peel through any occluders without obstruction. In the limit, i.e., with infinitely many slices, it is guaranteed to unveil all hidden object parts. We realize our idea by developing Slice3D, a novel method for single-view 3D reconstruction which first predicts multi-slice images from a single RGB image and then integrates the slices into a 3D model using a coordinate-based transformer network for signed distance prediction. The slice images can be regressed or generated, both through a U-Net based network. For the former, we inject a learnable slice indicator code to designate each decoded image into a spatial slice location, while the slice generator is a denoising diffusion model operating on the entirety of slice images stacked on the input channels. We conduct extensive evaluation against state-of-the-art alternatives to demonstrate superiority of our method, especially in recovering complex and severely occluded shape structures, amid ambiguities. All Slice3D results were produced by networks trained on a single Nvidia A40 GPU, with an inference time less than 20 seconds.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'We introduce multi-slice reasoning, a new notion for single-view 3D reconstruction which challenges the current and prevailing belief that multi-view synthesis is the most natural conduit between single-view and 3D. Our key observation is that object slicing is more advantageous than altering views to reveal occluded structures. Specifically, slicing is more occlusion-revealing since it can peel through any occluders without obstruction. In the limit, i.e., with infinitely many slices, it is guaranteed to unveil all hidden object parts. We realize our idea by developing Slice3D, a novel method for single-view 3D reconstruction which first predicts multi-slice images from a single RGB image and then integrates the slices into a 3D model using a coordinate-based transformer network for signed distance prediction. The slice images can be regressed or generated, both through a U-Net based network. For the former, we inject a learnable slice indicator code to designate each decoded image into a spatial slice location, while the slice generator is a denoising diffusion model operating on the entirety of slice images stacked on the input channels. We conduct extensive evaluation against state-of-the-art alternatives to demonstrate superiority of our method, especially in recovering complex and severely occluded shape structures, amid ambiguities. All Slice3D results were produced by networks trained on a single Nvidia A40 GPU, with an inference time less than 20 seconds.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'slice3d.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/4c7e9e6f-6978-4b3f-bd8f-1ef0a0ab0870/slice3d.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234740Z&X-Amz-Expires=3600&X-Amz-Signature=f5fb0d6f32a53b163284762f89e6fbaace9a624f410aca2423672671e06eacef&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:40.275Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://yizhiwang96.github.io/Slice3D/', 'link': {'url': 'https://yizhiwang96.github.io/Slice3D/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://yizhiwang96.github.io/Slice3D/', 'href': 'https://yizhiwang96.github.io/Slice3D/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2024', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2024', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '3957b1ca-5978-4d50-8f00-6e56c5ef8dc0'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'e16d2c21-c87f-4a73-aa66-85bb6bc6e5b0', 'name': 'Richard', 'color': 'pink'}, {'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2312.02221', 'link': {'url': 'https://arxiv.org/abs/2312.02221'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2312.02221', 'href': 'https://arxiv.org/abs/2312.02221'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Slice3D: Multi-Slice, Occlusion-Revealing, Single View 3D Reconstruction', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Slice3D: Multi-Slice, Occlusion-Revealing, Single View 3D Reconstruction', 'href': None}]}}",https://www.notion.so/Slice3D-Multi-Slice-Occlusion-Revealing-Single-View-3D-Reconstruction-b914f52c9b8a4676813f832639e87b28,https://yanxg.notion.site/Slice3D-Multi-Slice-Occlusion-Revealing-Single-View-3D-Reconstruction-b914f52c9b8a4676813f832639e87b28
27,page,e7fa23b8-059d-40d0-96fb-6329b650fbe7,2024-07-22T18:16:00.000Z,2024-08-26T20:59:00.000Z,"{'object': 'user', 'id': 'd92b046c-2061-4059-9e57-3f50c31b6e03'}","{'object': 'user', 'id': 'edfa4d94-358b-4482-91a3-67133bc15be8'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2024-08-26T20:59:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2024-06-22', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Shichong Peng, Yanshu Zhang, Ke Li', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Shichong Peng, Yanshu Zhang, Ke Li', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'edfa4d94-358b-4482-91a3-67133bc15be8'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'We propose the problem of point-level 3D scene interpolation, which aims to simultaneously reconstruct a 3D scene in two states from multiple views, synthesize smooth point-level interpolations between them, and render the scene from novel viewpoints, all without any supervision between the states. The primary challenge is on achieving a smooth transition between states that may involve significant and non-rigid changes. To address these challenges, we introduce ""PAPR in Motion"", a novel approach that builds upon the recent Proximity Attention Point Rendering (PAPR) technique, which can deform a point cloud to match a significantly different shape and render a visually coherent scene even after non-rigid deformations. Our approach is specifically designed to maintain the temporal consistency of the geometric structure by introducing various regularization techniques for PAPR. The result is a method that can effectively bridge large scene changes and produce visually coherent and temporally smooth interpolations in both geometry and appearance. Evaluation across diverse motion types demonstrates that ""PAPR in Motion"" outperforms the leading neural renderer for dynamic scenes.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'We propose the problem of point-level 3D scene interpolation, which aims to simultaneously reconstruct a 3D scene in two states from multiple views, synthesize smooth point-level interpolations between them, and render the scene from novel viewpoints, all without any supervision between the states. The primary challenge is on achieving a smooth transition between states that may involve significant and non-rigid changes. To address these challenges, we introduce ""PAPR in Motion"", a novel approach that builds upon the recent Proximity Attention Point Rendering (PAPR) technique, which can deform a point cloud to match a significantly different shape and render a visually coherent scene even after non-rigid deformations. Our approach is specifically designed to maintain the temporal consistency of the geometric structure by introducing various regularization techniques for PAPR. The result is a method that can effectively bridge large scene changes and produce visually coherent and temporally smooth interpolations in both geometry and appearance. Evaluation across diverse motion types demonstrates that ""PAPR in Motion"" outperforms the leading neural renderer for dynamic scenes.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'thumbnail.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/0faa91c2-e4e8-489c-bd85-8b81103f0329/thumbnail.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234740Z&X-Amz-Expires=3600&X-Amz-Signature=fbd1c4e953480cc0a1d93677e65a69c0b353d9e91a88560e61fd8ac98dd55ab2&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:40.276Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://niopeng.github.io/PAPR-in-Motion/', 'link': {'url': 'https://niopeng.github.io/PAPR-in-Motion/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://niopeng.github.io/PAPR-in-Motion/', 'href': 'https://niopeng.github.io/PAPR-in-Motion/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2024', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2024', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'd92b046c-2061-4059-9e57-3f50c31b6e03'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://youtu.be/vysmn3TN4FY', 'link': {'url': 'https://youtu.be/vysmn3TN4FY'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://youtu.be/vysmn3TN4FY', 'href': 'https://youtu.be/vysmn3TN4FY'}]}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}, {'id': '9ea36cd3-48d3-462d-baf9-b75ef77f4121', 'name': 'Ke', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2406.05533', 'link': {'url': 'https://arxiv.org/abs/2406.05533'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2406.05533', 'href': 'https://arxiv.org/abs/2406.05533'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'PAPR in Motion: Seamless Point-level 3D Scene Interpolation', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'PAPR in Motion: Seamless Point-level 3D Scene Interpolation', 'href': None}]}}",https://www.notion.so/PAPR-in-Motion-Seamless-Point-level-3D-Scene-Interpolation-e7fa23b8059d40d096fb6329b650fbe7,https://yanxg.notion.site/PAPR-in-Motion-Seamless-Point-level-3D-Scene-Interpolation-e7fa23b8059d40d096fb6329b650fbe7
28,page,9442ced9-0049-482f-ab4e-d8f31f56e64b,2024-07-22T17:43:00.000Z,2024-07-22T17:54:00.000Z,"{'object': 'user', 'id': 'b507e305-8efa-443f-9b61-e893102d503a'}","{'object': 'user', 'id': 'b507e305-8efa-443f-9b61-e893102d503a'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2024-07-22T17:54:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2024-05-07', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Jonah Philion,\xa0Xue Bin Peng, Sanja Fidler\n\n', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Jonah Philion,\xa0Xue Bin Peng, Sanja Fidler\n\n', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'b507e305-8efa-443f-9b61-e893102d503a'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'A longstanding challenge for self-driving development is simulating dynamic\ndriving scenarios seeded from recorded driving logs. In pursuit of this functionality, we apply tools from discrete sequence modeling to model how vehicles,\npedestrians and cyclists interact in driving scenarios. Using a simple data-driven\ntokenization scheme, we discretize trajectories to centimeter-level resolution using\na small vocabulary. We then model the multi-agent sequence of discrete motion\ntokens with a GPT-like encoder-decoder that is autoregressive in time and takes\ninto account intra-timestep interaction between agents. Scenarios sampled from\nour model exhibit state-of-the-art realism; our model tops the Waymo Sim Agents\nBenchmark, surpassing prior work along the realism meta metric by 3.3% and\nalong the interaction metric by 9.9%. We ablate our modeling choices in full autonomy and partial autonomy settings, and show that the representations learned\nby our model can quickly be adapted to improve performance on nuScenes. We\nadditionally evaluate the scalability of our model with respect to parameter count\nand dataset size, and use density estimates from our model to quantify the saliency\nof context length and intra-timestep interaction for the traffic modeling task.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'A longstanding challenge for self-driving development is simulating dynamic\ndriving scenarios seeded from recorded driving logs. In pursuit of this functionality, we apply tools from discrete sequence modeling to model how vehicles,\npedestrians and cyclists interact in driving scenarios. Using a simple data-driven\ntokenization scheme, we discretize trajectories to centimeter-level resolution using\na small vocabulary. We then model the multi-agent sequence of discrete motion\ntokens with a GPT-like encoder-decoder that is autoregressive in time and takes\ninto account intra-timestep interaction between agents. Scenarios sampled from\nour model exhibit state-of-the-art realism; our model tops the Waymo Sim Agents\nBenchmark, surpassing prior work along the realism meta metric by 3.3% and\nalong the interaction metric by 9.9%. We ablate our modeling choices in full autonomy and partial autonomy settings, and show that the representations learned\nby our model can quickly be adapted to improve performance on nuScenes. We\nadditionally evaluate the scalability of our model with respect to parameter count\nand dataset size, and use density estimates from our model to quantify the saliency\nof context length and intra-timestep interaction for the traffic modeling task.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'trajeglish_thumb.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/ecd938b2-8dbe-4550-bcaf-91b660b4405f/trajeglish_thumb.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234740Z&X-Amz-Expires=3600&X-Amz-Signature=a6509470b65edf35b664c8b8fe3efc502e57668377e43036d93efe90080e5eb9&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:40.276Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://xbpeng.github.io/projects/Trajeglish/index.html', 'link': {'url': 'https://xbpeng.github.io/projects/Trajeglish/index.html'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://xbpeng.github.io/projects/Trajeglish/index.html', 'href': 'https://xbpeng.github.io/projects/Trajeglish/index.html'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ICLR 2024', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ICLR 2024', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'b507e305-8efa-443f-9b61-e893102d503a'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://www.youtube.com/watch?v=Bk1v_Qct3i8&embeds_referring_euri=https%3A%2F%2Fxbpeng.github.io%2F&source_ve_path=Mjg2NjY', 'link': {'url': 'https://www.youtube.com/watch?v=Bk1v_Qct3i8&embeds_referring_euri=https%3A%2F%2Fxbpeng.github.io%2F&source_ve_path=Mjg2NjY'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://www.youtube.com/watch?v=Bk1v_Qct3i8&embeds_referring_euri=https%3A%2F%2Fxbpeng.github.io%2F&source_ve_path=Mjg2NjY', 'href': 'https://www.youtube.com/watch?v=Bk1v_Qct3i8&embeds_referring_euri=https%3A%2F%2Fxbpeng.github.io%2F&source_ve_path=Mjg2NjY'}]}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '847af5a9-e982-4683-ad24-59e89b263fa0', 'name': 'ICLR', 'color': 'green'}, {'id': 'dc677421-36af-48e6-be53-0598670c619b', 'name': 'Jason', 'color': 'red'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://xbpeng.github.io/projects/Trajeglish/Trajeglish_2024.pdf', 'link': {'url': 'https://xbpeng.github.io/projects/Trajeglish/Trajeglish_2024.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://xbpeng.github.io/projects/Trajeglish/Trajeglish_2024.pdf', 'href': 'https://xbpeng.github.io/projects/Trajeglish/Trajeglish_2024.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Trajeglish: Traffic Modeling as Next-Token Prediction', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Trajeglish: Traffic Modeling as Next-Token Prediction', 'href': None}]}}",https://www.notion.so/Trajeglish-Traffic-Modeling-as-Next-Token-Prediction-9442ced90049482fab4ed8f31f56e64b,https://yanxg.notion.site/Trajeglish-Traffic-Modeling-as-Next-Token-Prediction-9442ced90049482fab4ed8f31f56e64b
29,page,951b7e5b-aebc-4313-9633-3f84b4c852dc,2024-04-29T17:27:00.000Z,2024-04-29T17:33:00.000Z,"{'object': 'user', 'id': 'edfa4d94-358b-4482-91a3-67133bc15be8'}","{'object': 'user', 'id': 'edfa4d94-358b-4482-91a3-67133bc15be8'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2024-04-29T17:33:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2024-04-22', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Han-Hung Lee, Manolis Savva, Angel X Chang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Han-Hung Lee, Manolis Savva, Angel X Chang', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'edfa4d94-358b-4482-91a3-67133bc15be8'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Recent years have seen an explosion of work and interest in text-to-3D shape generation. Much of the progress is driven by advances in 3D representations, large-scale pretraining and representation learning for text and image data enabling generative AI models, and differentiable rendering. Computational systems that can perform text-to-3D shape generation have captivated the popular imagination as they enable non-expert users to easily create 3D content directly from text. However, there are still many limitations and challenges remaining in this problem space. In this state-of-the-art report, we provide a survey of the underlying technology and methods enabling text-to-3D shape generation to summarize the background literature. We then derive a systematic categorization of recent work on text-to-3D shape generation based on the type of supervision data required. Finally, we discuss limitations of the existing categories of methods, and delineate promising directions for future work.', 'link': None}, 'annotations': {'bold': False, 'italic': True, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Recent years have seen an explosion of work and interest in text-to-3D shape generation. Much of the progress is driven by advances in 3D representations, large-scale pretraining and representation learning for text and image data enabling generative AI models, and differentiable rendering. Computational systems that can perform text-to-3D shape generation have captivated the popular imagination as they enable non-expert users to easily create 3D content directly from text. However, there are still many limitations and challenges remaining in this problem space. In this state-of-the-art report, we provide a survey of the underlying technology and methods enabling text-to-3D shape generation to summarize the background literature. We then derive a systematic categorization of recent work on text-to-3D shape generation based on the type of supervision data required. Finally, we discuss limitations of the existing categories of methods, and delineate promising directions for future work.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'Text-to-3D_Shape_Generation.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/9c846b10-2508-44dd-8396-a31fdb5c1db9/Text-to-3D_Shape_Generation.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234740Z&X-Amz-Expires=3600&X-Amz-Signature=97441a9298b1f7476b2b1351767dcf7d0830d50423c65bcb0b702bba1c980a68&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:40.277Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://3dlg-hcvc.github.io/tt3dstar/', 'link': {'url': 'https://3dlg-hcvc.github.io/tt3dstar/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://3dlg-hcvc.github.io/tt3dstar/', 'href': 'https://3dlg-hcvc.github.io/tt3dstar/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Eurographics 2024', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Eurographics 2024', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'edfa4d94-358b-4482-91a3-67133bc15be8'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '67e44c1f-570b-416e-88ab-b7fa8631d96b', 'name': 'Eurographics', 'color': 'purple'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://3dlg-hcvc.github.io/tt3dstar/', 'link': {'url': 'https://3dlg-hcvc.github.io/tt3dstar/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://3dlg-hcvc.github.io/tt3dstar/', 'href': 'https://3dlg-hcvc.github.io/tt3dstar/'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Text-to-3D Shape Generation STAR ', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Text-to-3D Shape Generation STAR ', 'href': None}]}}",https://www.notion.so/Text-to-3D-Shape-Generation-STAR-951b7e5baebc431396333f84b4c852dc,https://yanxg.notion.site/Text-to-3D-Shape-Generation-STAR-951b7e5baebc431396333f84b4c852dc
30,page,6ba78a9f-244d-494e-b4e7-299a9dda8692,2023-10-26T18:21:00.000Z,2023-11-05T06:43:00.000Z,"{'object': 'user', 'id': '934b1b33-d80a-463d-a123-85f1c0bcc1eb'}","{'object': 'user', 'id': '934b1b33-d80a-463d-a123-85f1c0bcc1eb'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-11-05T06:43:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2024-03-18', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Qirui Wu, Daniel Ritchie, Manolis Savva, Angel X Chang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Qirui Wu, Daniel Ritchie, Manolis Savva, Angel X Chang', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '934b1b33-d80a-463d-a123-85f1c0bcc1eb'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Single-view 3D shape retrieval is a challenging task that is increasingly important with the growth of available 3D data. Prior work that has studied this task has not focused on evaluating how realistic occlusions impact performance, and how shape retrieval methods generalize to scenarios where either the target 3D shape database contains unseen shapes, or the input image contains unseen objects. In this paper, we systematically evaluate single-view 3D shape retrieval along three different axes: the presence of object occlusions and truncations, generalization to unseen 3D shape data, and generalization to unseen objects in the input images. We standardize two existing datasets of real images and propose a dataset generation pipeline to produce a synthetic dataset of scenes with multiple objects exhibiting realistic occlusions. Our experiments show that training on occlusion-free data as was commonly done in prior work leads to significant performance degradation for inputs with occlusion. We find that that by first pretraining on our synthetic dataset with occlusions and then finetuning on real data, we can significantly outperform models from prior work and demonstrate robustness to both unseen 3D shapes and unseen objects.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Single-view 3D shape retrieval is a challenging task that is increasingly important with the growth of available 3D data. Prior work that has studied this task has not focused on evaluating how realistic occlusions impact performance, and how shape retrieval methods generalize to scenarios where either the target 3D shape database contains unseen shapes, or the input image contains unseen objects. In this paper, we systematically evaluate single-view 3D shape retrieval along three different axes: the presence of object occlusions and truncations, generalization to unseen 3D shape data, and generalization to unseen objects in the input images. We standardize two existing datasets of real images and propose a dataset generation pipeline to produce a synthetic dataset of scenes with multiple objects exhibiting realistic occlusions. Our experiments show that training on occlusion-free data as was commonly done in prior work leads to significant performance degradation for inputs with occlusion. We find that that by first pretraining on our synthetic dataset with occlusions and then finetuning on real data, we can significantly outperform models from prior work and demonstrate robustness to both unseen 3D shapes and unseen objects.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'gcmic.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/787e60b3-a264-4aed-9e48-4d390886928c/gcmic.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234740Z&X-Amz-Expires=3600&X-Amz-Signature=9930b3b9ec3e0cba0af34ca378e08992001c3bc39d8af0b6b9bb6d16ad31f37e&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:40.277Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://3dlg-hcvc.github.io/', 'link': {'url': 'https://3dlg-hcvc.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://3dlg-hcvc.github.io/', 'href': 'https://3dlg-hcvc.github.io/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': '3DV 2024', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': '3DV 2024', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '934b1b33-d80a-463d-a123-85f1c0bcc1eb'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'd9d68329-9f05-4187-aa6b-052567aebad4', 'name': '3DV', 'color': 'gray'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}, {'id': 'bccbeae3-55ce-4b38-9abc-61c658ca30b4', 'name': 'Manolis', 'color': 'blue'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://3dlg-hcvc.github.io/', 'link': {'url': 'https://3dlg-hcvc.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://3dlg-hcvc.github.io/', 'href': 'https://3dlg-hcvc.github.io/'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Generalizing Single-View 3D Shape Retrieval to Occlusions and Unseen Objects', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Generalizing Single-View 3D Shape Retrieval to Occlusions and Unseen Objects', 'href': None}]}}",https://www.notion.so/Generalizing-Single-View-3D-Shape-Retrieval-to-Occlusions-and-Unseen-Objects-6ba78a9f244d494eb4e7299a9dda8692,https://yanxg.notion.site/Generalizing-Single-View-3D-Shape-Retrieval-to-Occlusions-and-Unseen-Objects-6ba78a9f244d494eb4e7299a9dda8692
31,page,99d34d16-28ec-4ada-abee-307b42832af2,2023-10-26T02:16:00.000Z,2023-10-26T02:21:00.000Z,"{'object': 'user', 'id': 'f4f45284-0dde-43dd-b5a8-b4c0e833608b'}","{'object': 'user', 'id': 'f4f45284-0dde-43dd-b5a8-b4c0e833608b'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-10-26T02:21:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2024-03-18', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Xiaohao Sun*, Hanxiao Jiang*, Manolis Savva, Angel X Chang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Xiaohao Sun*, Hanxiao Jiang*, Manolis Savva, Angel X Chang', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'f4f45284-0dde-43dd-b5a8-b4c0e833608b'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Openable part detection is the task of detecting the openable parts of an object in a single-view image, and predicting corresponding motion parameters. Prior work investigated the unrealistic setting where all input images only contain a single openable object. We generalize this task to scenes with multiple objects each potentially possessing openable parts, and create a corresponding dataset based on real-world scenes. We then address this more challenging scenario with OPDFormer: a part-aware transformer architecture. Our experiments show that the OPDFormer architecture significantly outperforms prior work. The more realistic multiple-object scenarios we investigated remain challenging for all methods, indicating opportunities for future work.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Openable part detection is the task of detecting the openable parts of an object in a single-view image, and predicting corresponding motion parameters. Prior work investigated the unrealistic setting where all input images only contain a single openable object. We generalize this task to scenes with multiple objects each potentially possessing openable parts, and create a corresponding dataset based on real-world scenes. We then address this more challenging scenario with OPDFormer: a part-aware transformer architecture. Our experiments show that the OPDFormer architecture significantly outperforms prior work. The more realistic multiple-object scenarios we investigated remain challenging for all methods, indicating opportunities for future work.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'opdmulti.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/a14d25de-bf9a-4152-8e96-bd50e29c39da/opdmulti.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234740Z&X-Amz-Expires=3600&X-Amz-Signature=e15c833c255b82f97a642aaecff59a1892c92ed3c3f1c3f05b950ef656b4f71e&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:40.278Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://3dlg-hcvc.github.io/OPDMulti/', 'link': {'url': 'https://3dlg-hcvc.github.io/OPDMulti/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://3dlg-hcvc.github.io/OPDMulti/', 'href': 'https://3dlg-hcvc.github.io/OPDMulti/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': '3DV 2024', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': '3DV 2024', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'f4f45284-0dde-43dd-b5a8-b4c0e833608b'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'd9d68329-9f05-4187-aa6b-052567aebad4', 'name': '3DV', 'color': 'gray'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2303.14087', 'link': {'url': 'https://arxiv.org/abs/2303.14087'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2303.14087', 'href': 'https://arxiv.org/abs/2303.14087'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'OPDMulti: Openable Part Detection for Multiple Objects', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'OPDMulti: Openable Part Detection for Multiple Objects', 'href': None}]}}",https://www.notion.so/OPDMulti-Openable-Part-Detection-for-Multiple-Objects-99d34d1628ec4adaabee307b42832af2,https://yanxg.notion.site/OPDMulti-Openable-Part-Detection-for-Multiple-Objects-99d34d1628ec4adaabee307b42832af2
32,page,ea26e5c5-0db8-4038-9dfc-305bbded0f83,2024-01-02T23:50:00.000Z,2024-01-02T23:53:00.000Z,"{'object': 'user', 'id': 'b8d7e6d8-82a6-4b57-b07a-0fdcc828f6c5'}","{'object': 'user', 'id': 'b8d7e6d8-82a6-4b57-b07a-0fdcc828f6c5'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2024-01-02T23:53:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-12-12', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Chris Careaga, S. Mahdi H. Miangoleh, Yağız Aksoy', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Chris Careaga, S. Mahdi H. Miangoleh, Yağız Aksoy', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'b8d7e6d8-82a6-4b57-b07a-0fdcc828f6c5'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://yaksoy.github.io/images/research/intrinsicCompositing.jpg', 'type': 'external', 'external': {'url': 'https://yaksoy.github.io/images/research/intrinsicCompositing.jpg'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://yaksoy.github.io/intrinsicCompositing/', 'link': {'url': 'https://yaksoy.github.io/intrinsicCompositing/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://yaksoy.github.io/intrinsicCompositing/', 'href': 'https://yaksoy.github.io/intrinsicCompositing/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'SIGGRAPH Asia 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'SIGGRAPH Asia 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'b8d7e6d8-82a6-4b57-b07a-0fdcc828f6c5'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://www.youtube.com/watch?v=M9hCUTp8bo4', 'link': {'url': 'https://www.youtube.com/watch?v=M9hCUTp8bo4'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://www.youtube.com/watch?v=M9hCUTp8bo4', 'href': 'https://www.youtube.com/watch?v=M9hCUTp8bo4'}]}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'd0404583-7d96-4756-b656-bb910bc1e011', 'name': 'SIGGRAPH Asia', 'color': 'blue'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://yaksoy.github.io/papers/SigAsia23-IntrinsicCompositing.pdf', 'link': {'url': 'https://yaksoy.github.io/papers/SigAsia23-IntrinsicCompositing.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://yaksoy.github.io/papers/SigAsia23-IntrinsicCompositing.pdf', 'href': 'https://yaksoy.github.io/papers/SigAsia23-IntrinsicCompositing.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '❌❌❌❌'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Intrinsic Harmonization for Illumination-Aware Compositing', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Intrinsic Harmonization for Illumination-Aware Compositing', 'href': None}]}}",https://www.notion.so/Intrinsic-Harmonization-for-Illumination-Aware-Compositing-ea26e5c50db840389dfc305bbded0f83,https://yanxg.notion.site/Intrinsic-Harmonization-for-Illumination-Aware-Compositing-ea26e5c50db840389dfc305bbded0f83
33,page,af27631d-4936-4a32-b742-dca9d9379a50,2023-10-25T17:44:00.000Z,2023-10-25T17:52:00.000Z,"{'object': 'user', 'id': 'a1e152f8-eb9e-4fec-b928-52e3571889ea'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-10-25T17:52:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-12-12', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Qimin Chen, Zhiqin Chen, Hang Zhou, Hao Zhang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Qimin Chen, Zhiqin Chen, Hang Zhou, Hao Zhang', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'We present ShaDDR, an example-based deep generative neural network which produces a high-resolution textured 3D shape through geometry detailization and conditional texture generation applied to an input coarse voxel shape. Trained on a small set of detailed and textured exemplar shapes, our method learns to detailize the geometry via multi-resolution voxel upsampling and generate textures on voxel surfaces via differentiable rendering against exemplar texture images from a few views. The generation is real-time, taking less than 1 second to produce a 3D model with voxel resolutions up to 5123. The generated shape preserves the overall structure of the input coarse voxel model, while the style of the generated geometric details and textures can be manipulated through learned latent codes. In the experiments, we show that our method can generate higher-resolution shapes with plausible and improved geometric details and clean textures compared to prior works. Furthermore, we showcase the ability of our method to learn geometric details and textures from shapes reconstructed from real-world photos. In addition, we have developed an interactive modeling application to demonstrate the generalizability of our method to various user inputs and the controllability it offers, allowing users to interactively sculpt a coarse voxel shape to define the overall structure of the detailized 3D shape.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'We present ShaDDR, an example-based deep generative neural network which produces a high-resolution textured 3D shape through geometry detailization and conditional texture generation applied to an input coarse voxel shape. Trained on a small set of detailed and textured exemplar shapes, our method learns to detailize the geometry via multi-resolution voxel upsampling and generate textures on voxel surfaces via differentiable rendering against exemplar texture images from a few views. The generation is real-time, taking less than 1 second to produce a 3D model with voxel resolutions up to 5123. The generated shape preserves the overall structure of the input coarse voxel model, while the style of the generated geometric details and textures can be manipulated through learned latent codes. In the experiments, we show that our method can generate higher-resolution shapes with plausible and improved geometric details and clean textures compared to prior works. Furthermore, we showcase the ability of our method to learn geometric details and textures from shapes reconstructed from real-world photos. In addition, we have developed an interactive modeling application to demonstrate the generalizability of our method to various user inputs and the controllability it offers, allowing users to interactively sculpt a coarse voxel shape to define the overall structure of the detailized 3D shape.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'teaser.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/4ece1f23-aa46-4ba4-9fc7-35328b2bca56/teaser.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234740Z&X-Amz-Expires=3600&X-Amz-Signature=ca4222cfa56499682eb8c001f67188d3f12cf4add6a5147e23bb75029e21e7fe&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:40.278Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://qiminchen.github.io/shaddr/', 'link': {'url': 'https://qiminchen.github.io/shaddr/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://qiminchen.github.io/shaddr/', 'href': 'https://qiminchen.github.io/shaddr/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'SIGGRAPH Asia 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'SIGGRAPH Asia 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'a1e152f8-eb9e-4fec-b928-52e3571889ea'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'e16d2c21-c87f-4a73-aa66-85bb6bc6e5b0', 'name': 'Richard', 'color': 'pink'}, {'id': 'd0404583-7d96-4756-b656-bb910bc1e011', 'name': 'SIGGRAPH Asia', 'color': 'blue'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2306.04889', 'link': {'url': 'https://arxiv.org/abs/2306.04889'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2306.04889', 'href': 'https://arxiv.org/abs/2306.04889'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'ShaDDR: Interactive Example-Based Geometry and Texture Generation via 3D Shape Detailization and Differentiable Rendering', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ShaDDR: Interactive Example-Based Geometry and Texture Generation via 3D Shape Detailization and Differentiable Rendering', 'href': None}]}}",https://www.notion.so/ShaDDR-Interactive-Example-Based-Geometry-and-Texture-Generation-via-3D-Shape-Detailization-and-Dif-af27631d49364a32b742dca9d9379a50,https://yanxg.notion.site/ShaDDR-Interactive-Example-Based-Geometry-and-Texture-Generation-via-3D-Shape-Detailization-and-Dif-af27631d49364a32b742dca9d9379a50
34,page,8fcbf0dc-f008-47d3-9190-245544f99ad3,2023-11-02T23:49:00.000Z,2023-11-02T23:57:00.000Z,"{'object': 'user', 'id': '697d2969-d64d-4753-8cd8-1311849d224f'}","{'object': 'user', 'id': '697d2969-d64d-4753-8cd8-1311849d224f'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-11-02T23:57:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-12-10', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Aditya Vora, Akshay Gadi Patil, Hao (Richard) Zhang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Aditya Vora, Akshay Gadi Patil, Hao (Richard) Zhang', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '697d2969-d64d-4753-8cd8-1311849d224f'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'We present a volume rendering-based neural surface reconstruction method that takes as few as three disparate RGB images as input. Our key idea is to regularize the reconstruction, which is severely ill-posed and leaving significant gaps between the sparse views, by learning a set of neural templates that act as surface priors. Our method coined DiViNet, operates in two stages. The first stage learns the templates, in the form of 3D Gaussian functions, across different scenes, without 3D supervision. In the reconstruction stage, our predicted templates serve as anchors to help ""stitch"" the surfaces over sparse regions. We demonstrate that our approach is not only able to complete the surface geometry but also reconstructs surface details to a reasonable extent from few disparate input views. On the DTU and BlendedMVS datasets, our approach achieves the best reconstruction quality among existing methods in the presence of such sparse views, and performs on par, if not better, with competing methods when dense views are employed as inputs.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'We present a volume rendering-based neural surface reconstruction method that takes as few as three disparate RGB images as input. Our key idea is to regularize the reconstruction, which is severely ill-posed and leaving significant gaps between the sparse views, by learning a set of neural templates that act as surface priors. Our method coined DiViNet, operates in two stages. The first stage learns the templates, in the form of 3D Gaussian functions, across different scenes, without 3D supervision. In the reconstruction stage, our predicted templates serve as anchors to help ""stitch"" the surfaces over sparse regions. We demonstrate that our approach is not only able to complete the surface geometry but also reconstructs surface details to a reasonable extent from few disparate input views. On the DTU and BlendedMVS datasets, our approach achieves the best reconstruction quality among existing methods in the presence of such sparse views, and performs on par, if not better, with competing methods when dense views are employed as inputs.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'DiViNeT_long.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/1c8a5bf0-df4c-4877-9ba3-57a0efb585d0/DiViNeT_long.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234740Z&X-Amz-Expires=3600&X-Amz-Signature=babb24d21c33d7421bebf8912206d7acbc37889b272efd0f22a1df9d8748786d&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:40.279Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://aditya-vora.github.io/divinetpp/', 'link': {'url': 'https://aditya-vora.github.io/divinetpp/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://aditya-vora.github.io/divinetpp/', 'href': 'https://aditya-vora.github.io/divinetpp/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'NeurIPS 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'NeurIPS 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '697d2969-d64d-4753-8cd8-1311849d224f'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'bb1c7b2a-93d8-4f0d-9e32-2a3594a652cd', 'name': 'NeurIPS', 'color': 'default'}, {'id': 'e16d2c21-c87f-4a73-aa66-85bb6bc6e5b0', 'name': 'Richard', 'color': 'pink'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2306.04699', 'link': {'url': 'https://arxiv.org/abs/2306.04699'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2306.04699', 'href': 'https://arxiv.org/abs/2306.04699'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'DiViNeT: 3D Reconstruction from Disparate Views via Neural Template Regularization', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'DiViNeT: 3D Reconstruction from Disparate Views via Neural Template Regularization', 'href': None}]}}",https://www.notion.so/DiViNeT-3D-Reconstruction-from-Disparate-Views-via-Neural-Template-Regularization-8fcbf0dcf00847d39190245544f99ad3,https://yanxg.notion.site/DiViNeT-3D-Reconstruction-from-Disparate-Views-via-Neural-Template-Regularization-8fcbf0dcf00847d39190245544f99ad3
35,page,a6aec9d2-5b4e-485c-b437-baa336078736,2023-10-27T07:36:00.000Z,2023-11-05T06:36:00.000Z,"{'object': 'user', 'id': '92b06c1f-5b2d-42a0-b5d2-a7a23e749f6b'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-11-05T06:36:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-12-10', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Jiacheng Chen, Ruizhi Deng, Yasutaka Furukawa', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Jiacheng Chen, Ruizhi Deng, Yasutaka Furukawa', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://jcchen.me/assets/img/publications/poly-diffuse.png', 'type': 'external', 'external': {'url': 'https://jcchen.me/assets/img/publications/poly-diffuse.png'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://poly-diffuse.github.io/', 'link': {'url': 'https://poly-diffuse.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://poly-diffuse.github.io/', 'href': 'https://poly-diffuse.github.io/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'NeurIPS 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'NeurIPS 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '92b06c1f-5b2d-42a0-b5d2-a7a23e749f6b'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'bb1c7b2a-93d8-4f0d-9e32-2a3594a652cd', 'name': 'NeurIPS', 'color': 'default'}, {'id': '898c1b27-4fe4-4c46-8f37-c2c8cdfe0997', 'name': 'Yasu', 'color': 'red'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2306.01461', 'link': {'url': 'https://arxiv.org/abs/2306.01461'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2306.01461', 'href': 'https://arxiv.org/abs/2306.01461'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '❌❌❌❌'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'PolyDiffuse: Polygonal Shape Reconstruction via Guided Set Diffusion Model', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'PolyDiffuse: Polygonal Shape Reconstruction via Guided Set Diffusion Model', 'href': None}]}}",https://www.notion.so/PolyDiffuse-Polygonal-Shape-Reconstruction-via-Guided-Set-Diffusion-Model-a6aec9d25b4e485cb437baa336078736,https://yanxg.notion.site/PolyDiffuse-Polygonal-Shape-Reconstruction-via-Guided-Set-Diffusion-Model-a6aec9d25b4e485cb437baa336078736
36,page,52024903-b5ec-47c0-83bf-a5f65531d5f8,2023-10-26T03:19:00.000Z,2023-10-27T20:58:00.000Z,"{'object': 'user', 'id': '0e2455bd-a873-4099-8102-14fa4f1ff893'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-10-27T20:58:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-12-10', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'S', 'link': {'url': 'https://tangshitao.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'S', 'href': 'https://tangshitao.github.io/'}, {'type': 'text', 'text': {'content': 'hitao Tang*, Fuyang Zhang*, Jiacheng Chen, Peng Wang, Yasutaka Furukawa', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'hitao Tang*, Fuyang Zhang*, Jiacheng Chen, Peng Wang, Yasutaka Furukawa', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'This paper introduces \\textit{MVDiffusion}, a simple yet effective method for generating consistent multi-view images from text prompts given pixel-to-pixel correspondences (\\eg, perspective crops from a panorama or multi-view images given depth maps and poses). Unlike prior methods that rely on iterative image warping and inpainting, MVDiffusion simultaneously generates all images with a global awareness, effectively addressing the prevalent error accumulation issue. At its core, MVDiffusion processes perspective images in parallel with a pre-trained text-to-image diffusion model, while integrating novel correspondence-aware attention layers to facilitate cross-view interactions. For panorama generation, while only trained with 10k panoramas, MVDiffusion is able to generate high-resolution photorealistic images for arbitrary texts or extrapolate one perspective image to a 360-degree view. For multi-view depth-to-image generation, MVDiffusion demonstrates state-of-the-art performance for texturing a scene mesh. The project page is at \\url{', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'This paper introduces \\textit{MVDiffusion}, a simple yet effective method for generating consistent multi-view images from text prompts given pixel-to-pixel correspondences (\\eg, perspective crops from a panorama or multi-view images given depth maps and poses). Unlike prior methods that rely on iterative image warping and inpainting, MVDiffusion simultaneously generates all images with a global awareness, effectively addressing the prevalent error accumulation issue. At its core, MVDiffusion processes perspective images in parallel with a pre-trained text-to-image diffusion model, while integrating novel correspondence-aware attention layers to facilitate cross-view interactions. For panorama generation, while only trained with 10k panoramas, MVDiffusion is able to generate high-resolution photorealistic images for arbitrary texts or extrapolate one perspective image to a 360-degree view. For multi-view depth-to-image generation, MVDiffusion demonstrates state-of-the-art performance for texturing a scene mesh. The project page is at \\url{', 'href': None}, {'type': 'text', 'text': {'content': 'https://mvdiffusion.github.io/', 'link': {'url': 'https://mvdiffusion.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://mvdiffusion.github.io/', 'href': 'https://mvdiffusion.github.io/'}, {'type': 'text', 'text': {'content': '}.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': '}.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'mvdiffusion_teaser.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/1ae24cf4-18ab-4c9b-a147-dd9297c251bd/mvdiffusion_teaser.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234740Z&X-Amz-Expires=3600&X-Amz-Signature=2433ef88e424233c2998647f435725b9a795da49aba0a028864f93b245fa102c&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:40.279Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://mvdiffusion.github.io/', 'link': {'url': 'https://mvdiffusion.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://mvdiffusion.github.io/', 'href': 'https://mvdiffusion.github.io/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'NeurIPS 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'NeurIPS 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '0e2455bd-a873-4099-8102-14fa4f1ff893'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'bb1c7b2a-93d8-4f0d-9e32-2a3594a652cd', 'name': 'NeurIPS', 'color': 'default'}, {'id': '898c1b27-4fe4-4c46-8f37-c2c8cdfe0997', 'name': 'Yasu', 'color': 'red'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2307.01097', 'link': {'url': 'https://arxiv.org/abs/2307.01097'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2307.01097', 'href': 'https://arxiv.org/abs/2307.01097'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'MVDiffusion: Enabling Holistic Multi-view Image Generation with Correspondence-Aware Diffusion', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'MVDiffusion: Enabling Holistic Multi-view Image Generation with Correspondence-Aware Diffusion', 'href': None}]}}",https://www.notion.so/MVDiffusion-Enabling-Holistic-Multi-view-Image-Generation-with-Correspondence-Aware-Diffusion-52024903b5ec47c083bfa5f65531d5f8,https://yanxg.notion.site/MVDiffusion-Enabling-Holistic-Multi-view-Image-Generation-with-Correspondence-Aware-Diffusion-52024903b5ec47c083bfa5f65531d5f8
37,page,e465f55d-74c4-42c4-bb93-c056ef05e56a,2023-10-26T02:49:00.000Z,2023-10-26T02:51:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-10-26T02:51:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-12-10', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Fangcheng Zhong, Kyle Fogarty, Param Hanji, Tianhao Wu, Alejandro Sztrajman, Andrew Spielberg, Andrea Tagliasacchi, Petra Bosilj, Cengiz Oztireli', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Fangcheng Zhong, Kyle Fogarty, Param Hanji, Tianhao Wu, Alejandro Sztrajman, Andrew Spielberg, Andrea Tagliasacchi, Petra Bosilj, Cengiz Oztireli', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'While deep learning techniques have become extremely popular for solving a broad range of optimization problems, methods to enforce hard constraints during optimization, particularly on deep neural networks, remain underdeveloped. Inspired by the rich literature on meshless interpolation and its extension to spectral collocation methods in scientific computing, we develop a series of approaches for enforcing hard constraints on neural fields, which we refer to as \\emph{Constrained Neural Fields} (CNF). The constraints can be specified as a linear operator applied to the neural field and its derivatives. We also design specific model representations and training strategies for problems where standard models may encounter difficulties, such as conditioning of the system, memory consumption, and capacity of the network when being constrained. Our approaches are demonstrated in a wide range of real-world applications. Additionally, we develop a framework that enables highly efficient model and constraint specification, which can be readily applied to any downstream task where hard constraints need to be explicitly satisfied during optimization.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'While deep learning techniques have become extremely popular for solving a broad range of optimization problems, methods to enforce hard constraints during optimization, particularly on deep neural networks, remain underdeveloped. Inspired by the rich literature on meshless interpolation and its extension to spectral collocation methods in scientific computing, we develop a series of approaches for enforcing hard constraints on neural fields, which we refer to as \\emph{Constrained Neural Fields} (CNF). The constraints can be specified as a linear operator applied to the neural field and its derivatives. We also design specific model representations and training strategies for problems where standard models may encounter difficulties, such as conditioning of the system, memory consumption, and capacity of the network when being constrained. Our approaches are demonstrated in a wide range of real-world applications. Additionally, we develop a framework that enables highly efficient model and constraint specification, which can be readily applied to any downstream task where hard constraints need to be explicitly satisfied during optimization.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://taiya.github.io/images/placeholder.jpg', 'type': 'external', 'external': {'url': 'https://taiya.github.io/images/placeholder.jpg'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2306.08943', 'link': {'url': 'https://arxiv.org/abs/2306.08943'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2306.08943', 'href': 'https://arxiv.org/abs/2306.08943'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'NeurIPS 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'NeurIPS 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'bb1c7b2a-93d8-4f0d-9e32-2a3594a652cd', 'name': 'NeurIPS', 'color': 'default'}, {'id': '1a31dc62-0b3c-4bfe-8684-3ffe669325d3', 'name': 'Andrea', 'color': 'purple'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2306.08943', 'link': {'url': 'https://arxiv.org/abs/2306.08943'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2306.08943', 'href': 'https://arxiv.org/abs/2306.08943'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Neural Fields with Hard Constraints of Arbitrary Differential Order', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Neural Fields with Hard Constraints of Arbitrary Differential Order', 'href': None}]}}",https://www.notion.so/Neural-Fields-with-Hard-Constraints-of-Arbitrary-Differential-Order-e465f55d74c442c4bb93c056ef05e56a,https://yanxg.notion.site/Neural-Fields-with-Hard-Constraints-of-Arbitrary-Differential-Order-e465f55d74c442c4bb93c056ef05e56a
38,page,e40d79ba-70ee-4aee-86a4-023047c0b26d,2023-10-26T02:48:00.000Z,2023-10-26T02:51:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-10-26T02:51:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-12-10', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Eric Hedlin, Gopal Sharma, Shweta Mahajan, Hossam Isack, Abhishek Kar, Andrea Tagliasacchi, Kwang Moo Yi', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Eric Hedlin, Gopal Sharma, Shweta Mahajan, Hossam Isack, Abhishek Kar, Andrea Tagliasacchi, Kwang Moo Yi', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Text-to-image diffusion models are now capable of generating images that are often indistinguishable from real images. To generate such images, these models must understand the semantics of the objects they are asked to generate. In this work we show that, without any training, one can leverage this semantic knowledge within diffusion models to find semantic correspondences -- locations in multiple images that have the same semantic meaning. Specifically, given an image, we optimize the prompt embeddings of these models for maximum attention on the regions of interest. These optimized embeddings capture semantic information about the location, which can then be transferred to another image. By doing so we obtain results on par with the strongly supervised state of the art on the PF-Willow dataset and significantly outperform (20.9% relative for the SPair-71k dataset) any existing weakly or unsupervised method on PF-Willow, CUB-200 and SPair-71k datasets.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Text-to-image diffusion models are now capable of generating images that are often indistinguishable from real images. To generate such images, these models must understand the semantics of the objects they are asked to generate. In this work we show that, without any training, one can leverage this semantic knowledge within diffusion models to find semantic correspondences -- locations in multiple images that have the same semantic meaning. Specifically, given an image, we optimize the prompt embeddings of these models for maximum attention on the regions of interest. These optimized embeddings capture semantic information about the location, which can then be transferred to another image. By doing so we obtain results on par with the strongly supervised state of the art on the PF-Willow dataset and significantly outperform (20.9% relative for the SPair-71k dataset) any existing weakly or unsupervised method on PF-Willow, CUB-200 and SPair-71k datasets.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://taiya.github.io/images/placeholder.jpg', 'type': 'external', 'external': {'url': 'https://taiya.github.io/images/placeholder.jpg'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://ubc-vision.github.io/LDM_correspondences/', 'link': {'url': 'https://ubc-vision.github.io/LDM_correspondences/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://ubc-vision.github.io/LDM_correspondences/', 'href': 'https://ubc-vision.github.io/LDM_correspondences/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'NeurIPS 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'NeurIPS 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'bb1c7b2a-93d8-4f0d-9e32-2a3594a652cd', 'name': 'NeurIPS', 'color': 'default'}, {'id': '1a31dc62-0b3c-4bfe-8684-3ffe669325d3', 'name': 'Andrea', 'color': 'purple'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2305.15581', 'link': {'url': 'https://arxiv.org/abs/2306.08943'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2305.15581', 'href': 'https://arxiv.org/abs/2306.08943'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Unsupervised Semantic Correspondence Using Stable Diffusion', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Unsupervised Semantic Correspondence Using Stable Diffusion', 'href': None}]}}",https://www.notion.so/Unsupervised-Semantic-Correspondence-Using-Stable-Diffusion-e40d79ba70ee4aee86a4023047c0b26d,https://yanxg.notion.site/Unsupervised-Semantic-Correspondence-Using-Stable-Diffusion-e40d79ba70ee4aee86a4023047c0b26d
39,page,062f2450-a093-4a1d-afc7-127b9ff2af39,2023-10-26T02:00:00.000Z,2023-10-26T02:09:00.000Z,"{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}","{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-10-26T02:09:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-12-10', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Zahra Gharaee, Zeming Gong, Nicholas Pellegrino, Iuliia Zarubiieva,\xa0Joakim Bruslund Haurum,\xa0Scott C Lowe, Jaclyn TA McKeown, Chris CY Ho, Joschka McLeod, Yi-Yun C Wei, Jireh Agda, Sujeevan Ratnasingham, Dirk Steinke,\xa0Angel X. Chang,', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Zahra Gharaee, Zeming Gong, Nicholas Pellegrino, Iuliia Zarubiieva,\xa0Joakim Bruslund Haurum,\xa0Scott C Lowe, Jaclyn TA McKeown, Chris CY Ho, Joschka McLeod, Yi-Yun C Wei, Jireh Agda, Sujeevan Ratnasingham, Dirk Steinke,\xa0Angel X. Chang,', 'href': None}, {'type': 'text', 'text': {'content': '\xa0', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': '\xa0', 'href': None}, {'type': 'text', 'text': {'content': 'Graham W Taylor,\xa0Paul Fieguth', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Graham W Taylor,\xa0Paul Fieguth', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'In an effort to catalog insect biodiversity, we propose a new large dataset of hand-labelled insect images, the BIOSCAN-Insect Dataset. Each record is taxonomically classified by an expert, and also has associated genetic information including raw nucleotide barcode sequences and assigned barcode index numbers, which are genetically-based proxies for species classification. This paper presents a curated million-image dataset, primarily to train computer-vision models capable of providing image-based taxonomic assessment, however, the dataset also presents compelling characteristics, the study of which would be of interest to the broader machine learning community. Driven by the biological nature inherent to the dataset, a characteristic long-tailed class-imbalance distribution is exhibited. Furthermore, taxonomic labelling is a hierarchical classification scheme, presenting a highly fine-grained classification problem at lower levels. Beyond spurring interest in biodiversity research within the machine learning community, progress on creating an image-based taxonomic classifier will also further the ultimate goal of all BIOSCAN research: to lay the foundation for a comprehensive survey of global biodiversity. This paper introduces the dataset and explores the classification task through the implementation and analysis of a baseline classifier.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'In an effort to catalog insect biodiversity, we propose a new large dataset of hand-labelled insect images, the BIOSCAN-Insect Dataset. Each record is taxonomically classified by an expert, and also has associated genetic information including raw nucleotide barcode sequences and assigned barcode index numbers, which are genetically-based proxies for species classification. This paper presents a curated million-image dataset, primarily to train computer-vision models capable of providing image-based taxonomic assessment, however, the dataset also presents compelling characteristics, the study of which would be of interest to the broader machine learning community. Driven by the biological nature inherent to the dataset, a characteristic long-tailed class-imbalance distribution is exhibited. Furthermore, taxonomic labelling is a hierarchical classification scheme, presenting a highly fine-grained classification problem at lower levels. Beyond spurring interest in biodiversity research within the machine learning community, progress on creating an image-based taxonomic classifier will also further the ultimate goal of all BIOSCAN research: to lay the foundation for a comprehensive survey of global biodiversity. This paper introduces the dataset and explores the classification task through the implementation and analysis of a baseline classifier.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'bioscan1m.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/f8b23333-750f-442c-88b7-76d10c28bb28/bioscan1m.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234740Z&X-Amz-Expires=3600&X-Amz-Signature=e27aff161827944e82799327b501c385119ab2119b8d8a3e18533c5e7c5f9c12&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:40.280Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://biodiversitygenomics.net/1M_insects/', 'link': {'url': 'https://biodiversitygenomics.net/1M_insects/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://biodiversitygenomics.net/1M_insects/', 'href': 'https://biodiversitygenomics.net/1M_insects/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'NeurIPS Datasets and Benchmarks Track 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'NeurIPS Datasets and Benchmarks Track 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '6a51b57b-f1f8-4b18-ac73-a0ace74eb41a', 'name': 'NeurIPS Datasets and Benchmarks', 'color': 'blue'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2307.10455', 'link': {'url': 'https://arxiv.org/abs/2307.10455'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2307.10455', 'href': 'https://arxiv.org/abs/2307.10455'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset', 'href': None}]}}",https://www.notion.so/A-Step-Towards-Worldwide-Biodiversity-Assessment-The-BIOSCAN-1M-Insect-Dataset-062f2450a0934a1dafc7127b9ff2af39,https://yanxg.notion.site/A-Step-Towards-Worldwide-Biodiversity-Assessment-The-BIOSCAN-1M-Insect-Dataset-062f2450a0934a1dafc7127b9ff2af39
40,page,b595d4de-624c-430c-a679-0429d50269dc,2023-10-25T23:13:00.000Z,2023-11-22T03:13:00.000Z,"{'object': 'user', 'id': '1d0f9cba-2d4d-4ca0-94d8-926cdf288b37'}","{'object': 'user', 'id': 'b8d7e6d8-82a6-4b57-b07a-0fdcc828f6c5'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-11-22T03:13:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-12-10', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Yanshu Zhang*,\xa0Shichong Peng*,\xa0Seyed Alireza Moazenipourasil,\xa0Ke Li', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Yanshu Zhang*,\xa0Shichong Peng*,\xa0Seyed Alireza Moazenipourasil,\xa0Ke Li', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'b8d7e6d8-82a6-4b57-b07a-0fdcc828f6c5'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Given a set of images from different views and their corresponding camera poses, PAPR learns a point-based surface representation of the scene and a rendering pipeline from scratch. Additionally, PAPR enables practical applications such as\xa0geometry editing,\xa0object manipulation,\xa0texture transfer, and\xa0exposure control.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Given a set of images from different views and their corresponding camera poses, PAPR learns a point-based surface representation of the scene and a rendering pipeline from scratch. Additionally, PAPR enables practical applications such as\xa0geometry editing,\xa0object manipulation,\xa0texture transfer, and\xa0exposure control.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'papr-thumbnail2.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/cb623732-7a24-44bf-b9de-f5bd5be90bad/papr-thumbnail2.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234740Z&X-Amz-Expires=3600&X-Amz-Signature=9b372190110592fc126d3f1898b80425dede2198b541c4c710393a49390d7c04&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:40.280Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://zvict.github.io/papr/', 'link': {'url': 'https://zvict.github.io/papr/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://zvict.github.io/papr/', 'href': 'https://zvict.github.io/papr/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'NeurIPS 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'NeurIPS 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '1d0f9cba-2d4d-4ca0-94d8-926cdf288b37'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'bb1c7b2a-93d8-4f0d-9e32-2a3594a652cd', 'name': 'NeurIPS', 'color': 'default'}, {'id': '9ea36cd3-48d3-462d-baf9-b75ef77f4121', 'name': 'Ke', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2307.11086', 'link': {'url': 'https://arxiv.org/abs/2307.11086'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2307.11086', 'href': 'https://arxiv.org/abs/2307.11086'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'PAPR: Proximity Attention Point Rendering', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'PAPR: Proximity Attention Point Rendering', 'href': None}]}}",https://www.notion.so/PAPR-Proximity-Attention-Point-Rendering-b595d4de624c430ca6790429d50269dc,https://yanxg.notion.site/PAPR-Proximity-Attention-Point-Rendering-b595d4de624c430ca6790429d50269dc
41,page,e9f11fd2-9460-41d8-8179-5cd059dcdb98,2023-10-25T17:45:00.000Z,2023-11-05T06:27:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-11-05T06:27:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-12-10', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Sepidehsadat Hosseini, Mohammad Amin Shabani, Saghar Irandoust, Yasutaka Furukawa', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Sepidehsadat Hosseini, Mohammad Amin Shabani, Saghar Irandoust, Yasutaka Furukawa', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'This paper presents an end-to-end neural architecture based on Diffusion Models\nfor spatial puzzle solving, particularly jigsaw puzzle and room arrangement tasks.\nIn the latter task, for instance, the proposed system takes a set of room layouts\nas polygonal curves in the top-down view and aligns the room layout pieces by\nestimating their 2D translations and rotations, akin to solving the jigsaw puzzle\nof room layouts. A surprising discovery of the paper is that the simple use of\na Diffusion Model effectively solves these challenging spatial puzzle tasks as a\nconditional generation process. To enable learning of an end-to-end neural system,\nthe paper introduces new datasets with ground-truth arrangements: 1) 2D Voronoi\njigsaw dataset, a synthetic one where pieces are generated by Voronoi diagram\nof 2D pointset; and 2) MagicPlan dataset, a real one offered by MagicPlan from\nits production pipeline, where pieces are room layouts constructed by augmented\nreality App by real-estate consumers. The qualitative and quantitative evaluations\ndemonstrate that our approach outperforms the competing methods by significant\nmargins in all the tasks. We will publicly share all our code and data.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'This paper presents an end-to-end neural architecture based on Diffusion Models\nfor spatial puzzle solving, particularly jigsaw puzzle and room arrangement tasks.\nIn the latter task, for instance, the proposed system takes a set of room layouts\nas polygonal curves in the top-down view and aligns the room layout pieces by\nestimating their 2D translations and rotations, akin to solving the jigsaw puzzle\nof room layouts. A surprising discovery of the paper is that the simple use of\na Diffusion Model effectively solves these challenging spatial puzzle tasks as a\nconditional generation process. To enable learning of an end-to-end neural system,\nthe paper introduces new datasets with ground-truth arrangements: 1) 2D Voronoi\njigsaw dataset, a synthetic one where pieces are generated by Voronoi diagram\nof 2D pointset; and 2) MagicPlan dataset, a real one offered by MagicPlan from\nits production pipeline, where pieces are room layouts constructed by augmented\nreality App by real-estate consumers. The qualitative and quantitative evaluations\ndemonstrate that our approach outperforms the competing methods by significant\nmargins in all the tasks. We will publicly share all our code and data.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'dataset2.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/1dfce877-56d4-4f3c-9c8d-283e92dd4c10/dataset2.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234740Z&X-Amz-Expires=3600&X-Amz-Signature=25f8b251345a194f0a661890afa8da860a29e47eeb6650af97602647fc96baad&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:40.281Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://github.com/sepidsh/PuzzleFussion', 'link': {'url': 'https://github.com/sepidsh/PuzzleFussion'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://github.com/sepidsh/PuzzleFussion', 'href': 'https://github.com/sepidsh/PuzzleFussion'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'NeurIPS 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'NeurIPS 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'bb1c7b2a-93d8-4f0d-9e32-2a3594a652cd', 'name': 'NeurIPS', 'color': 'default'}, {'id': '898c1b27-4fe4-4c46-8f37-c2c8cdfe0997', 'name': 'Yasu', 'color': 'red'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/pdf/2211.13785v2.pdf', 'link': {'url': 'https://arxiv.org/pdf/2211.13785v2.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/pdf/2211.13785v2.pdf', 'href': 'https://arxiv.org/pdf/2211.13785v2.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'PuzzleFusion: Unleashing the Power of Diffusion Models for Spatial Puzzle Solving', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'PuzzleFusion: Unleashing the Power of Diffusion Models for Spatial Puzzle Solving', 'href': None}]}}",https://www.notion.so/PuzzleFusion-Unleashing-the-Power-of-Diffusion-Models-for-Spatial-Puzzle-Solving-e9f11fd2946041d881795cd059dcdb98,https://yanxg.notion.site/PuzzleFusion-Unleashing-the-Power-of-Diffusion-Models-for-Spatial-Puzzle-Solving-e9f11fd2946041d881795cd059dcdb98
42,page,10ad0ead-0668-44d3-8be8-e1627048574a,2023-10-25T18:01:00.000Z,2023-10-26T02:17:00.000Z,"{'object': 'user', 'id': '957d7586-6e7c-4058-a945-a39fb973a60a'}","{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-10-26T02:17:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-11-20', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Sepidehsadat Hosseini, Yasutaka Furukawa', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Sepidehsadat Hosseini, Yasutaka Furukawa', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'teaser.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/0c1a87a7-75d8-4b7f-8a5c-9cd4a02feca6/teaser.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234740Z&X-Amz-Expires=3600&X-Amz-Signature=d987cbdf13dc11d1aa5dd6dd75e4690dc1b8239ba5a9349d9f290abecb4f60f6&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:40.281Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://sepidsh.github.io/floorplan_restore/', 'link': {'url': 'https://sepidsh.github.io/floorplan_restore/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://sepidsh.github.io/floorplan_restore/', 'href': 'https://sepidsh.github.io/floorplan_restore/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'BMVC 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'BMVC 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '957d7586-6e7c-4058-a945-a39fb973a60a'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '898c1b27-4fe4-4c46-8f37-c2c8cdfe0997', 'name': 'Yasu', 'color': 'red'}, {'id': '2c73299d-5d90-4ce0-bdf0-5a4fb4bf7e49', 'name': 'BMVC', 'color': 'orange'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/pdf/2206.00645.pdf', 'link': {'url': 'https://arxiv.org/pdf/2206.00645.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/pdf/2206.00645.pdf', 'href': 'https://arxiv.org/pdf/2206.00645.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '❌❌❌❌'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Floorplan Restoration by Structure Hallucinating Transformer Cascades', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Floorplan Restoration by Structure Hallucinating Transformer Cascades', 'href': None}]}}",https://www.notion.so/Floorplan-Restoration-by-Structure-Hallucinating-Transformer-Cascades-10ad0ead066844d38be8e1627048574a,https://yanxg.notion.site/Floorplan-Restoration-by-Structure-Hallucinating-Transformer-Cascades-10ad0ead066844d38be8e1627048574a
43,page,1b12c251-91ff-4e6d-9098-46e325abba8f,2023-10-26T01:54:00.000Z,2023-10-26T02:10:00.000Z,"{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}","{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-10-26T02:10:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-11-06', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Sriram Yenamandra, Arun Ramachandran,\xa0Karmesh Yadav, Austin Wang,\xa0Mukul Khanna,\xa0Theo Gervet, Tsung-Yen Yang, Vidhi Jain, Alexander William Clegg, John Turner,\xa0Zsolt Kira,\xa0Manolis Savva,\xa0Angel X. Chang,', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Sriram Yenamandra, Arun Ramachandran,\xa0Karmesh Yadav, Austin Wang,\xa0Mukul Khanna,\xa0Theo Gervet, Tsung-Yen Yang, Vidhi Jain, Alexander William Clegg, John Turner,\xa0Zsolt Kira,\xa0Manolis Savva,\xa0Angel X. Chang,', 'href': None}, {'type': 'text', 'text': {'content': '\xa0', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': '\xa0', 'href': None}, {'type': 'text', 'text': {'content': 'Devendra Chaplot,\xa0Dhruv Batra,\xa0Roozbeh Mottaghi,\xa0Yonatan Bisk,\xa0Chris Paxton', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Devendra Chaplot,\xa0Dhruv Batra,\xa0Roozbeh Mottaghi,\xa0Yonatan Bisk,\xa0Chris Paxton', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'HomeRobot (noun): An affordable compliant robot that navigates homes and manipulates a wide range of objects in order to complete everyday tasks. Open-Vocabulary Mobile Manipulation (OVMM) is the problem of picking any object in any unseen environment, and placing it in a commanded location. This is a foundational challenge for robots to be useful assistants in human environments, because it involves tackling sub-problems from across robotics: perception, language understanding, navigation, and manipulation are all essential to OVMM. In addition, integration of the solutions to these sub-problems poses its own substantial challenges. To drive research in this area, we introduce the HomeRobot OVMM benchmark, where an agent navigates household environments to grasp novel objects and place them on target receptacles. HomeRobot has two components: a simulation component, which uses a large and diverse curated object set in new, high-quality multi-room home environments; and a real-world component, providing a software stack for the low-cost Hello Robot Stretch to encourage replication of real-world experiments across labs. We implement both reinforcement learning and heuristic (model-based) baselines and show evidence of sim-to-real transfer. Our baselines achieve a 20% success rate in the real world; our experiments identify ways future research work improve performance. See videos on our website:\xa0', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'HomeRobot (noun): An affordable compliant robot that navigates homes and manipulates a wide range of objects in order to complete everyday tasks. Open-Vocabulary Mobile Manipulation (OVMM) is the problem of picking any object in any unseen environment, and placing it in a commanded location. This is a foundational challenge for robots to be useful assistants in human environments, because it involves tackling sub-problems from across robotics: perception, language understanding, navigation, and manipulation are all essential to OVMM. In addition, integration of the solutions to these sub-problems poses its own substantial challenges. To drive research in this area, we introduce the HomeRobot OVMM benchmark, where an agent navigates household environments to grasp novel objects and place them on target receptacles. HomeRobot has two components: a simulation component, which uses a large and diverse curated object set in new, high-quality multi-room home environments; and a real-world component, providing a software stack for the low-cost Hello Robot Stretch to encourage replication of real-world experiments across labs. We implement both reinforcement learning and heuristic (model-based) baselines and show evidence of sim-to-real transfer. Our baselines achieve a 20% success rate in the real world; our experiments identify ways future research work improve performance. See videos on our website:\xa0', 'href': None}, {'type': 'text', 'text': {'content': 'this https URL', 'link': {'url': 'https://ovmm.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'this https URL', 'href': 'https://ovmm.github.io/'}, {'type': 'text', 'text': {'content': '.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': '.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'ovmm.jpeg', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/5540b6c9-77d5-4d68-89cd-07a836f8d185/ovmm.jpeg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234740Z&X-Amz-Expires=3600&X-Amz-Signature=368b02502640fdb0f205d6e5e28c6fb7db25ab94b733545b7043953d44f383b6&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:40.282Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://ovmm.github.io/', 'link': {'url': 'https://ovmm.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://ovmm.github.io/', 'href': 'https://ovmm.github.io/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CoRL 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CoRL 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://www.youtube.com/watch?v=EA8HEzopkc0', 'link': {'url': 'https://www.youtube.com/watch?v=EA8HEzopkc0'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://www.youtube.com/watch?v=EA8HEzopkc0', 'href': 'https://www.youtube.com/watch?v=EA8HEzopkc0'}]}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '5ded552f-60de-4443-b51a-741164f786fd', 'name': 'ICCV', 'color': 'red'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}, {'id': 'bccbeae3-55ce-4b38-9abc-61c658ca30b4', 'name': 'Manolis', 'color': 'blue'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2306.11565', 'link': {'url': 'https://arxiv.org/abs/2306.11565'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2306.11565', 'href': 'https://arxiv.org/abs/2306.11565'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'HomeRobot: Open Vocabulary Mobile Manipulation', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'HomeRobot: Open Vocabulary Mobile Manipulation', 'href': None}]}}",https://www.notion.so/HomeRobot-Open-Vocabulary-Mobile-Manipulation-1b12c25191ff4e6d909846e325abba8f,https://yanxg.notion.site/HomeRobot-Open-Vocabulary-Mobile-Manipulation-1b12c25191ff4e6d909846e325abba8f
44,page,739be168-bd35-43c7-927c-10be6c4196ef,2023-11-22T03:13:00.000Z,2024-02-23T23:54:00.000Z,"{'object': 'user', 'id': 'b8d7e6d8-82a6-4b57-b07a-0fdcc828f6c5'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2024-02-23T23:54:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-10-28', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Chris Careaga and Yağız Aksoy', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Chris Careaga and Yağız Aksoy', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://yaksoy.github.io/images/research/intrinsic.jpg', 'type': 'external', 'external': {'url': 'https://yaksoy.github.io/images/research/intrinsic.jpg'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://yaksoy.github.io/intrinsic/', 'link': {'url': 'https://yaksoy.github.io/intrinsic/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://yaksoy.github.io/intrinsic/', 'href': 'https://yaksoy.github.io/intrinsic/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ACM Transactions on Graphics, Vol. 43, Issue 1, Article 12', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ACM Transactions on Graphics, Vol. 43, Issue 1, Article 12', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'b8d7e6d8-82a6-4b57-b07a-0fdcc828f6c5'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://youtu.be/pWtJd3hqL3c', 'link': {'url': 'https://youtu.be/pWtJd3hqL3c'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://youtu.be/pWtJd3hqL3c', 'href': 'https://youtu.be/pWtJd3hqL3c'}]}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'f48bbf5b-5539-4d7c-a7f0-43e78134e9fa', 'name': 'TOG', 'color': 'brown'}, {'id': '3eb057d5-ed85-4850-b9ac-ff060c999382', 'name': 'Yagiz', 'color': 'gray'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://yaksoy.github.io/papers/TOG23-Intrinsic.pdf', 'link': {'url': 'https://yaksoy.github.io/papers/TOG23-Intrinsic.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://yaksoy.github.io/papers/TOG23-Intrinsic.pdf', 'href': 'https://yaksoy.github.io/papers/TOG23-Intrinsic.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '❌❌❌❌'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Intrinsic Image Decomposition via Ordinal Shading', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Intrinsic Image Decomposition via Ordinal Shading', 'href': None}]}}",https://www.notion.so/Intrinsic-Image-Decomposition-via-Ordinal-Shading-739be168bd3543c7927c10be6c4196ef,https://yanxg.notion.site/Intrinsic-Image-Decomposition-via-Ordinal-Shading-739be168bd3543c7927c10be6c4196ef
45,page,cdc1a96a-7fa0-4e3b-8f2b-4ba676ac8d73,2023-10-26T01:54:00.000Z,2023-10-26T21:01:00.000Z,"{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}","{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-10-26T21:01:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-10-02', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Enrico Cancelli,\xa0Tommaso Campari,\xa0Luciano Serafini,\xa0Angel X. Chang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Enrico Cancelli,\xa0Tommaso Campari,\xa0Luciano Serafini,\xa0Angel X. Chang', 'href': None}, {'type': 'text', 'text': {'content': ',\xa0', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': ',\xa0', 'href': None}, {'type': 'text', 'text': {'content': 'Lamberto Ballan', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Lamberto Ballan', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Learning how to navigate among humans in an occluded and spatially constrained indoor environment, is a key ability required to embodied agent to be integrated into our society. In this paper, we propose an end-to-end architecture that exploits Proximity-Aware Tasks (referred as to Risk and Proximity Compass) to inject into a reinforcement learning navigation policy the ability to infer common-sense social behaviors. To this end, our tasks exploit the notion of immediate and future dangers of collision. Furthermore, we propose an evaluation protocol specifically designed for the Social Navigation Task in simulated environments. This is done to capture fine-grained features and characteristics of the policy by analyzing the minimal unit of human-robot spatial interaction, called Encounter. We validate our approach on Gibson4+ and Habitat-Matterport3D datasets.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Learning how to navigate among humans in an occluded and spatially constrained indoor environment, is a key ability required to embodied agent to be integrated into our society. In this paper, we propose an end-to-end architecture that exploits Proximity-Aware Tasks (referred as to Risk and Proximity Compass) to inject into a reinforcement learning navigation policy the ability to infer common-sense social behaviors. To this end, our tasks exploit the notion of immediate and future dangers of collision. Furthermore, we propose an evaluation protocol specifically designed for the Social Navigation Task in simulated environments. This is done to capture fine-grained features and characteristics of the policy by analyzing the minimal unit of human-robot spatial interaction, called Encounter. We validate our approach on Gibson4+ and Habitat-Matterport3D datasets.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'prox.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/98c951ee-51cf-449f-b00f-3974721133f9/prox.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234740Z&X-Amz-Expires=3600&X-Amz-Signature=3ea1321bf90a9d55ef9f1d9587412bf77105a1855b6d21cb07cb065c5a9e2aab&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:40.283Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2212.00767', 'link': {'url': 'https://arxiv.org/abs/2212.00767'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2212.00767', 'href': 'https://arxiv.org/abs/2212.00767'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ICCV 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ICCV 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '5ded552f-60de-4443-b51a-741164f786fd', 'name': 'ICCV', 'color': 'red'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2212.00767', 'link': {'url': 'https://arxiv.org/abs/2212.00767.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2212.00767', 'href': 'https://arxiv.org/abs/2212.00767.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Exploiting Proximity-Aware Tasks for Embodied Social Navigation', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Exploiting Proximity-Aware Tasks for Embodied Social Navigation', 'href': None}]}}",https://www.notion.so/Exploiting-Proximity-Aware-Tasks-for-Embodied-Social-Navigation-cdc1a96a7fa04e3b8f2b4ba676ac8d73,https://yanxg.notion.site/Exploiting-Proximity-Aware-Tasks-for-Embodied-Social-Navigation-cdc1a96a7fa04e3b8f2b4ba676ac8d73
46,page,96b1c296-9893-4784-801a-f0daed0f43ef,2023-09-18T23:26:00.000Z,2023-09-25T23:41:00.000Z,"{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-09-25T23:41:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-10-02', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Dave Zhenyu Chen,\xa0Ronghang Hu,\xa0Xinlei Chen,\xa0Matthias Nießner,\xa0Angel X. Chang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Dave Zhenyu Chen,\xa0Ronghang Hu,\xa0Xinlei Chen,\xa0Matthias Nießner,\xa0Angel X. Chang', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Performing 3D dense captioning and visual grounding requires a common and shared understanding of the underlying multimodal relationships. However, despite some previous attempts on connecting these two related tasks with highly task-specific neural modules, it remains understudied how to explicitly depict their shared nature to learn them simultaneously. In this work, we propose UniT3D, a simple yet effective fully unified transformer-based architecture for jointly solving 3D visual grounding and dense captioning. UniT3D enables learning a strong multimodal representation across the two tasks through a supervised joint pre-training scheme with bidirectional and seq-to-seq objectives. With a generic architecture design, UniT3D allows expanding the pre-training scope to more various training sources such as the synthesized data from 2D prior knowledge to benefit 3D vision-language tasks. Extensive experiments and analysis demonstrate that UniT3D obtains significant gains for 3D dense captioning and visual grounding.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Performing 3D dense captioning and visual grounding requires a common and shared understanding of the underlying multimodal relationships. However, despite some previous attempts on connecting these two related tasks with highly task-specific neural modules, it remains understudied how to explicitly depict their shared nature to learn them simultaneously. In this work, we propose UniT3D, a simple yet effective fully unified transformer-based architecture for jointly solving 3D visual grounding and dense captioning. UniT3D enables learning a strong multimodal representation across the two tasks through a supervised joint pre-training scheme with bidirectional and seq-to-seq objectives. With a generic architecture design, UniT3D allows expanding the pre-training scope to more various training sources such as the synthesized data from 2D prior knowledge to benefit 3D vision-language tasks. Extensive experiments and analysis demonstrate that UniT3D obtains significant gains for 3D dense captioning and visual grounding.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'unit3d.jpg', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/412818c8-c973-4209-b9a6-8cad6cbdf2c4/unit3d.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234740Z&X-Amz-Expires=3600&X-Amz-Signature=b76f1718e81d66cccc012e947f34dc4a227fd5324910e4e7f016b25eb9843c1d&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:40.282Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2212.00836', 'link': {'url': 'https://arxiv.org/abs/2212.00836'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2212.00836', 'href': 'https://arxiv.org/abs/2212.00836'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ICCV 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ICCV 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '5ded552f-60de-4443-b51a-741164f786fd', 'name': 'ICCV', 'color': 'red'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2212.00836', 'link': {'url': 'https://arxiv.org/abs/2212.00836'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2212.00836', 'href': 'https://arxiv.org/abs/2212.00836'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'UniT3D: A Unified Transformer for 3D Dense Captioning and Visual Grounding', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'UniT3D: A Unified Transformer for 3D Dense Captioning and Visual Grounding', 'href': None}]}}",https://www.notion.so/UniT3D-A-Unified-Transformer-for-3D-Dense-Captioning-and-Visual-Grounding-96b1c29698934784801af0daed0f43ef,https://yanxg.notion.site/UniT3D-A-Unified-Transformer-for-3D-Dense-Captioning-and-Visual-Grounding-96b1c29698934784801af0daed0f43ef
47,page,97a335a8-5de9-4af8-97fe-b8cdbbc02c06,2023-09-18T22:17:00.000Z,2023-09-18T22:23:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-09-18T22:23:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-10-02', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Fenggen Yu, Yiming Qian, Francisca Gil-Ureta, Brian Jackson, Eric Bennett, and Hao Zhang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Fenggen Yu, Yiming Qian, Francisca Gil-Ureta, Brian Jackson, Eric Bennett, and Hao Zhang', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://www.cs.sfu.ca/~haoz/pubs/images/hal3d.png', 'type': 'external', 'external': {'url': 'https://www.cs.sfu.ca/~haoz/pubs/images/hal3d.png'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2301.10460', 'link': {'url': 'https://arxiv.org/abs/2301.10460'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2301.10460', 'href': 'https://arxiv.org/abs/2301.10460'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ICCV 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ICCV 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'e16d2c21-c87f-4a73-aa66-85bb6bc6e5b0', 'name': 'Richard', 'color': 'pink'}, {'id': '5ded552f-60de-4443-b51a-741164f786fd', 'name': 'ICCV', 'color': 'red'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2301.10460', 'link': {'url': 'https://arxiv.org/abs/2301.10460'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2301.10460', 'href': 'https://arxiv.org/abs/2301.10460'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '❌❌❌❌'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'HAL3D: Hierarchical Active Learning for Fine-Grained 3D Part Labeling', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'HAL3D: Hierarchical Active Learning for Fine-Grained 3D Part Labeling', 'href': None}]}}",https://www.notion.so/HAL3D-Hierarchical-Active-Learning-for-Fine-Grained-3D-Part-Labeling-97a335a85de94af897feb8cdbbc02c06,https://yanxg.notion.site/HAL3D-Hierarchical-Active-Learning-for-Fine-Grained-3D-Part-Labeling-97a335a85de94af897feb8cdbbc02c06
48,page,c34ba525-ac8d-4b9b-91e7-d412e7fce348,2023-09-18T21:52:00.000Z,2023-09-18T22:26:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-09-18T22:26:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-10-02', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Jiayi Liu,\xa0Ali Mahdavi-Amiri, Manolis Savva', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Jiayi Liu,\xa0Ali Mahdavi-Amiri, Manolis Savva', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'Snipaste_2023-09-18_14-56-16.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/93befd22-b013-4613-bee2-c9e2bb53c1a5/Snipaste_2023-09-18_14-56-16.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234740Z&X-Amz-Expires=3600&X-Amz-Signature=03d2906bc10e6b99b2000929172cd5d16baa60ca135dc5db41d16b6bd527a6a7&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:40.283Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://3dlg-hcvc.github.io/paris/', 'link': {'url': 'https://3dlg-hcvc.github.io/paris/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://3dlg-hcvc.github.io/paris/', 'href': 'https://3dlg-hcvc.github.io/paris/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ICCV 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ICCV 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'ed487f9e-f5ad-446d-89eb-2ab46ab0892c', 'name': 'Ali', 'color': 'purple'}, {'id': 'bccbeae3-55ce-4b38-9abc-61c658ca30b4', 'name': 'Manolis', 'color': 'blue'}, {'id': '5ded552f-60de-4443-b51a-741164f786fd', 'name': 'ICCV', 'color': 'red'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2303.10735', 'link': {'url': 'https://arxiv.org/abs/2308.07391'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2303.10735', 'href': 'https://arxiv.org/abs/2308.07391'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '❌❌❌❌'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'PARIS: Part-level Reconstruction and Motion Analysis for Articulated Objects', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'PARIS: Part-level Reconstruction and Motion Analysis for Articulated Objects', 'href': None}]}}",https://www.notion.so/PARIS-Part-level-Reconstruction-and-Motion-Analysis-for-Articulated-Objects-c34ba525ac8d4b9b91e7d412e7fce348,https://yanxg.notion.site/PARIS-Part-level-Reconstruction-and-Motion-Analysis-for-Articulated-Objects-c34ba525ac8d4b9b91e7d412e7fce348
49,page,76c3667b-3a91-4d35-94e6-0d33b0b6a293,2023-09-18T21:52:00.000Z,2023-09-18T22:24:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-09-18T22:24:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-10-02', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Aryan Mikaeili, Or Perel, Mehdi Safaee, Daniel Cohen-Or, Ali Mahdavi-Amiri', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Aryan Mikaeili, Or Perel, Mehdi Safaee, Daniel Cohen-Or, Ali Mahdavi-Amiri', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://sked-paper.github.io/resources/teaser.png', 'type': 'external', 'external': {'url': 'https://sked-paper.github.io/resources/teaser.png'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://sked-paper.github.io/', 'link': {'url': 'https://sked-paper.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://sked-paper.github.io/', 'href': 'https://sked-paper.github.io/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ICCV 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ICCV 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'ed487f9e-f5ad-446d-89eb-2ab46ab0892c', 'name': 'Ali', 'color': 'purple'}, {'id': '5ded552f-60de-4443-b51a-741164f786fd', 'name': 'ICCV', 'color': 'red'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2303.10735v3', 'link': {'url': 'https://arxiv.org/abs/2303.10735v3'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2303.10735v3', 'href': 'https://arxiv.org/abs/2303.10735v3'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '❌❌❌❌'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'SKED: Sketch-guided Text-based 3D Editing', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'SKED: Sketch-guided Text-based 3D Editing', 'href': None}]}}",https://www.notion.so/SKED-Sketch-guided-Text-based-3D-Editing-76c3667b3a914d3594e60d33b0b6a293,https://yanxg.notion.site/SKED-Sketch-guided-Text-based-3D-Editing-76c3667b3a914d3594e60d33b0b6a293
50,page,40055d39-bb6d-44ea-8d68-453f503ef32d,2023-09-11T20:58:00.000Z,2023-10-26T02:00:00.000Z,"{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}","{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-10-26T02:00:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-10-02', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Maham Tanveer, Yizhi Wang,\xa0Ali Mahdavi-Amiri, Hao Zhang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Maham Tanveer, Yizhi Wang,\xa0Ali Mahdavi-Amiri, Hao Zhang', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Learning how to navigate among humans in an occluded and spatially constrained indoor environment, is a key ability required to embodied agent to be integrated into our society. In this paper, we propose an end-to-end architecture that exploits Proximity Aware Tasks (referred as to Risk and Proximity Compass) to inject into a reinforcement learning navigation policy the ability to infer common-sense social behaviors. To this end, our tasks exploit the notion of immediate and future dangers of collision. Furthermore, we propose an evaluation protocol specifically designed for the Social Navigation Task in simulated environments. This is done to capture fine-grained features and characteristics of the policy by analyzing the minimal unit of human-robot spatial interaction, called Encounter. We validate our approach on Gibson4+ and Habitat-Matterport3D datasets.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Learning how to navigate among humans in an occluded and spatially constrained indoor environment, is a key ability required to embodied agent to be integrated into our society. In this paper, we propose an end-to-end architecture that exploits Proximity Aware Tasks (referred as to Risk and Proximity Compass) to inject into a reinforcement learning navigation policy the ability to infer common-sense social behaviors. To this end, our tasks exploit the notion of immediate and future dangers of collision. Furthermore, we propose an evaluation protocol specifically designed for the Social Navigation Task in simulated environments. This is done to capture fine-grained features and characteristics of the policy by analyzing the minimal unit of human-robot spatial interaction, called Encounter. We validate our approach on Gibson4+ and Habitat-Matterport3D datasets.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://www.cs.sfu.ca/~haoz/pubs/images/dsfusion.png', 'type': 'external', 'external': {'url': 'https://www.cs.sfu.ca/~haoz/pubs/images/dsfusion.png'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://ds-fusion.github.io/', 'link': {'url': 'https://ds-fusion.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://ds-fusion.github.io/', 'href': 'https://ds-fusion.github.io/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ICCV 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ICCV 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'e16d2c21-c87f-4a73-aa66-85bb6bc6e5b0', 'name': 'Richard', 'color': 'pink'}, {'id': 'ed487f9e-f5ad-446d-89eb-2ab46ab0892c', 'name': 'Ali', 'color': 'purple'}, {'id': '5ded552f-60de-4443-b51a-741164f786fd', 'name': 'ICCV', 'color': 'red'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://ds-fusion.github.io/', 'link': {'url': 'https://ds-fusion.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://ds-fusion.github.io/', 'href': 'https://ds-fusion.github.io/'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'DS-Fusion: Artistic Typography via Discriminated and Stylized Diffusion', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'DS-Fusion: Artistic Typography via Discriminated and Stylized Diffusion', 'href': None}]}}",https://www.notion.so/DS-Fusion-Artistic-Typography-via-Discriminated-and-Stylized-Diffusion-40055d39bb6d44ea8d68453f503ef32d,https://yanxg.notion.site/DS-Fusion-Artistic-Typography-via-Discriminated-and-Stylized-Diffusion-40055d39bb6d44ea8d68453f503ef32d
51,page,770d6d8f-f95b-46d8-8cab-db33928836de,2023-09-11T20:58:00.000Z,2023-10-26T01:58:00.000Z,"{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}","{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-10-26T01:58:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-10-02', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Yiming Zhang, ZeMing Gong, Angel X. Chang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Yiming Zhang, ZeMing Gong, Angel X. Chang', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'We introduce the task of localizing a flexible number of objects in real-world 3D scenes using natural language descriptions. Existing 3D visual grounding tasks focus on localizing a unique object given a text description. However, such a strict setting is unnatural as localizing potentially multiple objects is a common need in real-world scenarios and robotic tasks (e.g., visual navigation and object rearrangement). To address this setting we propose Multi3DRefer, generalizing the ScanRefer dataset and task. Our dataset contains 61926 descriptions of 11609 objects, where zero, single or multiple target objects are referenced by each description. We also introduce a new evaluation metric and benchmark methods from prior work to enable further investigation of multi-modal 3D scene understanding. Furthermore, we develop a better baseline leveraging 2D features from CLIP by rendering object proposals online with contrastive learning, which outperforms the state of the art on the ScanRefer benchmark.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'We introduce the task of localizing a flexible number of objects in real-world 3D scenes using natural language descriptions. Existing 3D visual grounding tasks focus on localizing a unique object given a text description. However, such a strict setting is unnatural as localizing potentially multiple objects is a common need in real-world scenarios and robotic tasks (e.g., visual navigation and object rearrangement). To address this setting we propose Multi3DRefer, generalizing the ScanRefer dataset and task. Our dataset contains 61926 descriptions of 11609 objects, where zero, single or multiple target objects are referenced by each description. We also introduce a new evaluation metric and benchmark methods from prior work to enable further investigation of multi-modal 3D scene understanding. Furthermore, we develop a better baseline leveraging 2D features from CLIP by rendering object proposals online with contrastive learning, which outperforms the state of the art on the ScanRefer benchmark.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'abstract-b1185106.jpg', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/aaeef2a9-fa43-468e-a201-2ed47a1c9214/abstract-b1185106.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234741Z&X-Amz-Expires=3600&X-Amz-Signature=ed4c24c9ae73cd036b53d1e7b9eecedc6da297275f28e4b5ca0722ad6376b911&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:41.262Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://3dlg-hcvc.github.io/multi3drefer/', 'link': {'url': 'https://3dlg-hcvc.github.io/multi3drefer/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://3dlg-hcvc.github.io/multi3drefer/', 'href': 'https://3dlg-hcvc.github.io/multi3drefer/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ICCV 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ICCV 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '5ded552f-60de-4443-b51a-741164f786fd', 'name': 'ICCV', 'color': 'red'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2309.05251', 'link': {'url': 'https://arxiv.org/abs/2309.05251'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2309.05251', 'href': 'https://arxiv.org/abs/2309.05251'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Multi3DRefer: Grounding Text Description to Multiple 3D Objects', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Multi3DRefer: Grounding Text Description to Multiple 3D Objects', 'href': None}]}}",https://www.notion.so/Multi3DRefer-Grounding-Text-Description-to-Multiple-3D-Objects-770d6d8ff95b46d88cabdb33928836de,https://yanxg.notion.site/Multi3DRefer-Grounding-Text-Description-to-Multiple-3D-Objects-770d6d8ff95b46d88cabdb33928836de
52,page,fd9e8fb3-ecc3-4c62-abb8-631e62ebd6e1,2023-08-05T03:12:00.000Z,2023-09-09T22:41:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '8fcf1039-1097-4bdb-abc1-f66830a27e9e'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-09-09T22:41:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-07-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Haotian Zhang, Ye Yuan, Viktor Makoviychuk, Yunrong Guo, Sanja Fidler,\xa0', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Haotian Zhang, Ye Yuan, Viktor Makoviychuk, Yunrong Guo, Sanja Fidler,\xa0', 'href': None}, {'type': 'text', 'text': {'content': 'Xue Bin Peng', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Xue Bin Peng', 'href': None}, {'type': 'text', 'text': {'content': ', Kayvon Fatahalian', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': ', Kayvon Fatahalian', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '8fcf1039-1097-4bdb-abc1-f66830a27e9e'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://xbpeng.github.io/projects/Vid2Player3D/vid2player3d_thumb.png', 'type': 'external', 'external': {'url': 'https://xbpeng.github.io/projects/Vid2Player3D/vid2player3d_thumb.png'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://xbpeng.github.io/projects/Vid2Player3D/index.html', 'link': {'url': 'https://xbpeng.github.io/projects/Vid2Player3D/index.html'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://xbpeng.github.io/projects/Vid2Player3D/index.html', 'href': 'https://xbpeng.github.io/projects/Vid2Player3D/index.html'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'SIGGRAPH 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'SIGGRAPH 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'dc677421-36af-48e6-be53-0598670c619b', 'name': 'Jason', 'color': 'red'}, {'id': '45ac99f3-aeab-4f6c-9370-e80f8adbc8c8', 'name': 'SIGGRAPH', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://xbpeng.github.io/projects/Vid2Player3D/2023_TOG_Vid2Player3D.pdf', 'link': {'url': 'https://xbpeng.github.io/projects/Vid2Player3D/2023_TOG_Vid2Player3D.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://xbpeng.github.io/projects/Vid2Player3D/2023_TOG_Vid2Player3D.pdf', 'href': 'https://xbpeng.github.io/projects/Vid2Player3D/2023_TOG_Vid2Player3D.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '❌❌❌❌'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Learning Physically Simulated Tennis Skills from Broadcast Videos', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Learning Physically Simulated Tennis Skills from Broadcast Videos', 'href': None}]}}",https://www.notion.so/Learning-Physically-Simulated-Tennis-Skills-from-Broadcast-Videos-fd9e8fb3ecc34c62abb8631e62ebd6e1,https://yanxg.notion.site/Learning-Physically-Simulated-Tennis-Skills-from-Broadcast-Videos-fd9e8fb3ecc34c62abb8631e62ebd6e1
53,page,d34f0d36-07ec-4c96-a246-3213aa65f9e0,2023-08-05T03:11:00.000Z,2023-08-05T03:12:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-08-05T03:12:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-07-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Mohamed Hassan, Yunrong Guo, Tingwu Wang, Michael Black, Sanja Fidler, Xue Bin Peng', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Mohamed Hassan, Yunrong Guo, Tingwu Wang, Michael Black, Sanja Fidler, Xue Bin Peng', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://xbpeng.github.io/projects/InterPhys/inter_phys_thumb.png', 'type': 'external', 'external': {'url': 'https://xbpeng.github.io/projects/InterPhys/inter_phys_thumb.png'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://xbpeng.github.io/projects/CALM/index.html', 'link': {'url': 'https://xbpeng.github.io/projects/CALM/index.html'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://xbpeng.github.io/projects/CALM/index.html', 'href': 'https://xbpeng.github.io/projects/CALM/index.html'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'SIGGRAPH 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'SIGGRAPH 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'dc677421-36af-48e6-be53-0598670c619b', 'name': 'Jason', 'color': 'red'}, {'id': '45ac99f3-aeab-4f6c-9370-e80f8adbc8c8', 'name': 'SIGGRAPH', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://xbpeng.github.io/projects/CALM/2023_SIGGRAPH_CALM.pdf', 'link': {'url': 'https://xbpeng.github.io/projects/CALM/2023_SIGGRAPH_CALM.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://xbpeng.github.io/projects/CALM/2023_SIGGRAPH_CALM.pdf', 'href': 'https://xbpeng.github.io/projects/CALM/2023_SIGGRAPH_CALM.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '❌❌❌❌'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Synthesizing Physical Character-Scene Interactions', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Synthesizing Physical Character-Scene Interactions', 'href': None}]}}",https://www.notion.so/Synthesizing-Physical-Character-Scene-Interactions-d34f0d3607ec4c96a2463213aa65f9e0,https://yanxg.notion.site/Synthesizing-Physical-Character-Scene-Interactions-d34f0d3607ec4c96a2463213aa65f9e0
54,page,2a5a8d54-c945-4a54-bc8f-ea9130197bc3,2023-08-05T03:08:00.000Z,2023-08-05T03:12:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-08-05T03:12:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-07-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Chen Tessler, Yoni Kasten, Yunrong Guo, Shie Mannor, Gal Chechik, Xue Bin Peng', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Chen Tessler, Yoni Kasten, Yunrong Guo, Shie Mannor, Gal Chechik, Xue Bin Peng', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://xbpeng.github.io/projects/CALM/calm_thumb.png', 'type': 'external', 'external': {'url': 'https://xbpeng.github.io/projects/CALM/calm_thumb.png'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://xbpeng.github.io/projects/CALM/index.html', 'link': {'url': 'https://xbpeng.github.io/projects/CALM/index.html'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://xbpeng.github.io/projects/CALM/index.html', 'href': 'https://xbpeng.github.io/projects/CALM/index.html'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'SIGGRAPH 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'SIGGRAPH 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'dc677421-36af-48e6-be53-0598670c619b', 'name': 'Jason', 'color': 'red'}, {'id': '45ac99f3-aeab-4f6c-9370-e80f8adbc8c8', 'name': 'SIGGRAPH', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://xbpeng.github.io/projects/CALM/2023_SIGGRAPH_CALM.pdf', 'link': {'url': 'https://xbpeng.github.io/projects/CALM/2023_SIGGRAPH_CALM.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://xbpeng.github.io/projects/CALM/2023_SIGGRAPH_CALM.pdf', 'href': 'https://xbpeng.github.io/projects/CALM/2023_SIGGRAPH_CALM.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '❌❌❌❌'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'CALM: Conditional Adversarial Latent Models for Directable Virtual Characters', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CALM: Conditional Adversarial Latent Models for Directable Virtual Characters', 'href': None}]}}",https://www.notion.so/CALM-Conditional-Adversarial-Latent-Models-for-Directable-Virtual-Characters-2a5a8d54c9454a54bc8fea9130197bc3,https://yanxg.notion.site/CALM-Conditional-Adversarial-Latent-Models-for-Directable-Virtual-Characters-2a5a8d54c9454a54bc8fea9130197bc3
55,page,59e4ea73-0aa4-4d38-8eb7-88a3428b942e,2023-06-14T19:18:00.000Z,2023-06-14T22:27:00.000Z,"{'object': 'user', 'id': '752f8d93-6b11-44d7-80a5-0d790586a229'}","{'object': 'user', 'id': '752f8d93-6b11-44d7-80a5-0d790586a229'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-06-14T22:27:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-06-14', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'S. Mahdi H. Miangoleh, Zoya Bylinskii, Eric Kee, Eli Shechtman, Yağız Aksoy', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'S. Mahdi H. Miangoleh, Zoya Bylinskii, Eric Kee, Eli Shechtman, Yağız Aksoy', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '752f8d93-6b11-44d7-80a5-0d790586a229'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': ""Common editing operations performed by professional photographers include the cleanup operations: de-emphasizing distracting elements and enhancing subjects. These edits are challenging, requiring a delicate balance between manipulating the viewer's attention while maintaining photo realism. While recent approaches can boast successful examples of attention attenuation or amplification, most of them also suffer from frequent unrealistic edits. We propose a realism loss for saliency-guided image enhancement to maintain high realism across varying image types, while attenuating distractors and amplifying objects of interest. Evaluations with professional photographers confirm that we achieve the dual objective of realism and effectiveness, and outperform the recent approaches on their own datasets, while requiring a smaller memory footprint and runtime. We thus offer a viable solution for automating image enhancement and photo cleanup operations."", 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': ""Common editing operations performed by professional photographers include the cleanup operations: de-emphasizing distracting elements and enhancing subjects. These edits are challenging, requiring a delicate balance between manipulating the viewer's attention while maintaining photo realism. While recent approaches can boast successful examples of attention attenuation or amplification, most of them also suffer from frequent unrealistic edits. We propose a realism loss for saliency-guided image enhancement to maintain high realism across varying image types, while attenuating distractors and amplifying objects of interest. Evaluations with professional photographers confirm that we achieve the dual objective of realism and effectiveness, and outperform the recent approaches on their own datasets, while requiring a smaller memory footprint and runtime. We thus offer a viable solution for automating image enhancement and photo cleanup operations."", 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'realisticEditing.jpg', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/ea8fb772-c58b-4bcd-b06a-ffb4eddbd84b/realisticEditing.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234741Z&X-Amz-Expires=3600&X-Amz-Signature=93baf4c731555e40b0d5f3f033b33d9bad882ccdf9a6168a26473c7ea17d1a1b&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:41.289Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'http://yaksoy.github.io/realisticEditing/', 'link': {'url': 'http://yaksoy.github.io/realisticEditing/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'http://yaksoy.github.io/realisticEditing/', 'href': 'http://yaksoy.github.io/realisticEditing/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '752f8d93-6b11-44d7-80a5-0d790586a229'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://www.youtube.com/watch?v=5dKUDMnnjuo', 'link': {'url': 'https://www.youtube.com/watch?v=5dKUDMnnjuo'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://www.youtube.com/watch?v=5dKUDMnnjuo', 'href': 'https://www.youtube.com/watch?v=5dKUDMnnjuo'}]}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '3eb057d5-ed85-4850-b9ac-ff060c999382', 'name': 'Yagiz', 'color': 'gray'}, {'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'http://yaksoy.github.io/realisticEditing/', 'link': {'url': 'http://yaksoy.github.io/realisticEditing/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'http://yaksoy.github.io/realisticEditing/', 'href': 'http://yaksoy.github.io/realisticEditing/'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Realistic Saliency Guided Image Enhancement', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Realistic Saliency Guided Image Enhancement', 'href': None}]}}",https://www.notion.so/Realistic-Saliency-Guided-Image-Enhancement-59e4ea730aa44d388eb788a3428b942e,https://yanxg.notion.site/Realistic-Saliency-Guided-Image-Enhancement-59e4ea730aa44d388eb788a3428b942e
56,page,6a1f6f77-f792-415f-a4b9-15a19cda6d15,2023-05-13T22:38:00.000Z,2023-06-14T19:28:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '752f8d93-6b11-44d7-80a5-0d790586a229'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-06-14T19:28:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-06-14', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Sepideh Sarajian Maralan, Chris Careaga, and Yağız Aksoy', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Sepideh Sarajian Maralan, Chris Careaga, and Yağız Aksoy', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '752f8d93-6b11-44d7-80a5-0d790586a229'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'intrinsicFlash.jpg', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/4ce64201-44b3-4acf-a0e1-02215d5c0add/intrinsicFlash.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234741Z&X-Amz-Expires=3600&X-Amz-Signature=bdb843b841c6495f31198f50af1b801294f94b1dd50f04cb16a1b8e90a8c9fc6&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:41.309Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'http://yaksoy.github.io/intrinsicFlash/', 'link': {'url': 'http://yaksoy.github.io/intrinsicFlash/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'http://yaksoy.github.io/intrinsicFlash/', 'href': 'http://yaksoy.github.io/intrinsicFlash/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}, {'id': '3eb057d5-ed85-4850-b9ac-ff060c999382', 'name': 'Yagiz', 'color': 'gray'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'http://yaksoy.github.io/intrinsicFlash/', 'link': {'url': 'http://yaksoy.github.io/intrinsicFlash/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'http://yaksoy.github.io/intrinsicFlash/', 'href': 'http://yaksoy.github.io/intrinsicFlash/'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '❌❌❌❌'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Computational Flash Photography through Intrinsics', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Computational Flash Photography through Intrinsics', 'href': None}]}}",https://www.notion.so/Computational-Flash-Photography-through-Intrinsics-6a1f6f77f792415fa4b915a19cda6d15,https://yanxg.notion.site/Computational-Flash-Photography-through-Intrinsics-6a1f6f77f792415fa4b915a19cda6d15
57,page,2ff4a77a-2833-429c-b89c-3c0c2873aeae,2023-10-25T19:16:00.000Z,2023-10-25T19:19:00.000Z,"{'object': 'user', 'id': '1c9c6e1c-bbd6-4f5b-935a-c4d2b38e004b'}","{'object': 'user', 'id': '1c9c6e1c-bbd6-4f5b-935a-c4d2b38e004b'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-10-25T19:19:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-05-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Mehran Aghabozorgi, Shichong Peng, Ke Li', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Mehran Aghabozorgi, Shichong Peng, Ke Li', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '1c9c6e1c-bbd6-4f5b-935a-c4d2b38e004b'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Despite their success on large datasets, GANs have been difficult to apply in the few-shot setting, where only a limited number of training examples are provided. Due to mode collapse, GANs tend to ignore some training examples, causing overfitting to a subset of the training dataset, which is small in the first place. A recent method called Implicit Maximum Likelihood Estimation (IMLE) is an alternative to GAN that tries to address this issue. It uses the same kind of generators as GANs but trains it with a different objective that encourages mode coverage. However, the theoretical guarantees of IMLE hold under a restrictive condition that the optimal likelihood at all data points is the same. In this paper, we present a more generalized formulation of IMLE which includes the original formulation as a special case, and we prove that the theoretical guarantees hold under weaker conditions. Using this generalized formulation, we further derive a new algorithm, which we dub Adaptive IMLE, which can adapt to the varying difficulty of different training examples. We demonstrate on multiple few-shot image synthesis datasets that our method significantly outperforms existing methods. Our code is available at ', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Despite their success on large datasets, GANs have been difficult to apply in the few-shot setting, where only a limited number of training examples are provided. Due to mode collapse, GANs tend to ignore some training examples, causing overfitting to a subset of the training dataset, which is small in the first place. A recent method called Implicit Maximum Likelihood Estimation (IMLE) is an alternative to GAN that tries to address this issue. It uses the same kind of generators as GANs but trains it with a different objective that encourages mode coverage. However, the theoretical guarantees of IMLE hold under a restrictive condition that the optimal likelihood at all data points is the same. In this paper, we present a more generalized formulation of IMLE which includes the original formulation as a special case, and we prove that the theoretical guarantees hold under weaker conditions. Using this generalized formulation, we further derive a new algorithm, which we dub Adaptive IMLE, which can adapt to the varying difficulty of different training examples. We demonstrate on multiple few-shot image synthesis datasets that our method significantly outperforms existing methods. Our code is available at ', 'href': None}, {'type': 'text', 'text': {'content': 'https://github.com/mehranagh20/AdaIMLE', 'link': {'url': 'https://github.com/mehranagh20/AdaIMLE'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://github.com/mehranagh20/AdaIMLE', 'href': 'https://github.com/mehranagh20/AdaIMLE'}, {'type': 'text', 'text': {'content': '.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': '.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'Screenshot 2023-10-25 at 12.18.12 PM.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/f6dc87d0-a612-4235-a8f1-cd49492d7456/Screenshot_2023-10-25_at_12.18.12_PM.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234741Z&X-Amz-Expires=3600&X-Amz-Signature=0e0c731ee17223cb7c1e61f3a6e59e19834c711d79e45c8914093b1c7452aa39&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:41.312Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://mehranagh20.github.io/AdaIMLE/', 'link': {'url': 'https://mehranagh20.github.io/AdaIMLE/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://mehranagh20.github.io/AdaIMLE/', 'href': 'https://mehranagh20.github.io/AdaIMLE/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ICLR 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ICLR 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '1c9c6e1c-bbd6-4f5b-935a-c4d2b38e004b'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://youtu.be/xKt6YYY4hq8', 'link': {'url': 'https://youtu.be/xKt6YYY4hq8'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://youtu.be/xKt6YYY4hq8', 'href': 'https://youtu.be/xKt6YYY4hq8'}]}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '847af5a9-e982-4683-ad24-59e89b263fa0', 'name': 'ICLR', 'color': 'green'}, {'id': '9ea36cd3-48d3-462d-baf9-b75ef77f4121', 'name': 'Ke', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://openreview.net/pdf?id=CNq0JvrDfw', 'link': {'url': 'https://openreview.net/pdf?id=CNq0JvrDfw'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://openreview.net/pdf?id=CNq0JvrDfw', 'href': 'https://openreview.net/pdf?id=CNq0JvrDfw'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Adaptive IMLE for Few-shot Pretraining-free Generative Modelling', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Adaptive IMLE for Few-shot Pretraining-free Generative Modelling', 'href': None}]}}",https://www.notion.so/Adaptive-IMLE-for-Few-shot-Pretraining-free-Generative-Modelling-2ff4a77a2833429cb89c3c0c2873aeae,https://yanxg.notion.site/Adaptive-IMLE-for-Few-shot-Pretraining-free-Generative-Modelling-2ff4a77a2833429cb89c3c0c2873aeae
58,page,c08df0c7-8898-4d63-95ee-20e93f9f5b8f,2023-05-15T03:18:00.000Z,2023-10-26T02:10:00.000Z,"{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}","{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-10-26T02:10:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-05-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Erik Wijmans, Manolis Savva, Irfan Essa, Stefan Lee, Ari S Morcos, Dhruv Batra', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Erik Wijmans, Manolis Savva, Irfan Essa, Stefan Lee, Ari S Morcos, Dhruv Batra', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'eom-teaser.jpg', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/4419448b-ae1c-4c46-9acc-f315f86a53fd/eom-teaser.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234741Z&X-Amz-Expires=3600&X-Amz-Signature=a3b333d1a30df13712ab7def9245ca0f70470f78ee8d1d47ddd419e2ab88e97f&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:41.321Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://wijmans.xyz/publication/eom/', 'link': {'url': 'https://wijmans.xyz/publication/eom/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://wijmans.xyz/publication/eom/', 'href': 'https://wijmans.xyz/publication/eom/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ICLR 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ICLR 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '847af5a9-e982-4683-ad24-59e89b263fa0', 'name': 'ICLR', 'color': 'green'}, {'id': 'bccbeae3-55ce-4b38-9abc-61c658ca30b4', 'name': 'Manolis', 'color': 'blue'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2301.13261', 'link': {'url': 'https://arxiv.org/abs/2301.13261'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2301.13261', 'href': 'https://arxiv.org/abs/2301.13261'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '❌❌❌❌'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Emergence of Maps in the Memories of Blind Navigation Agents', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Emergence of Maps in the Memories of Blind Navigation Agents', 'href': None}]}}",https://www.notion.so/Emergence-of-Maps-in-the-Memories-of-Blind-Navigation-Agents-c08df0c788984d6395ee20e93f9f5b8f,https://yanxg.notion.site/Emergence-of-Maps-in-the-Memories-of-Blind-Navigation-Agents-c08df0c788984d6395ee20e93f9f5b8f
59,page,a71873ed-851a-4a82-a39c-d96ac1486e2b,2023-05-14T00:54:00.000Z,2023-10-25T19:13:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '1c9c6e1c-bbd6-4f5b-935a-c4d2b38e004b'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-10-25T19:13:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-04-24', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Xiang Xu, Pradeep Kumar Jayaraman, Joseph G. Lambourne, Karl D.D. Willis, Yasutaka Furukawa', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Xiang Xu, Pradeep Kumar Jayaraman, Joseph G. Lambourne, Karl D.D. Willis, Yasutaka Furukawa', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '1c9c6e1c-bbd6-4f5b-935a-c4d2b38e004b'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'This paper presents a novel generative model for Computer Aided Design (CAD) that 1) represents high-level design concepts of a CAD model as a\nthree-level hierarchical tree of neural codes, from global part arrangement down to local curve geometry; and 2) controls the generation or completion of CAD models by specifying the target\ndesign using a code tree. Concretely, a novel variant of a vector quantized VAE with “masked\nskip connection” extracts design variations as neural codebooks at three levels. Two-stage cascaded auto-regressive transformers learn to generate code trees from incomplete CAD models and then complete CAD models following the\nintended design. Extensive experiments demonstrate superior performance on conventional tasks\nsuch as unconditional generation while enabling novel interaction capabilities on conditional gen-\neration tasks.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'This paper presents a novel generative model for Computer Aided Design (CAD) that 1) represents high-level design concepts of a CAD model as a\nthree-level hierarchical tree of neural codes, from global part arrangement down to local curve geometry; and 2) controls the generation or completion of CAD models by specifying the target\ndesign using a code tree. Concretely, a novel variant of a vector quantized VAE with “masked\nskip connection” extracts design variations as neural codebooks at three levels. Two-stage cascaded auto-regressive transformers learn to generate code trees from incomplete CAD models and then complete CAD models following the\nintended design. Extensive experiments demonstrate superior performance on conventional tasks\nsuch as unconditional generation while enabling novel interaction capabilities on conditional gen-\neration tasks.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'teaser.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/32c28145-f878-45b9-8c86-e4d6cc3c82dd/teaser.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234741Z&X-Amz-Expires=3600&X-Amz-Signature=94552d6e0d963f38aad828ffe6b3153b4339be6c8acfd2b47f05a9a3ab2b4787&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:41.324Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://hnc-cad.github.io/', 'link': {'url': 'https://hnc-cad.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://hnc-cad.github.io/', 'href': 'https://hnc-cad.github.io/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ICML 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ICML 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://www.youtube.com/watch?v=1XVUJIKioO4', 'link': {'url': 'https://www.youtube.com/watch?v=1XVUJIKioO4'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://www.youtube.com/watch?v=1XVUJIKioO4', 'href': 'https://www.youtube.com/watch?v=1XVUJIKioO4'}]}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '7f7fc247-a823-45f6-903f-d3f0c5635c85', 'name': 'ICML', 'color': 'default'}, {'id': '898c1b27-4fe4-4c46-8f37-c2c8cdfe0997', 'name': 'Yasu', 'color': 'red'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/pdf/2307.00149.pdf', 'link': {'url': 'https://arxiv.org/pdf/2307.00149.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/pdf/2307.00149.pdf', 'href': 'https://arxiv.org/pdf/2307.00149.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Hierarchical Neural Coding for Controllable CAD Model Generation', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Hierarchical Neural Coding for Controllable CAD Model Generation', 'href': None}]}}",https://www.notion.so/Hierarchical-Neural-Coding-for-Controllable-CAD-Model-Generation-a71873ed851a4a82a39cd96ac1486e2b,https://yanxg.notion.site/Hierarchical-Neural-Coding-for-Controllable-CAD-Model-Generation-a71873ed851a4a82a39cd96ac1486e2b
60,page,4e0e1234-968a-4b5b-af83-3f46de5449b5,2023-05-14T00:34:00.000Z,2023-10-26T02:18:00.000Z,"{'object': 'user', 'id': '33afb694-d4ea-4523-b754-d7e8e89793d3'}","{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-10-26T02:18:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-04-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Karmesh Yadav,\xa0Ram Ramrakhya,\xa0Santhosh Kumar Ramakrishnan,\xa0Theo Gervet,\xa0John Turner,\xa0Aaron Gokaslan,\xa0Noah Maestre,\xa0Angel Xuan Chang,\xa0Dhruv Batra,\xa0Manolis Savva,\xa0Alexander William Clegg,\xa0Devendra Singh Chaplot', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Karmesh Yadav,\xa0Ram Ramrakhya,\xa0Santhosh Kumar Ramakrishnan,\xa0Theo Gervet,\xa0John Turner,\xa0Aaron Gokaslan,\xa0Noah Maestre,\xa0Angel Xuan Chang,\xa0Dhruv Batra,\xa0Manolis Savva,\xa0Alexander William Clegg,\xa0Devendra Singh Chaplot', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://angelxuanchang.github.io/files/hm3dsem.png', 'type': 'external', 'external': {'url': 'https://angelxuanchang.github.io/files/hm3dsem.png'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://aihabitat.org/datasets/hm3d-semantics/', 'link': {'url': 'https://aihabitat.org/datasets/hm3d-semantics/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://aihabitat.org/datasets/hm3d-semantics/', 'href': 'https://aihabitat.org/datasets/hm3d-semantics/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '33afb694-d4ea-4523-b754-d7e8e89793d3'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}, {'id': 'bccbeae3-55ce-4b38-9abc-61c658ca30b4', 'name': 'Manolis', 'color': 'blue'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/pdf/2210.05633.pdf', 'link': {'url': 'https://arxiv.org/pdf/2210.05633.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/pdf/2210.05633.pdf', 'href': 'https://arxiv.org/pdf/2210.05633.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '❌❌❌❌'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Habitat-Matterport 3D Semantics Dataset', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Habitat-Matterport 3D Semantics Dataset', 'href': None}]}}",https://www.notion.so/Habitat-Matterport-3D-Semantics-Dataset-4e0e1234968a4b5baf833f46de5449b5,https://yanxg.notion.site/Habitat-Matterport-3D-Semantics-Dataset-4e0e1234968a4b5baf833f46de5449b5
61,page,09e8bfb7-cc10-4c12-91be-940f77be6a1e,2023-05-13T23:06:00.000Z,2023-05-13T23:20:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-05-13T23:20:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-04-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Yizhi Wang, Zeyu Huang, Ariel Shamir, Hui Huang, Hao Zhang, and Ruizhen Hu', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Yizhi Wang, Zeyu Huang, Ariel Shamir, Hui Huang, Hao Zhang, and Ruizhen Hu', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'aronet[1].png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/4f445b0e-49d2-48f5-a046-7eede103bfb8/aronet1.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234741Z&X-Amz-Expires=3600&X-Amz-Signature=e8a2a42fab94bc8ca33dac54dfae8e12190e0e79a4f12d5d3139327b4f5de83c&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:41.337Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2212.10275', 'link': {'url': 'https://arxiv.org/abs/2212.10275'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2212.10275', 'href': 'https://arxiv.org/abs/2212.10275'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}, {'id': 'e16d2c21-c87f-4a73-aa66-85bb6bc6e5b0', 'name': 'Richard', 'color': 'pink'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2212.10275', 'link': {'url': 'https://arxiv.org/abs/2212.10275'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2212.10275', 'href': 'https://arxiv.org/abs/2212.10275'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '❌❌❌❌'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'ARO-Net: Learning Implicit Fields from Anchored Radial Observations', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ARO-Net: Learning Implicit Fields from Anchored Radial Observations', 'href': None}]}}",https://www.notion.so/ARO-Net-Learning-Implicit-Fields-from-Anchored-Radial-Observations-09e8bfb7cc104c1291be940f77be6a1e,https://yanxg.notion.site/ARO-Net-Learning-Implicit-Fields-from-Anchored-Radial-Observations-09e8bfb7cc104c1291be940f77be6a1e
62,page,fb5c2a9b-b3da-4f72-9213-ab918b301345,2023-05-13T23:03:00.000Z,2023-06-13T03:49:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-06-13T03:49:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-04-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Cristina Vasconcelos, Kevin Swersky, Mark Matthews, Milad Hashemi, Cengiz Oztireli, Andrea Tagliasacchi', 'link': None}, 'annotations': {'bold': False, 'italic': True, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Cristina Vasconcelos, Kevin Swersky, Mark Matthews, Milad Hashemi, Cengiz Oztireli, Andrea Tagliasacchi', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://taiya.github.io/images/placeholder.jpg', 'type': 'external', 'external': {'url': 'https://taiya.github.io/images/placeholder.jpg'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://cuf-paper.github.io/', 'link': {'url': 'https://cuf-paper.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://cuf-paper.github.io/', 'href': 'https://cuf-paper.github.io/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}, {'id': '1a31dc62-0b3c-4bfe-8684-3ffe669325d3', 'name': 'Andrea', 'color': 'purple'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://cuf-paper.github.io/cuf.pdf', 'link': {'url': 'https://cuf-paper.github.io/cuf.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://cuf-paper.github.io/cuf.pdf', 'href': 'https://cuf-paper.github.io/cuf.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '❌❌❌❌'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'CUF: Continuous Upsampling Filters', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CUF: Continuous Upsampling Filters', 'href': None}]}}",https://www.notion.so/CUF-Continuous-Upsampling-Filters-fb5c2a9bb3da4f729213ab918b301345,https://yanxg.notion.site/CUF-Continuous-Upsampling-Filters-fb5c2a9bb3da4f729213ab918b301345
63,page,d050430b-4429-465d-9ad0-3697d4124f37,2023-05-13T23:01:00.000Z,2023-06-13T03:49:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-06-13T03:49:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-04-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Songyou Peng, Kyle Genova, Chiyu Max Jiang, Andrea Tagliasacchi, Marc Pollefeys, Thomas Funkhouser', 'link': None}, 'annotations': {'bold': False, 'italic': True, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Songyou Peng, Kyle Genova, Chiyu Max Jiang, Andrea Tagliasacchi, Marc Pollefeys, Thomas Funkhouser', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://taiya.github.io/images/placeholder.jpg', 'type': 'external', 'external': {'url': 'https://taiya.github.io/images/placeholder.jpg'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2211.15654', 'link': {'url': 'https://arxiv.org/abs/2211.15654'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2211.15654', 'href': 'https://arxiv.org/abs/2211.15654'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}, {'id': '1a31dc62-0b3c-4bfe-8684-3ffe669325d3', 'name': 'Andrea', 'color': 'purple'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2211.15654', 'link': {'url': 'https://arxiv.org/abs/2211.15654'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2211.15654', 'href': 'https://arxiv.org/abs/2211.15654'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '❌❌❌❌'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'OpenScene: 3D Scene Understanding with Open Vocabularies', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'OpenScene: 3D Scene Understanding with Open Vocabularies', 'href': None}]}}",https://www.notion.so/OpenScene-3D-Scene-Understanding-with-Open-Vocabularies-d050430b4429465d9ad03697d4124f37,https://yanxg.notion.site/OpenScene-3D-Scene-Understanding-with-Open-Vocabularies-d050430b4429465d9ad03697d4124f37
64,page,14c20053-18fd-405a-bcd5-64ff012adf1c,2023-05-13T23:01:00.000Z,2023-06-13T03:49:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-06-13T03:49:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-04-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Samarth Sinha, Jason Y. Zhang, Andrea Tagliasacchi, Igor Gilitschenski, David B. Lindell', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Samarth Sinha, Jason Y. Zhang, Andrea Tagliasacchi, Igor Gilitschenski, David B. Lindell', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://taiya.github.io/images/placeholder.jpg', 'type': 'external', 'external': {'url': 'https://taiya.github.io/images/placeholder.jpg'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://taiya.github.io/', 'link': {'url': 'https://taiya.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://taiya.github.io/', 'href': 'https://taiya.github.io/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}, {'id': '1a31dc62-0b3c-4bfe-8684-3ffe669325d3', 'name': 'Andrea', 'color': 'purple'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://taiya.github.io/', 'link': {'url': 'https://taiya.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://taiya.github.io/', 'href': 'https://taiya.github.io/'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '❌❌❌❌'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'SparsePose: Sparse-View Camera Pose Regression and Refinement', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'SparsePose: Sparse-View Camera Pose Regression and Refinement', 'href': None}]}}",https://www.notion.so/SparsePose-Sparse-View-Camera-Pose-Regression-and-Refinement-14c2005318fd405abcd564ff012adf1c,https://yanxg.notion.site/SparsePose-Sparse-View-Camera-Pose-Regression-and-Refinement-14c2005318fd405abcd564ff012adf1c
65,page,c5d52e42-8a31-439a-b18a-110cbaf1692d,2023-05-13T23:01:00.000Z,2023-06-13T03:48:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-06-13T03:48:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-04-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Kacper Kania, Stephan J. Garbin, Andrea Tagliasacchi, Virginia Estellers, Kwang Moo Yi, Tomasz Trzcinski, Julien Valentin, Marek Kowalski', 'link': None}, 'annotations': {'bold': False, 'italic': True, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Kacper Kania, Stephan J. Garbin, Andrea Tagliasacchi, Virginia Estellers, Kwang Moo Yi, Tomasz Trzcinski, Julien Valentin, Marek Kowalski', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://taiya.github.io/images/placeholder.jpg', 'type': 'external', 'external': {'url': 'https://taiya.github.io/images/placeholder.jpg'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://taiya.github.io/', 'link': {'url': 'https://taiya.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://taiya.github.io/', 'href': 'https://taiya.github.io/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}, {'id': '1a31dc62-0b3c-4bfe-8684-3ffe669325d3', 'name': 'Andrea', 'color': 'purple'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://taiya.github.io/', 'link': {'url': 'https://taiya.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://taiya.github.io/', 'href': 'https://taiya.github.io/'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '❌❌❌❌'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'BlendFields: Few-Shot Example-Driven Facial Modeling', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'BlendFields: Few-Shot Example-Driven Facial Modeling', 'href': None}]}}",https://www.notion.so/BlendFields-Few-Shot-Example-Driven-Facial-Modeling-c5d52e428a31439ab18a110cbaf1692d,https://yanxg.notion.site/BlendFields-Few-Shot-Example-Driven-Facial-Modeling-c5d52e428a31439ab18a110cbaf1692d
66,page,e597ffc2-5b25-4391-b4a1-6f533e25e983,2023-05-13T22:59:00.000Z,2023-05-13T23:12:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-05-13T23:12:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-04-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Zhiqin Chen, Tom Funkhouser, Peter Hedman, Andrea Tagliasacchi', 'link': None}, 'annotations': {'bold': False, 'italic': True, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Zhiqin Chen, Tom Funkhouser, Peter Hedman, Andrea Tagliasacchi', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'cvpr23_mobilenerf[1].png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/e3ff6c29-1c6e-49bd-aadd-192b8d03620b/cvpr23_mobilenerf1.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234741Z&X-Amz-Expires=3600&X-Amz-Signature=3a7c18f35024736a58998cabb1b8391d6f0663a6d4e9282cd0e2270b75b0a88a&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:41.355Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://mobile-nerf.github.io/', 'link': {'url': 'https://mobile-nerf.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://mobile-nerf.github.io/', 'href': 'https://mobile-nerf.github.io/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}, {'id': '1a31dc62-0b3c-4bfe-8684-3ffe669325d3', 'name': 'Andrea', 'color': 'purple'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://taiya.github.io/pubs/chen2023mobilenerf.pdf', 'link': {'url': 'https://taiya.github.io/pubs/chen2023mobilenerf.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://taiya.github.io/pubs/chen2023mobilenerf.pdf', 'href': 'https://taiya.github.io/pubs/chen2023mobilenerf.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '❌❌❌❌'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures', 'link': None}, 'annotations': {'bold': True, 'italic': True, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'MobileNeRF: Exploiting the Polygon Rasterization Pipeline for Efficient Neural Field Rendering on Mobile Architectures', 'href': None}]}}",https://www.notion.so/MobileNeRF-Exploiting-the-Polygon-Rasterization-Pipeline-for-Efficient-Neural-Field-Rendering-on-Mo-e597ffc25b254391b4a16f533e25e983,https://yanxg.notion.site/MobileNeRF-Exploiting-the-Polygon-Rasterization-Pipeline-for-Efficient-Neural-Field-Rendering-on-Mo-e597ffc25b254391b4a16f533e25e983
67,page,11d5ea9f-22c9-42a4-80fb-a9ab21e7b616,2023-05-13T22:56:00.000Z,2023-05-13T23:11:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-05-13T23:11:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-04-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Ryuhei Hamaguchi, Yasutaka Furukawa, Masaki Onishi, and Ken Sakurada', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Ryuhei Hamaguchi, Yasutaka Furukawa, Masaki Onishi, and Ken Sakurada', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': '2023-cvpr-event-camera[1].jpg', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/f982027f-5810-4b38-9b08-73b69c219ee9/2023-cvpr-event-camera1.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234741Z&X-Amz-Expires=3600&X-Amz-Signature=f2a19aad61f9a10983b88d777fba1ae01e1ff78669d9f603f546d8210a298c24&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:41.363Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://www.cs.sfu.ca/~furukawa/', 'link': {'url': 'https://www.cs.sfu.ca/~furukawa/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://www.cs.sfu.ca/~furukawa/', 'href': 'https://www.cs.sfu.ca/~furukawa/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}, {'id': '898c1b27-4fe4-4c46-8f37-c2c8cdfe0997', 'name': 'Yasu', 'color': 'red'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://www.cs.sfu.ca/~furukawa/', 'link': {'url': 'https://www.cs.sfu.ca/~furukawa/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://www.cs.sfu.ca/~furukawa/', 'href': 'https://www.cs.sfu.ca/~furukawa/'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '❌❌❌❌'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Hierarchical Neural Memory Network for Low Latency Event Processing', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Hierarchical Neural Memory Network for Low Latency Event Processing', 'href': None}]}}",https://www.notion.so/Hierarchical-Neural-Memory-Network-for-Low-Latency-Event-Processing-11d5ea9f22c942a480fba9ab21e7b616,https://yanxg.notion.site/Hierarchical-Neural-Memory-Network-for-Low-Latency-Event-Processing-11d5ea9f22c942a480fba9ab21e7b616
68,page,86fb7dc9-1442-41a1-8d90-8291a3e07865,2023-05-13T22:56:00.000Z,2023-05-13T23:10:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-05-13T23:10:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-04-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Amin Shabani, Sepidehsadat Hosseini, and Yasutaka Furukawa', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Amin Shabani, Sepidehsadat Hosseini, and Yasutaka Furukawa', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': '2023-cvpr-house-diffusion[1].jpg', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/af4ac260-ebc3-4317-91ce-27e164dbfee3/2023-cvpr-house-diffusion1.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234741Z&X-Amz-Expires=3600&X-Amz-Signature=288a591aac295bf8222e59c828bd7530fc8014a29ee467cf7fbc12a9a608479f&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:41.366Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://github.com/Tangshitao/NeuMap', 'link': {'url': 'https://github.com/aminshabani/house_diffusion'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://github.com/Tangshitao/NeuMap', 'href': 'https://github.com/aminshabani/house_diffusion'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}, {'id': '898c1b27-4fe4-4c46-8f37-c2c8cdfe0997', 'name': 'Yasu', 'color': 'red'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2211.11177', 'link': {'url': 'https://arxiv.org/abs/2211.13287'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2211.11177', 'href': 'https://arxiv.org/abs/2211.13287'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '❌❌❌❌'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'HouseDiffusion: Vector Floorplan Generation via a Diffusion Model with Discrete and Continuous Denoising', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'HouseDiffusion: Vector Floorplan Generation via a Diffusion Model with Discrete and Continuous Denoising', 'href': None}]}}",https://www.notion.so/HouseDiffusion-Vector-Floorplan-Generation-via-a-Diffusion-Model-with-Discrete-and-Continuous-Denoi-86fb7dc9144241a18d908291a3e07865,https://yanxg.notion.site/HouseDiffusion-Vector-Floorplan-Generation-via-a-Diffusion-Model-with-Discrete-and-Continuous-Denoi-86fb7dc9144241a18d908291a3e07865
69,page,73a510c5-02c6-4a27-9b9a-09e875390a45,2023-05-13T22:38:00.000Z,2023-05-14T00:08:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': 'a8269098-e822-4102-ae4e-5dee77e53650'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-05-14T00:08:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2023-04-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Shitao Tang, Sicong Tang, Andrea Tagliasacchi, Ping Tan, and Yasutaka Furukawa', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Shitao Tang, Sicong Tang, Andrea Tagliasacchi, Ping Tan, and Yasutaka Furukawa', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'a8269098-e822-4102-ae4e-5dee77e53650'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'This paper presents an end-to-end neural mapping method for camera localization, dubbed NeuMap, encoding a whole scene into a grid of latent codes, with which a Transformer-based auto-decoder regresses 3D coordinates of query pixels. State-of-the-art feature matching methods require each scene to be stored as a 3D point cloud with per-point features, consuming several gigabytes of storage per scene. While compression is possible, performance drops significantly at high compression rates. Conversely, coordinate regression methods achieve high compression by storing scene information in a neural network but suffer from reduced robustness. NeuMap combines the advantages of both approaches by utilizing 1) learnable latent codes for efficient scene representation and 2) a scene-agnostic Transformer-based auto-decoder to infer coordinates for query pixels. This scene-agnostic network design learns robust matching priors from large-scale data and enables rapid optimization of codes for new scenes while keeping the network weights fixed. Extensive evaluations on five benchmarks show that NeuMap significantly outperforms other coordinate regression methods and achieves comparable performance to feature matching methods while requiring a much smaller scene representation size. For example, NeuMap achieves 39.1\\% accuracy in the Aachen night benchmark with only 6MB of data, whereas alternative methods require 100MB or several gigabytes and fail completely under high compression settings.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'This paper presents an end-to-end neural mapping method for camera localization, dubbed NeuMap, encoding a whole scene into a grid of latent codes, with which a Transformer-based auto-decoder regresses 3D coordinates of query pixels. State-of-the-art feature matching methods require each scene to be stored as a 3D point cloud with per-point features, consuming several gigabytes of storage per scene. While compression is possible, performance drops significantly at high compression rates. Conversely, coordinate regression methods achieve high compression by storing scene information in a neural network but suffer from reduced robustness. NeuMap combines the advantages of both approaches by utilizing 1) learnable latent codes for efficient scene representation and 2) a scene-agnostic Transformer-based auto-decoder to infer coordinates for query pixels. This scene-agnostic network design learns robust matching priors from large-scale data and enables rapid optimization of codes for new scenes while keeping the network weights fixed. Extensive evaluations on five benchmarks show that NeuMap significantly outperforms other coordinate regression methods and achieves comparable performance to feature matching methods while requiring a much smaller scene representation size. For example, NeuMap achieves 39.1\\% accuracy in the Aachen night benchmark with only 6MB of data, whereas alternative methods require 100MB or several gigabytes and fail completely under high compression settings.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': '2023-cvpr-neumap[1].jpg', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/656beba7-f157-4b8a-ad8c-ee8fd1317813/2023-cvpr-neumap1.jpg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234741Z&X-Amz-Expires=3600&X-Amz-Signature=52ba7bd71dfa6923e6592eb8bea2a61b6a1578f8e4a3f254bba3d7f4deb5104d&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:41.376Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://github.com/Tangshitao/NeuMap', 'link': {'url': 'https://github.com/Tangshitao/NeuMap'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://github.com/Tangshitao/NeuMap', 'href': 'https://github.com/Tangshitao/NeuMap'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2023', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2023', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}, {'id': '898c1b27-4fe4-4c46-8f37-c2c8cdfe0997', 'name': 'Yasu', 'color': 'red'}, {'id': '1a31dc62-0b3c-4bfe-8684-3ffe669325d3', 'name': 'Andrea', 'color': 'purple'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2211.11177', 'link': {'url': 'https://arxiv.org/abs/2211.11177'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2211.11177', 'href': 'https://arxiv.org/abs/2211.11177'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'NeuMap: Neural Coordinate Mapping by Auto-Transdecoder for Camera Localization', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'NeuMap: Neural Coordinate Mapping by Auto-Transdecoder for Camera Localization', 'href': None}]}}",https://www.notion.so/NeuMap-Neural-Coordinate-Mapping-by-Auto-Transdecoder-for-Camera-Localization-73a510c502c64a279b9a09e875390a45,https://yanxg.notion.site/NeuMap-Neural-Coordinate-Mapping-by-Auto-Transdecoder-for-Camera-Localization-73a510c502c64a279b9a09e875390a45
70,page,555d2cb2-472e-4c25-bd27-ae0be3ea3524,2023-06-14T22:40:00.000Z,2023-06-15T03:14:00.000Z,"{'object': 'user', 'id': '811c8c6b-f3a1-43f3-ae6d-d85d1c2cf848'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-06-15T03:14:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2022-12-05', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Hang Zhou, Rui Ma, Lingxiao Zhang, Lin Gao,\xa0Ali Mahdavi-Amiri, and Hao Zhang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Hang Zhou, Rui Ma, Lingxiao Zhang, Lin Gao,\xa0Ali Mahdavi-Amiri, and Hao Zhang', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://github.com/RyanHangZhou/SAC-GAN/raw/master/img/teaser.png', 'type': 'external', 'external': {'url': 'https://github.com/RyanHangZhou/SAC-GAN/raw/master/img/teaser.png'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2112.06596', 'link': {'url': 'https://arxiv.org/abs/2112.06596'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2112.06596', 'href': 'https://arxiv.org/abs/2112.06596'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'IEEE Transaction of Visualization and Computer Graphics', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'IEEE Transaction of Visualization and Computer Graphics', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '811c8c6b-f3a1-43f3-ae6d-d85d1c2cf848'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'e16d2c21-c87f-4a73-aa66-85bb6bc6e5b0', 'name': 'Richard', 'color': 'pink'}, {'id': 'ed487f9e-f5ad-446d-89eb-2ab46ab0892c', 'name': 'Ali', 'color': 'purple'}, {'id': '0412070e-afc8-47c5-97be-9e0b9dac1955', 'name': 'IEEE', 'color': 'default'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2112.06596', 'link': {'url': 'https://arxiv.org/abs/2112.06596'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2112.06596', 'href': 'https://arxiv.org/abs/2112.06596'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '❌❌❌❌'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'SAC-GAN: Structure-Aware Image Composition', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'SAC-GAN: Structure-Aware Image Composition', 'href': None}]}}",https://www.notion.so/SAC-GAN-Structure-Aware-Image-Composition-555d2cb2472e4c25bd27ae0be3ea3524,https://yanxg.notion.site/SAC-GAN-Structure-Aware-Image-Composition-555d2cb2472e4c25bd27ae0be3ea3524
71,page,c9b9e48d-4f8c-48b8-ba80-17396fad3344,2023-06-14T20:15:00.000Z,2023-06-14T20:19:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-06-14T20:19:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2022-12-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Weilian Song, Mahsa Maleki Abyaneh, Mohammad Amin Shabani, and Yasutaka Furukawa', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Weilian Song, Mahsa Maleki Abyaneh, Mohammad Amin Shabani, and Yasutaka Furukawa', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://www.cs.sfu.ca/~furukawa/newimages/2022-accv-blueprint.jpg', 'type': 'external', 'external': {'url': 'https://www.cs.sfu.ca/~furukawa/newimages/2022-accv-blueprint.jpg'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://drive.google.com/file/d/1ASIAVhs0HeTiTdyHrEim2G4cF7PXCOs0/view', 'link': {'url': 'https://drive.google.com/file/d/1ASIAVhs0HeTiTdyHrEim2G4cF7PXCOs0/view'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://drive.google.com/file/d/1ASIAVhs0HeTiTdyHrEim2G4cF7PXCOs0/view', 'href': 'https://drive.google.com/file/d/1ASIAVhs0HeTiTdyHrEim2G4cF7PXCOs0/view'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ACCV 2022', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ACCV 2022', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '898c1b27-4fe4-4c46-8f37-c2c8cdfe0997', 'name': 'Yasu', 'color': 'red'}, {'id': '1580931b-4de8-4792-97a6-9c715191a9b9', 'name': 'ACCV', 'color': 'pink'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://drive.google.com/file/d/1ASIAVhs0HeTiTdyHrEim2G4cF7PXCOs0/view', 'link': {'url': 'https://drive.google.com/file/d/1ASIAVhs0HeTiTdyHrEim2G4cF7PXCOs0/view'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://drive.google.com/file/d/1ASIAVhs0HeTiTdyHrEim2G4cF7PXCOs0/view', 'href': 'https://drive.google.com/file/d/1ASIAVhs0HeTiTdyHrEim2G4cF7PXCOs0/view'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '❌❌❌❌'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Vectorizing Building Blueprints', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Vectorizing Building Blueprints', 'href': None}]}}",https://www.notion.so/Vectorizing-Building-Blueprints-c9b9e48d4f8c48b8ba8017396fad3344,https://yanxg.notion.site/Vectorizing-Building-Blueprints-c9b9e48d4f8c48b8ba8017396fad3344
72,page,89b26044-a63b-46bb-8335-b310c47194f2,2023-05-15T03:14:00.000Z,2023-05-15T03:16:00.000Z,"{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}","{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-05-15T03:16:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2022-11-29', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Yongsen Mao, Yiming Zhang,\xa0Hanxiao Jiang,\xa0Angel X. Chang,', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Yongsen Mao, Yiming Zhang,\xa0Hanxiao Jiang,\xa0Angel X. Chang,', 'href': None}, {'type': 'text', 'text': {'content': '\xa0', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': '\xa0', 'href': None}, {'type': 'text', 'text': {'content': 'Manolis Savva', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Manolis Savva', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'multiscan.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/27b5a4e7-1a49-4d92-be15-57ac47feaaa6/multiscan.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234741Z&X-Amz-Expires=3600&X-Amz-Signature=4ae699302489c73c576f692db3935f2ada0d63b6144f6c2c43b912e1217ba996&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:41.389Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://3dlg-hcvc.github.io/multiscan/', 'link': {'url': 'https://3dlg-hcvc.github.io/multiscan/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://3dlg-hcvc.github.io/multiscan/', 'href': 'https://3dlg-hcvc.github.io/multiscan/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'NeurIPS 2022', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'NeurIPS 2022', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'bb1c7b2a-93d8-4f0d-9e32-2a3594a652cd', 'name': 'NeurIPS', 'color': 'default'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}, {'id': 'bccbeae3-55ce-4b38-9abc-61c658ca30b4', 'name': 'Manolis', 'color': 'blue'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://proceedings.neurips.cc/paper_files/paper/2022/file/3b3a83a5d86e1d424daefed43d998079-Paper-Conference.pdf', 'link': {'url': 'https://proceedings.neurips.cc/paper_files/paper/2022/file/3b3a83a5d86e1d424daefed43d998079-Paper-Conference.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://proceedings.neurips.cc/paper_files/paper/2022/file/3b3a83a5d86e1d424daefed43d998079-Paper-Conference.pdf', 'href': 'https://proceedings.neurips.cc/paper_files/paper/2022/file/3b3a83a5d86e1d424daefed43d998079-Paper-Conference.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '❌❌❌❌'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'MultiScan: Scalable RGBD scanning for 3D environments with articulated objects', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'MultiScan: Scalable RGBD scanning for 3D environments with articulated objects', 'href': None}]}}",https://www.notion.so/MultiScan-Scalable-RGBD-scanning-for-3D-environments-with-articulated-objects-89b26044a63b46bb8335b310c47194f2,https://yanxg.notion.site/MultiScan-Scalable-RGBD-scanning-for-3D-environments-with-articulated-objects-89b26044a63b46bb8335b310c47194f2
73,page,62603cb1-eb87-4492-9360-62a4f88afdc3,2023-10-26T01:45:00.000Z,2023-10-26T01:52:00.000Z,"{'object': 'user', 'id': 'd92b046c-2061-4059-9e57-3f50c31b6e03'}","{'object': 'user', 'id': 'd92b046c-2061-4059-9e57-3f50c31b6e03'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-10-26T01:52:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2022-11-28', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Shichong Peng, Alireza Moazeni, Ke Li', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Shichong Peng, Alireza Moazeni, Ke Li', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'd92b046c-2061-4059-9e57-3f50c31b6e03'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'A persistent challenge in conditional image synthesis has been to generate diverse output images from the same input image despite only one output image being observed per input image. GAN-based methods are prone to mode collapse, which leads to low diversity. To get around this, we leverage Implicit Maximum Likelihood Estimation (IMLE) which can overcome mode collapse fundamentally. IMLE uses the same generator as GANs but trains it with a different, non-adversarial objective which ensures each observed image has a generated sample nearby. Unfortunately, to generate high-fidelity images, prior IMLE-based methods require a large number of samples, which is expensive. In this paper, we propose a new method to get around this limitation, which we dub Conditional Hierarchical IMLE (CHIMLE), which can generate high-fidelity images without requiring many samples. We show CHIMLE significantly outperforms the prior best IMLE, GAN and diffusion-based methods in terms of image fidelity and mode coverage across four tasks, namely night-to-day, 16x single image super-resolution, image colourization and image decompression. Quantitatively, our method improves Fréchet Inception Distance (FID) by 36.9% on average compared to the prior best IMLE-based method, and by 27.5% on average compared to the best non-IMLE-based general-purpose methods.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'A persistent challenge in conditional image synthesis has been to generate diverse output images from the same input image despite only one output image being observed per input image. GAN-based methods are prone to mode collapse, which leads to low diversity. To get around this, we leverage Implicit Maximum Likelihood Estimation (IMLE) which can overcome mode collapse fundamentally. IMLE uses the same generator as GANs but trains it with a different, non-adversarial objective which ensures each observed image has a generated sample nearby. Unfortunately, to generate high-fidelity images, prior IMLE-based methods require a large number of samples, which is expensive. In this paper, we propose a new method to get around this limitation, which we dub Conditional Hierarchical IMLE (CHIMLE), which can generate high-fidelity images without requiring many samples. We show CHIMLE significantly outperforms the prior best IMLE, GAN and diffusion-based methods in terms of image fidelity and mode coverage across four tasks, namely night-to-day, 16x single image super-resolution, image colourization and image decompression. Quantitatively, our method improves Fréchet Inception Distance (FID) by 36.9% on average compared to the prior best IMLE-based method, and by 27.5% on average compared to the best non-IMLE-based general-purpose methods.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'teaser.gif', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/a7cc1543-a9bc-4308-b103-9565c87941c6/teaser.gif?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234741Z&X-Amz-Expires=3600&X-Amz-Signature=25d52e5741c655aeecb3a853968fe65b01cbbd1dade5f814c5836c0d493d7779&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:41.394Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://niopeng.github.io/CHIMLE/', 'link': {'url': 'https://niopeng.github.io/CHIMLE/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://niopeng.github.io/CHIMLE/', 'href': 'https://niopeng.github.io/CHIMLE/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'NeurIPS 2022', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'NeurIPS 2022', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'd92b046c-2061-4059-9e57-3f50c31b6e03'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://www.youtube.com/watch?v=plgPL3XyzRg', 'link': {'url': 'https://www.youtube.com/watch?v=plgPL3XyzRg'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://www.youtube.com/watch?v=plgPL3XyzRg', 'href': 'https://www.youtube.com/watch?v=plgPL3XyzRg'}]}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'bb1c7b2a-93d8-4f0d-9e32-2a3594a652cd', 'name': 'NeurIPS', 'color': 'default'}, {'id': '9ea36cd3-48d3-462d-baf9-b75ef77f4121', 'name': 'Ke', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2211.14286', 'link': {'url': 'https://arxiv.org/abs/2211.14286'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2211.14286', 'href': 'https://arxiv.org/abs/2211.14286'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'CHIMLE: Conditional Hierarchical IMLE for Multimodal Conditional Image Synthesis', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CHIMLE: Conditional Hierarchical IMLE for Multimodal Conditional Image Synthesis', 'href': None}]}}",https://www.notion.so/CHIMLE-Conditional-Hierarchical-IMLE-for-Multimodal-Conditional-Image-Synthesis-62603cb1eb874492936062a4f88afdc3,https://yanxg.notion.site/CHIMLE-Conditional-Hierarchical-IMLE-for-Multimodal-Conditional-Image-Synthesis-62603cb1eb874492936062a4f88afdc3
74,page,6acd66a6-a889-4438-bcc7-551da232ee3f,2023-05-15T03:07:00.000Z,2023-05-15T03:10:00.000Z,"{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}","{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-05-15T03:10:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2022-10-25', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Dave Zhenyu Chen, Qirui Wu,\xa0Matthias Nießner,\xa0Angel X. Chang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Dave Zhenyu Chen, Qirui Wu,\xa0Matthias Nießner,\xa0Angel X. Chang', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'd3net.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/dd7d9ce0-1958-445f-8fdd-9bc398c939ac/d3net.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234741Z&X-Amz-Expires=3600&X-Amz-Signature=e8334ec017926f55a27c094a5891e3b49afa8f6394888f471f8b9122d9547d46&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:41.400Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://daveredrum.github.io/D3Net/', 'link': {'url': 'https://daveredrum.github.io/D3Net/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://daveredrum.github.io/D3Net/', 'href': 'https://daveredrum.github.io/D3Net/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ECCV 2022', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ECCV 2022', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '5cbe73f7-7a55-4251-80d4-2375865f5724', 'name': 'ECCV', 'color': 'yellow'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2112.01551', 'link': {'url': 'https://arxiv.org/abs/2112.01551'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2112.01551', 'href': 'https://arxiv.org/abs/2112.01551'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '❌❌❌❌'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'D3Net: A Unified Speaker-Listener Architecture for 3D Dense Captioning and Visual Grounding', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'D3Net: A Unified Speaker-Listener Architecture for 3D Dense Captioning and Visual Grounding', 'href': None}]}}",https://www.notion.so/D3Net-A-Unified-Speaker-Listener-Architecture-for-3D-Dense-Captioning-and-Visual-Grounding-6acd66a6a8894438bcc7551da232ee3f,https://yanxg.notion.site/D3Net-A-Unified-Speaker-Listener-Architecture-for-3D-Dense-Captioning-and-Visual-Grounding-6acd66a6a8894438bcc7551da232ee3f
75,page,7cc9a376-f7a0-47e6-b725-ee0a6d7550de,2023-05-15T03:03:00.000Z,2023-05-15T03:08:00.000Z,"{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}","{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-05-15T03:08:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2022-10-25', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Hanxiao Jiang,\xa0Yongsen Mao, Manolis Savva,\xa0Angel X. Chang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Hanxiao Jiang,\xa0Yongsen Mao, Manolis Savva,\xa0Angel X. Chang', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'opd.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/0858bf93-e96c-477a-9774-efacd682ce29/opd.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234741Z&X-Amz-Expires=3600&X-Amz-Signature=e3cf836db8eb0f8da54a30e1dec048ea61b06c1b94d115af4c6781b54403ffe8&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:41.408Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://3dlg-hcvc.github.io/OPD/', 'link': {'url': 'https://3dlg-hcvc.github.io/OPD/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://3dlg-hcvc.github.io/OPD/', 'href': 'https://3dlg-hcvc.github.io/OPD/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ECCV 2022', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ECCV 2022', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '5cbe73f7-7a55-4251-80d4-2375865f5724', 'name': 'ECCV', 'color': 'yellow'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}, {'id': 'bccbeae3-55ce-4b38-9abc-61c658ca30b4', 'name': 'Manolis', 'color': 'blue'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2203.16421', 'link': {'url': 'https://arxiv.org/abs/2203.16421'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2203.16421', 'href': 'https://arxiv.org/abs/2203.16421'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '❌❌❌❌'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'OPD: Single-view 3D Openable Part Detection', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'OPD: Single-view 3D Openable Part Detection', 'href': None}]}}",https://www.notion.so/OPD-Single-view-3D-Openable-Part-Detection-7cc9a376f7a047e6b725ee0a6d7550de,https://yanxg.notion.site/OPD-Single-view-3D-Openable-Part-Detection-7cc9a376f7a047e6b725ee0a6d7550de
76,page,4ba354c2-1b60-493d-97d3-1d9213f4c3df,2023-06-14T23:20:00.000Z,2023-06-14T23:58:00.000Z,"{'object': 'user', 'id': '2de96350-7917-4a98-8014-dc5695a68171'}","{'object': 'user', 'id': '2de96350-7917-4a98-8014-dc5695a68171'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-06-14T23:58:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2022-10-01', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'M.Mahdavian, KangKang Yin, and Mo Chen', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'M.Mahdavian, KangKang Yin, and Mo Chen', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '2de96350-7917-4a98-8014-dc5695a68171'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://www.cs.sfu.ca/~kkyin/papers/VTR.jpg', 'type': 'external', 'external': {'url': 'https://www.cs.sfu.ca/~kkyin/papers/VTR.jpg'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2109.10445', 'link': {'url': 'https://arxiv.org/abs/2109.10445'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2109.10445', 'href': 'https://arxiv.org/abs/2109.10445'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'IEEE Robotics and Automation Letters', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'IEEE Robotics and Automation Letters', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '2de96350-7917-4a98-8014-dc5695a68171'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '0412070e-afc8-47c5-97be-9e0b9dac1955', 'name': 'IEEE', 'color': 'default'}, {'id': '7059893d-3726-44a9-af21-36fdbfdcb80d', 'name': 'KangKang', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2109.10445', 'link': {'url': 'https://arxiv.org/abs/2109.10445'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2109.10445', 'href': 'https://arxiv.org/abs/2109.10445'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '❌❌❌❌'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Robust Visual Teach and Repeat for UGVs Using 3D Semantic Maps', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Robust Visual Teach and Repeat for UGVs Using 3D Semantic Maps', 'href': None}]}}",https://www.notion.so/Robust-Visual-Teach-and-Repeat-for-UGVs-Using-3D-Semantic-Maps-4ba354c21b60493d97d31d9213f4c3df,https://yanxg.notion.site/Robust-Visual-Teach-and-Repeat-for-UGVs-Using-3D-Semantic-Maps-4ba354c21b60493d97d31d9213f4c3df
77,page,315625ea-6b60-4cf6-b1df-990c9c46a53b,2023-05-15T03:10:00.000Z,2023-05-15T03:13:00.000Z,"{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}","{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-05-15T03:13:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2022-09-12', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Sanjay Haresh,\xa0Xiaohao Sun,\xa0Hanxiao Jiang,\xa0Angel X. Chang,\xa0Manolis Savva', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Sanjay Haresh,\xa0Xiaohao Sun,\xa0Hanxiao Jiang,\xa0Angel X. Chang,\xa0Manolis Savva', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': '3dhoi.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/0d1ee7fa-0137-43f3-838a-9fad087503cc/3dhoi.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234741Z&X-Amz-Expires=3600&X-Amz-Signature=67ae6481edee2d58121e7cec154511849678a72ec70ace4af49d2e9e72c1e6df&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:41.418Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://3dlg-hcvc.github.io/3dhoi/', 'link': {'url': 'https://3dlg-hcvc.github.io/3dhoi/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://3dlg-hcvc.github.io/3dhoi/', 'href': 'https://3dlg-hcvc.github.io/3dhoi/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': '3DV 2022', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': '3DV 2022', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'd9d68329-9f05-4187-aa6b-052567aebad4', 'name': '3DV', 'color': 'gray'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}, {'id': 'bccbeae3-55ce-4b38-9abc-61c658ca30b4', 'name': 'Manolis', 'color': 'blue'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2209.05612', 'link': {'url': 'https://arxiv.org/abs/2209.05612'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2209.05612', 'href': 'https://arxiv.org/abs/2209.05612'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '❌❌❌❌'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Articulated 3D Human-Object Interactions from RGB Videos:An Empirical Analysis of Approaches and Challenges', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Articulated 3D Human-Object Interactions from RGB Videos:An Empirical Analysis of Approaches and Challenges', 'href': None}]}}",https://www.notion.so/Articulated-3D-Human-Object-Interactions-from-RGB-Videos-An-Empirical-Analysis-of-Approaches-and-Cha-315625ea6b604cf6b1df990c9c46a53b,https://yanxg.notion.site/Articulated-3D-Human-Object-Interactions-from-RGB-Videos-An-Empirical-Analysis-of-Approaches-and-Cha-315625ea6b604cf6b1df990c9c46a53b
78,page,d678f388-423a-436a-8fb4-e65ec066498c,2023-06-14T23:13:00.000Z,2023-06-14T23:58:00.000Z,"{'object': 'user', 'id': '2de96350-7917-4a98-8014-dc5695a68171'}","{'object': 'user', 'id': '2de96350-7917-4a98-8014-dc5695a68171'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-06-14T23:58:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2022-07-30', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Zeshi Yang, KangKang Yin, and Libin Liu', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Zeshi Yang, KangKang Yin, and Libin Liu', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '2de96350-7917-4a98-8014-dc5695a68171'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://www.cs.sfu.ca/~kkyin/papers/chopsticks.jpg', 'type': 'external', 'external': {'url': 'https://www.cs.sfu.ca/~kkyin/papers/chopsticks.jpg'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://github.com/chopsticks-research2022/learning2usechopsticks', 'link': {'url': 'https://github.com/chopsticks-research2022/learning2usechopsticks'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://github.com/chopsticks-research2022/learning2usechopsticks', 'href': 'https://github.com/chopsticks-research2022/learning2usechopsticks'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'SIGGRAPH 2022', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'SIGGRAPH 2022', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '2de96350-7917-4a98-8014-dc5695a68171'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '45ac99f3-aeab-4f6c-9370-e80f8adbc8c8', 'name': 'SIGGRAPH', 'color': 'brown'}, {'id': '7059893d-3726-44a9-af21-36fdbfdcb80d', 'name': 'KangKang', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2205.14313', 'link': {'url': 'https://arxiv.org/abs/2205.14313'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2205.14313', 'href': 'https://arxiv.org/abs/2205.14313'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '❌❌❌❌'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Learning to Use Chopsticks in Diverse Gripping Styles', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Learning to Use Chopsticks in Diverse Gripping Styles', 'href': None}]}}",https://www.notion.so/Learning-to-Use-Chopsticks-in-Diverse-Gripping-Styles-d678f388423a436a8fb4e65ec066498c,https://yanxg.notion.site/Learning-to-Use-Chopsticks-in-Diverse-Gripping-Styles-d678f388423a436a8fb4e65ec066498c
79,page,c98548ee-e3c8-4015-8b44-f2af87f8c61f,2023-05-15T02:59:00.000Z,2023-05-15T03:08:00.000Z,"{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}","{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-05-15T03:08:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2022-05-31', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Yasaman Etesam, Leon Kochiev,\xa0Angel X. Chang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Yasaman Etesam, Leon Kochiev,\xa0Angel X. Chang', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': '3dvqa.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/9525ee57-f626-43f5-a153-603c609aa764/3dvqa.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234741Z&X-Amz-Expires=3600&X-Amz-Signature=9c813db591071fb86bc1f6d946815a1014c8e20259e1c472fc0b1aafe573abd3&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:41.426Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://3dlg-hcvc.github.io/3DVQA/', 'link': {'url': 'https://3dlg-hcvc.github.io/3DVQA/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://3dlg-hcvc.github.io/3DVQA/', 'href': 'https://3dlg-hcvc.github.io/3DVQA/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CRV 2022', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CRV 2022', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '8492aa9e-2202-4887-9296-8c313873f664', 'name': 'CRV', 'color': 'green'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://ieeexplore.ieee.org/document/9866910', 'link': {'url': 'https://ieeexplore.ieee.org/document/9866910'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://ieeexplore.ieee.org/document/9866910', 'href': 'https://ieeexplore.ieee.org/document/9866910'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '❌❌❌❌'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': '3DVQA: Visual Question Answering for 3D Environments', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': '3DVQA: Visual Question Answering for 3D Environments', 'href': None}]}}",https://www.notion.so/3DVQA-Visual-Question-Answering-for-3D-Environments-c98548eee3c840158b44f2af87f8c61f,https://yanxg.notion.site/3DVQA-Visual-Question-Answering-for-3D-Environments-c98548eee3c840158b44f2af87f8c61f
80,page,93aea945-8b99-4fa3-aaef-ae5b7d9e943e,2023-05-14T00:50:00.000Z,2023-06-15T05:34:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '493c6f46-4677-4e9d-89ae-4f368ad116e8'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-06-15T05:34:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2022-05-14', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Xiang Xu, Karl Willis, Joseph Lambourne, Chin-Yi Cheng, Pradeep Kumar Jayaraman, and Yasutaka Furukawa', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Xiang Xu, Karl Willis, Joseph Lambourne, Chin-Yi Cheng, Pradeep Kumar Jayaraman, and Yasutaka Furukawa', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '493c6f46-4677-4e9d-89ae-4f368ad116e8'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'We present SkexGen, a novel autoregressive generative model for computer-aided design (CAD) construction sequences containing sketch-and-extrude modeling operations. Our model utilizes distinct Transformer architectures to encode topological, geometric, and extrusion variations of construction sequences into disentangled codebooks. Autoregressive Transformer decoders generate CAD construction sequences sharing certain properties specified by the codebook vectors. Extensive experiments demonstrate that our disentangled codebook representation generates diverse and high-quality CAD models, enhances user control, and enables efficient exploration of the design space.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'We present SkexGen, a novel autoregressive generative model for computer-aided design (CAD) construction sequences containing sketch-and-extrude modeling operations. Our model utilizes distinct Transformer architectures to encode topological, geometric, and extrusion variations of construction sequences into disentangled codebooks. Autoregressive Transformer decoders generate CAD construction sequences sharing certain properties specified by the codebook vectors. Extensive experiments demonstrate that our disentangled codebook representation generates diverse and high-quality CAD models, enhances user control, and enables efficient exploration of the design space.', 'href': None}]}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'cad_rand[1].png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/8f4caa5c-6a35-406b-aed4-8a8595b252ec/cad_rand1.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234741Z&X-Amz-Expires=3600&X-Amz-Signature=b0e93b12113a121fcecad5127a9d04003bb0c6bb223be9ba04f2b92c5d92ecdb&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:41.446Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://samxuxiang.github.io/skexgen/index.html', 'link': {'url': 'https://samxuxiang.github.io/skexgen/index.html'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://samxuxiang.github.io/skexgen/index.html', 'href': 'https://samxuxiang.github.io/skexgen/index.html'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ICML', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ICML', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '7f7fc247-a823-45f6-903f-d3f0c5635c85', 'name': 'ICML', 'color': 'default'}, {'id': '898c1b27-4fe4-4c46-8f37-c2c8cdfe0997', 'name': 'Yasu', 'color': 'red'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2207.04632', 'link': {'url': 'https://arxiv.org/abs/2207.04632'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2207.04632', 'href': 'https://arxiv.org/abs/2207.04632'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '✔️'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'SkexGen: Generating CAD Construction Sequences by Autoregressive VAE with Disentangled Codebooks', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'SkexGen: Generating CAD Construction Sequences by Autoregressive VAE with Disentangled Codebooks', 'href': None}]}}",https://www.notion.so/SkexGen-Generating-CAD-Construction-Sequences-by-Autoregressive-VAE-with-Disentangled-Codebooks-93aea9458b994fa3aaefae5b7d9e943e,https://yanxg.notion.site/SkexGen-Generating-CAD-Construction-Sequences-by-Autoregressive-VAE-with-Disentangled-Codebooks-93aea9458b994fa3aaefae5b7d9e943e
81,page,7d93afe8-9aea-4cba-b951-e14e6849e611,2023-06-14T20:19:00.000Z,2023-06-14T20:21:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-06-14T20:21:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2022-04-14', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Jiacheng Chen, Yiming Qian, and Yasutaka Furukawa', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Jiacheng Chen, Yiming Qian, and Yasutaka Furukawa', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://www.cs.sfu.ca/~furukawa/newimages/cvpr2022-heat.jpg', 'type': 'external', 'external': {'url': 'https://www.cs.sfu.ca/~furukawa/newimages/cvpr2022-heat.jpg'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://heat-structured-reconstruction.github.io/', 'link': {'url': 'https://heat-structured-reconstruction.github.io/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://heat-structured-reconstruction.github.io/', 'href': 'https://heat-structured-reconstruction.github.io/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2022', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2022', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '898c1b27-4fe4-4c46-8f37-c2c8cdfe0997', 'name': 'Yasu', 'color': 'red'}, {'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2111.15143', 'link': {'url': 'https://arxiv.org/abs/2111.15143'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2111.15143', 'href': 'https://arxiv.org/abs/2111.15143'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '❌❌❌❌'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'HEAT: Holistic Edge Attention Transformer for Structured Reconstruction', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'HEAT: Holistic Edge Attention Transformer for Structured Reconstruction', 'href': None}]}}",https://www.notion.so/HEAT-Holistic-Edge-Attention-Transformer-for-Structured-Reconstruction-7d93afe89aea4cbab951e14e6849e611,https://yanxg.notion.site/HEAT-Holistic-Edge-Attention-Transformer-for-Structured-Reconstruction-7d93afe89aea4cbab951e14e6849e611
82,page,a0971bb6-97d2-4960-a91b-747093d53f8b,2023-06-14T22:47:00.000Z,2023-06-15T03:14:00.000Z,"{'object': 'user', 'id': '811c8c6b-f3a1-43f3-ae6d-d85d1c2cf848'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-06-15T03:14:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2022-03-30', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Qimin Chen, Johannes Merz, Aditya Sanghi, Hooman Shayani,\xa0Ali Mahdavi-Amiri, Hao Zhang', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Qimin Chen, Johannes Merz, Aditya Sanghi, Hooman Shayani,\xa0Ali Mahdavi-Amiri, Hao Zhang', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://qiminchen.github.io/unist/images/teaser.svg', 'type': 'external', 'external': {'url': 'https://qiminchen.github.io/unist/images/teaser.svg'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://qiminchen.github.io/unist/', 'link': {'url': 'https://qiminchen.github.io/unist/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://qiminchen.github.io/unist/', 'href': 'https://qiminchen.github.io/unist/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2022', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2022', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '811c8c6b-f3a1-43f3-ae6d-d85d1c2cf848'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'e16d2c21-c87f-4a73-aa66-85bb6bc6e5b0', 'name': 'Richard', 'color': 'pink'}, {'id': 'ed487f9e-f5ad-446d-89eb-2ab46ab0892c', 'name': 'Ali', 'color': 'purple'}, {'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_UNIST_Unpaired_Neural_Implicit_Shape_Translation_Network_CVPR_2022_paper.pdf', 'link': {'url': 'https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_UNIST_Unpaired_Neural_Implicit_Shape_Translation_Network_CVPR_2022_paper.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_UNIST_Unpaired_Neural_Implicit_Shape_Translation_Network_CVPR_2022_paper.pdf', 'href': 'https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_UNIST_Unpaired_Neural_Implicit_Shape_Translation_Network_CVPR_2022_paper.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '❌❌❌❌'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'UNIST: Unpaired Neural Implicit Shape Translation Network', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'UNIST: Unpaired Neural Implicit Shape Translation Network', 'href': None}]}}",https://www.notion.so/UNIST-Unpaired-Neural-Implicit-Shape-Translation-Network-a0971bb697d24960a91b747093d53f8b,https://yanxg.notion.site/UNIST-Unpaired-Neural-Implicit-Shape-Translation-Network-a0971bb697d24960a91b747093d53f8b
83,page,386b9443-f01c-4dbe-b712-18d7b1910e35,2023-06-14T20:21:00.000Z,2023-06-14T20:22:00.000Z,"{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-06-14T20:22:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2022-03-14', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Yiming Qian, Hang Yan, Sachini Herath, Pyojin Kim, and Yasutaka Furukawa', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Yiming Qian, Hang Yan, Sachini Herath, Pyojin Kim, and Yasutaka Furukawa', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://www.cs.sfu.ca/~furukawa/newimages/2022-icra-wifi-sfm.jpg', 'type': 'external', 'external': {'url': 'https://www.cs.sfu.ca/~furukawa/newimages/2022-icra-wifi-sfm.jpg'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://drive.google.com/file/d/1rOKqGDQ1NtVay3-DUvhJPDhjIXD5ydSH/view?usp=sharing', 'link': {'url': 'https://drive.google.com/file/d/1rOKqGDQ1NtVay3-DUvhJPDhjIXD5ydSH/view?usp=sharing'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://drive.google.com/file/d/1rOKqGDQ1NtVay3-DUvhJPDhjIXD5ydSH/view?usp=sharing', 'href': 'https://drive.google.com/file/d/1rOKqGDQ1NtVay3-DUvhJPDhjIXD5ydSH/view?usp=sharing'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'ICRA 2022', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'ICRA 2022', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '898c1b27-4fe4-4c46-8f37-c2c8cdfe0997', 'name': 'Yasu', 'color': 'red'}, {'id': '2d02ae48-7ae4-4c33-9b05-fb8723245ad7', 'name': 'ICRA', 'color': 'purple'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://drive.google.com/file/d/1rOKqGDQ1NtVay3-DUvhJPDhjIXD5ydSH/view?usp=sharing', 'link': {'url': 'https://drive.google.com/file/d/1rOKqGDQ1NtVay3-DUvhJPDhjIXD5ydSH/view?usp=sharing'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://drive.google.com/file/d/1rOKqGDQ1NtVay3-DUvhJPDhjIXD5ydSH/view?usp=sharing', 'href': 'https://drive.google.com/file/d/1rOKqGDQ1NtVay3-DUvhJPDhjIXD5ydSH/view?usp=sharing'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '❌❌❌❌'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Single User WiFi Structure from Motion in the Wild', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Single User WiFi Structure from Motion in the Wild', 'href': None}]}}",https://www.notion.so/Single-User-WiFi-Structure-from-Motion-in-the-Wild-386b9443f01c4dbeb71218d7b1910e35,https://yanxg.notion.site/Single-User-WiFi-Structure-from-Motion-in-the-Wild-386b9443f01c4dbeb71218d7b1910e35
84,page,23711644-bc80-4dc8-b9f6-5a112d0a4200,2023-05-15T03:24:00.000Z,2023-06-14T19:19:00.000Z,"{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}","{'object': 'user', 'id': '752f8d93-6b11-44d7-80a5-0d790586a229'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-06-14T19:19:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2021-12-07', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Santhosh K Ramakrishnan, Aaron Gokaslan, Erik Wijmans, Oleksandr Maksymets, Alex Clegg, John Turner, Eric Undersander, Wojciech Galuba, Andrew Westbury, Angel X Chang, Manolis Savva, Yili Zhao, Dhruv Batra', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Santhosh K Ramakrishnan, Aaron Gokaslan, Erik Wijmans, Oleksandr Maksymets, Alex Clegg, John Turner, Eric Undersander, Wojciech Galuba, Andrew Westbury, Angel X Chang, Manolis Savva, Yili Zhao, Dhruv Batra', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '752f8d93-6b11-44d7-80a5-0d790586a229'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'hm3d.jpeg', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/c4db31f4-116e-4d67-8189-a9eb0e7a1e5e/hm3d.jpeg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234741Z&X-Amz-Expires=3600&X-Amz-Signature=a9c04dd413eba271ab3f63cfe113905de4f2c69c39142a23b8d940d1fc5fac02&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:41.462Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://aihabitat.org/datasets/hm3d/', 'link': {'url': 'https://aihabitat.org/datasets/hm3d/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://aihabitat.org/datasets/hm3d/', 'href': 'https://aihabitat.org/datasets/hm3d/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'NeurIPS Datasets and Benchmarks Track 2021', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'NeurIPS Datasets and Benchmarks Track 2021', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '6a51b57b-f1f8-4b18-ac73-a0ace74eb41a', 'name': 'NeurIPS Datasets and Benchmarks', 'color': 'blue'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}, {'id': 'bccbeae3-55ce-4b38-9abc-61c658ca30b4', 'name': 'Manolis', 'color': 'blue'}, {'id': 'bb1c7b2a-93d8-4f0d-9e32-2a3594a652cd', 'name': 'NeurIPS', 'color': 'default'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2109.08238', 'link': {'url': 'https://arxiv.org/abs/2109.08238'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2109.08238', 'href': 'https://arxiv.org/abs/2109.08238'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '❌❌❌❌'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Habitat-Matterport 3D Dataset (HM3D): 1000 Large-scale 3D Environments for Embodied AI', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Habitat-Matterport 3D Dataset (HM3D): 1000 Large-scale 3D Environments for Embodied AI', 'href': None}]}}",https://www.notion.so/Habitat-Matterport-3D-Dataset-HM3D-1000-Large-scale-3D-Environments-for-Embodied-AI-23711644bc804dc8b9f65a112d0a4200,https://yanxg.notion.site/Habitat-Matterport-3D-Dataset-HM3D-1000-Large-scale-3D-Environments-for-Embodied-AI-23711644bc804dc8b9f65a112d0a4200
85,page,bcca4a74-3f90-456c-b372-a45393aeb3ca,2023-05-15T03:21:00.000Z,2023-05-15T03:27:00.000Z,"{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}","{'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-05-15T03:27:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2021-12-07', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Andrew Szot, Alexander Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Singh Chaplot, Oleksandr Maksymets, Aaron Gokaslan, Vladimír Vondruš, Sameer Dharur, Franziska Meier, Wojciech Galuba, Angel Chang, Zsolt Kira, Vladlen Koltun, Jitendra Malik, Manolis Savva, Dhruv Batra', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Andrew Szot, Alexander Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre, Mustafa Mukadam, Devendra Singh Chaplot, Oleksandr Maksymets, Aaron Gokaslan, Vladimír Vondruš, Sameer Dharur, Franziska Meier, Wojciech Galuba, Angel Chang, Zsolt Kira, Vladlen Koltun, Jitendra Malik, Manolis Savva, Dhruv Batra', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'hab2.jpeg', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/b5c2b23a-6b97-45c9-bd38-8ff135a65906/hab2.jpeg?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234741Z&X-Amz-Expires=3600&X-Amz-Signature=a7304549e9549568da33240492aa90abebb92de97551b6c14637f4384de435dc&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:41.466Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://aihabitat.org/', 'link': {'url': 'https://aihabitat.org/'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://aihabitat.org/', 'href': 'https://aihabitat.org/'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'NeurIPS 2021', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'NeurIPS 2021', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': 'f31fa168-6351-401c-86ba-83d31c9bfe45'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'bb1c7b2a-93d8-4f0d-9e32-2a3594a652cd', 'name': 'NeurIPS', 'color': 'default'}, {'id': '3f09b3b2-12ce-418c-8bbd-329349d82802', 'name': 'Angel', 'color': 'brown'}, {'id': 'bccbeae3-55ce-4b38-9abc-61c658ca30b4', 'name': 'Manolis', 'color': 'blue'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2106.14405', 'link': {'url': 'https://arxiv.org/abs/2106.14405'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2106.14405', 'href': 'https://arxiv.org/abs/2106.14405'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '❌❌❌❌'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Habitat 2.0: Training Home Assistants to Rearrange their Habitat', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Habitat 2.0: Training Home Assistants to Rearrange their Habitat', 'href': None}]}}",https://www.notion.so/Habitat-2-0-Training-Home-Assistants-to-Rearrange-their-Habitat-bcca4a743f90456cb372a45393aeb3ca,https://yanxg.notion.site/Habitat-2-0-Training-Home-Assistants-to-Rearrange-their-Habitat-bcca4a743f90456cb372a45393aeb3ca
86,page,772535f6-7463-4bd9-a12c-36c8030b9b9e,2023-06-14T22:36:00.000Z,2023-06-15T03:13:00.000Z,"{'object': 'user', 'id': '2de96350-7917-4a98-8014-dc5695a68171'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-06-15T03:13:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2021-06-30', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Himanshu Arora,\xa0Saurabh Mishra,\xa0Shichong Peng,\xa0Ke Li,\xa0Ali Mahdavi-Amiri', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Himanshu Arora,\xa0Saurabh Mishra,\xa0Shichong Peng,\xa0Ke Li,\xa0Ali Mahdavi-Amiri', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'Snipaste_2023-06-14_20-13-32.png', 'type': 'file', 'file': {'url': 'https://prod-files-secure.s3.us-west-2.amazonaws.com/50d2f7ab-6ba6-45d4-bc3a-917e5e89e239/5610ebd8-2712-46d0-93e6-c42a6e3336ac/Snipaste_2023-06-14_20-13-32.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=AKIAT73L2G45FSPPWI6X%2F20241203%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Date=20241203T234741Z&X-Amz-Expires=3600&X-Amz-Signature=6442edf4816bdd00b594ff17af0fa3c1c7995f5c664aebdf73ad1b9896ea9a1e&X-Amz-SignedHeaders=host&x-id=GetObject', 'expiry_time': '2024-12-04T00:47:41.475Z'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2106.16237', 'link': {'url': 'https://arxiv.org/abs/2106.16237'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2106.16237', 'href': 'https://arxiv.org/abs/2106.16237'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2022', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2022', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '2de96350-7917-4a98-8014-dc5695a68171'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'ed487f9e-f5ad-446d-89eb-2ab46ab0892c', 'name': 'Ali', 'color': 'purple'}, {'id': '9ea36cd3-48d3-462d-baf9-b75ef77f4121', 'name': 'Ke', 'color': 'brown'}, {'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/abs/2106.16237', 'link': {'url': 'https://arxiv.org/abs/2106.16237'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/abs/2106.16237', 'href': 'https://arxiv.org/abs/2106.16237'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '❌❌❌❌'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Multimodal Shape Completion via IMLE', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Multimodal Shape Completion via IMLE', 'href': None}]}}",https://www.notion.so/Multimodal-Shape-Completion-via-IMLE-772535f674634bd9a12c36c8030b9b9e,https://yanxg.notion.site/Multimodal-Shape-Completion-via-IMLE-772535f674634bd9a12c36c8030b9b9e
87,page,87e25aef-ee38-4efe-ab82-5ad3b3edaebb,2023-06-14T23:59:00.000Z,2023-06-15T00:02:00.000Z,"{'object': 'user', 'id': '2de96350-7917-4a98-8014-dc5695a68171'}","{'object': 'user', 'id': '2de96350-7917-4a98-8014-dc5695a68171'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-06-15T00:02:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2021-05-02', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Zhiqi Yin, Zeshi Yang, Michiel van de Panne, KangKang Yin', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Zhiqi Yin, Zeshi Yang, Michiel van de Panne, KangKang Yin', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '2de96350-7917-4a98-8014-dc5695a68171'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://arpspoof.github.io/project/jump/teaser.png', 'type': 'external', 'external': {'url': 'https://arpspoof.github.io/project/jump/teaser.png'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arpspoof.github.io/project/jump/jump.html', 'link': {'url': 'https://arpspoof.github.io/project/jump/jump.html'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arpspoof.github.io/project/jump/jump.html', 'href': 'https://arpspoof.github.io/project/jump/jump.html'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'SIGGRAPH 2021', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'SIGGRAPH 2021', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '2de96350-7917-4a98-8014-dc5695a68171'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': '45ac99f3-aeab-4f6c-9370-e80f8adbc8c8', 'name': 'SIGGRAPH', 'color': 'brown'}, {'id': '7059893d-3726-44a9-af21-36fdbfdcb80d', 'name': 'KangKang', 'color': 'brown'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://www.cs.sfu.ca/~kkyin/papers/Jump.pdf', 'link': {'url': 'https://www.cs.sfu.ca/~kkyin/papers/Jump.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://www.cs.sfu.ca/~kkyin/papers/Jump.pdf', 'href': 'https://www.cs.sfu.ca/~kkyin/papers/Jump.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '❌❌❌❌'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'Discovering Diverse Athletic Jumping Strategies', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Discovering Diverse Athletic Jumping Strategies', 'href': None}]}}",https://www.notion.so/Discovering-Diverse-Athletic-Jumping-Strategies-87e25aefee384efeab825ad3b3edaebb,https://yanxg.notion.site/Discovering-Diverse-Athletic-Jumping-Strategies-87e25aefee384efeab825ad3b3edaebb
88,page,8e49a3c8-3bc1-4910-ab61-559e02920f96,2023-06-14T22:50:00.000Z,2023-06-15T03:12:00.000Z,"{'object': 'user', 'id': '811c8c6b-f3a1-43f3-ae6d-d85d1c2cf848'}","{'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}",,,"{'type': 'database_id', 'database_id': '54cb3960-b402-46e2-be42-cfb58e2b45fd'}",False,False,"{'Last edited time': {'id': 'FM%3Dx', 'type': 'last_edited_time', 'last_edited_time': '2023-06-15T03:12:00.000Z'}, 'ConferenceDate': {'id': 'JnqY', 'type': 'date', 'date': {'start': '2021-04-12', 'end': None, 'time_zone': None}}, 'Authors': {'id': 'Km%3Db', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'Fenggen Yu, Zhiqin Chen, Manyi Li, Aditya Sanghi, Hooman Shayani,\xa0Ali Mahdavi-Amiri, Hao Zhang.', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'Fenggen Yu, Zhiqin Chen, Manyi Li, Aditya Sanghi, Hooman Shayani,\xa0Ali Mahdavi-Amiri, Hao Zhang.', 'href': None}]}, 'Last edited by': {'id': 'MZ%40S', 'type': 'last_edited_by', 'last_edited_by': {'object': 'user', 'id': '147bdf86-4261-4d70-ba95-2923623aa778'}}, 'Abstract': {'id': 'UIOf', 'type': 'rich_text', 'rich_text': []}, 'Thumbnail': {'id': 'YalY', 'type': 'files', 'files': [{'name': 'https://fenggenyu.github.io/images/capri/teaser.png', 'type': 'external', 'external': {'url': 'https://fenggenyu.github.io/images/capri/teaser.png'}}]}, 'project_link': {'id': '%5D%5EpD', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://fenggenyu.github.io/capri.html', 'link': {'url': 'https://fenggenyu.github.io/capri.html'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://fenggenyu.github.io/capri.html', 'href': 'https://fenggenyu.github.io/capri.html'}]}, 'Venue': {'id': '_e%40%3F', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'CVPR 2022', 'link': None}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CVPR 2022', 'href': None}]}, 'Created by': {'id': 'aBTi', 'type': 'created_by', 'created_by': {'object': 'user', 'id': '811c8c6b-f3a1-43f3-ae6d-d85d1c2cf848'}}, 'video_link': {'id': 'k%5Ei%5D', 'type': 'rich_text', 'rich_text': []}, 'presentation_link': {'id': 'lwp%7D', 'type': 'rich_text', 'rich_text': []}, 'Tags': {'id': 'mula', 'type': 'multi_select', 'multi_select': [{'id': 'e16d2c21-c87f-4a73-aa66-85bb6bc6e5b0', 'name': 'Richard', 'color': 'pink'}, {'id': 'ed487f9e-f5ad-446d-89eb-2ab46ab0892c', 'name': 'Ali', 'color': 'purple'}, {'id': '1ffd3246-db7a-4ea8-855d-f920b194ab96', 'name': 'CVPR', 'color': 'orange'}]}, 'paper_link': {'id': 'ut%5Ex', 'type': 'rich_text', 'rich_text': [{'type': 'text', 'text': {'content': 'https://arxiv.org/pdf/2104.05652.pdf', 'link': {'url': 'https://arxiv.org/pdf/2104.05652.pdf'}}, 'annotations': {'bold': False, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'https://arxiv.org/pdf/2104.05652.pdf', 'href': 'https://arxiv.org/pdf/2104.05652.pdf'}]}, 'If Valid': {'id': '%7C%3DNR', 'type': 'formula', 'formula': {'type': 'string', 'string': '❌❌❌❌'}}, 'Name': {'id': 'title', 'type': 'title', 'title': [{'type': 'text', 'text': {'content': 'CAPRI-Net: Learning Compact CAD Shapes with Adaptive Primitive Assembly', 'link': None}, 'annotations': {'bold': True, 'italic': False, 'strikethrough': False, 'underline': False, 'code': False, 'color': 'default'}, 'plain_text': 'CAPRI-Net: Learning Compact CAD Shapes with Adaptive Primitive Assembly', 'href': None}]}}",https://www.notion.so/CAPRI-Net-Learning-Compact-CAD-Shapes-with-Adaptive-Primitive-Assembly-8e49a3c83bc14910ab61559e02920f96,https://yanxg.notion.site/CAPRI-Net-Learning-Compact-CAD-Shapes-with-Adaptive-Primitive-Assembly-8e49a3c83bc14910ab61559e02920f96
